{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00453283",
   "metadata": {},
   "source": [
    "# <font color='red'> <b> <center> TRANSFORMER </center> </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4abf77",
   "metadata": {},
   "source": [
    "## <font color='blue'> Table of contents </font>\n",
    "\n",
    "- [1 .Intro](#1)\n",
    "- [2. Setup](#2)\n",
    "- [3. Positional Encoding](#3) <br>\n",
    "    - [3.1. Concept and motivation](#3.1)\n",
    "    - [3.2. Implementation](#3.2)\n",
    "- [4. Masking](#4) <br>\n",
    "    - [4.1. Padding Mask](#4.1) <br>\n",
    "    - [4.2. Look Ahead Mask](#4.1) <br>\n",
    "- [5. Attention](#5) <br>\n",
    "    - [5.1. Types of Attention](#5.1) <br>\n",
    "    - [5.2. Scaled dot-product attention](#5.2) <br>\n",
    "    - [5.3. Multi-head Attention](#5.3) <br>\n",
    "    - [5.3. Multi-head Attention](#5.3) <br>\n",
    "- [6. Feed Forward Neural Network](#6) <br>\n",
    "- [7. Encoder](#7) <br>\n",
    "    - [7.1. Encoder Layer](#7.1) <br>\n",
    "    - [7.2. Full Encoder](#7.2) <br>\n",
    "- [8. Decoder](#8) <br>\n",
    "    - [8.1. Decoder Layer](#8.1) <br>\n",
    "    - [8.2. Full Decoder](#8.2) <br>\n",
    "- [9. Transformer](#9) <br>\n",
    "    - [9.1. Model](#9.1) <br>\n",
    "    - [9.2. Training and preidctions](#9.2) <br>\n",
    "- [ANNEX](#annex) <br>\n",
    "    - [Custom Training](#custom_training) <br>\n",
    "- [References](#references) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1b6da",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <font color='red'> <b>  1. Intro </b> </font>\n",
    "\n",
    "This notebook is dedicated to implementing a Transformer model from scratch, following the architecture described in the groundbreaking paper \"Attention Is All You Need\" by Vaswani et al. The primary objective of this implementation is to learn and understand the inner workings of Transformers, rather than simply using prebuilt libraries.\n",
    "\n",
    "I am going to cover key components such as:\n",
    "\n",
    "- Positional Encoding: Injecting order into sequences.\n",
    "- Scaled Dot-Product Attention: The core mechanism behind attention.\n",
    "- Multi-Head Attention: Learning diverse relationships in data.\n",
    "- Feed-Forward Networks: Extending attention's capabilities.\n",
    "- Layer Normalization and Residual Connections: Stabilizing and enhancing learning.\n",
    "- Encoder-Decoder Structure: Processing input and generating output.\n",
    "\n",
    "By the end of this notebook, you should gain a conceptual understanding of each component and how they fit together to form the Transformer. This hands-on approach will help demystify the architecture and prepare you to adapt or experiment with it for your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc891d8",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <font color='red'> <b>  2. Setup </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d678e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aacec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the print options\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69acea5",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <font color='red'> <b>  3. Positional Encoding </b> </font>\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### <font color='orange'> <b> 3.1. Concept and motivation </b> </font>\n",
    "\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model.  However, when you train a Transformer network, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network.\n",
    "\n",
    "\n",
    "There are various types of positional encoding, for example:\n",
    "\n",
    "- Sinusoidal encoding\n",
    "- Learned Positional Encoding\n",
    "- Relative Positional encoding\n",
    "\n",
    "\n",
    "In this notebook I will use the sinusoidal encoding, given by the following formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{n}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{n}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $i$ refers to each of the different dimensions of the positional encoding.\n",
    "* $n$ is a user defined scalar (10000 in the paper)\n",
    "\n",
    "\n",
    "The values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data. \n",
    "\n",
    "In this notebook I am going to use horizontal vectors, that's why all the matrix multiplications should be adjusted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb63d2b",
   "metadata": {},
   "source": [
    "Let’s clarify the **number of positions in the sequence** with an example.\n",
    "\n",
    "\n",
    "Imagine you have a sequence of words:\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\n",
    "\n",
    "Here:\n",
    "\n",
    "- Each word in the sequence corresponds to a position.\n",
    "- The total number of positions is the number of words in the sequence, which is 6 in this case.\n",
    "\n",
    "Now, if you're embedding these words (e.g., using word embeddings), you might represent the sequence as a matrix where:\n",
    "\n",
    "- Rows correspond to positions in the sequence (one row for each word).\n",
    "- Columns correspond to the embedding dimensions (let’s assume d=4 for simplicity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b735c3",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### <font color='orange'> <b> 3.2. Implementation </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5601a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get the angles for the positional encoding\n",
    "def get_angles(pos, i, d, n=10000):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        i --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "        n -- user defined scalar\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    angles = pos/ (np.power(n, (2 * (i//2)) / np.float32(d)))\n",
    "\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3cb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If i = 0 -> i//2 = 0\n",
      "If i = 1 -> i//2 = 0\n",
      "If i = 2 -> i//2 = 1\n",
      "If i = 3 -> i//2 = 1\n"
     ]
    }
   ],
   "source": [
    "# NOTE\n",
    "for i in range(4):\n",
    "    print(f\"If i = {i} -> i//2 = {i//2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e7168",
   "metadata": {},
   "source": [
    "Let's try our function.\n",
    "\n",
    "First let's remember that we can use np.newaxis to add an extra dimension to a numpy array.\n",
    "\n",
    "For example, in np.arange(7)[:, np.newaxis], [:, np.newaxis] adds a new axis (dimension) to the array along the columns, essentially converting the 1D array into a 2D column vector.\n",
    "\n",
    "In the case of np.arange(3)[np.newaxis, :], the new axis is added along the rows, resulting in a row vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5245bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7,), (7, 1), (3,), (1, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(7).shape, np.arange(7)[:, np.newaxis].shape, np.arange(3).shape,  np.arange(3)[np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb099cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(7), np.arange(7)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5748742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([[0, 1, 2]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3), np.arange(3)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13af562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  ],\n",
       "       [1.  , 1.  , 0.01],\n",
       "       [2.  , 2.  , 0.02],\n",
       "       [3.  , 3.  , 0.03],\n",
       "       [4.  , 4.  , 0.04],\n",
       "       [5.  , 5.  , 0.05],\n",
       "       [6.  , 6.  , 0.06]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = get_angles(np.arange(7)[:, np.newaxis], np.arange(3)[np.newaxis, :], 4)\n",
    "print(example.shape)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad6f37",
   "metadata": {},
   "source": [
    "Now, let's implement the positional encoding. We have to use the sine equation when $i$ is an even number and the cosine equation when $i$ is an odd number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c540b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[ np.newaxis,:],\n",
    "                            d)\n",
    "      \n",
    "    # -> angle_rads has dim (positions,d)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # #  add an extra dimension at the beginning of the array, \n",
    "    # so the final shape of pos_encoding becomes (1, positions, d)\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e10d5",
   "metadata": {},
   "source": [
    "**Note.**. Let's talk a little bit more about the line \"pos_encoding = angle_rads[np.newaxis, ...]\". The purpose of np.newaxis is to add an extra dimension at the beginning of the array, so the final shape of pos_encoding becomes (1, positions, d). This means the positional encoding has an additional dimension at the start, representing a \"batch\" of size 1 (this is typical in TensorFlow and other deep learning frameworks).\n",
    "\n",
    "For example, if positions = 3 and d = 4, the shape of pos_encoding will be (1, 3, 4).\n",
    "\n",
    "- 1 represents the batch size (in this case, just one sequence set).\n",
    "- 3 is the number of positions in the sequence.\n",
    "- 4 is the size of the embedding vectors.\n",
    "\n",
    "To retrieve the positional encoding for the positions, you can take pos_encoding[0], which selects the matrix with dimensions (3, 4). This is the positional encoding to be added to your input embeddings, representing the application of positional encoding to your input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b607f1",
   "metadata": {},
   "source": [
    "Let's try our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e8a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 3), dtype=float32, numpy=\n",
       "array([[[ 0.   ,  1.   ,  0.   ],\n",
       "        [ 0.841,  0.54 ,  0.002],\n",
       "        [ 0.909, -0.416,  0.004],\n",
       "        [ 0.141, -0.99 ,  0.006],\n",
       "        [-0.757, -0.654,  0.009],\n",
       "        [-0.959,  0.284,  0.011],\n",
       "        [-0.279,  0.96 ,  0.013],\n",
       "        [ 0.657,  0.754,  0.015]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 positions with an embedding size of 3\n",
    "example = positional_encoding(8, 3)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8468ff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 3), dtype=float32, numpy=\n",
       "array([[ 0.   ,  1.   ,  0.   ],\n",
       "       [ 0.841,  0.54 ,  0.002],\n",
       "       [ 0.909, -0.416,  0.004],\n",
       "       [ 0.141, -0.99 ,  0.006],\n",
       "       [-0.757, -0.654,  0.009],\n",
       "       [-0.959,  0.284,  0.011],\n",
       "       [-0.279,  0.96 ,  0.013],\n",
       "       [ 0.657,  0.754,  0.015]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43604db5",
   "metadata": {},
   "source": [
    "Let's plot our positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2888e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAG2CAYAAAC3VWZSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6MUlEQVR4nOzdd3wUdf7H8deUnd1NNtn0BqEXQYoUpSmiIvZy1rNgxe4pcmdBzxP9eba7Q/RsZzv0VNCzFyygAipFpAlIr4EkpJK6bWb298dmlwRCS+Ai5vN8POaRyex3ZmdTNt98Z96frxIOh8MIIYQQQhwm1JY+ASGEEEKIAyGdFyGEEEIcVqTzIoQQQojDinRehBBCCHFYkc6LEEIIIQ4r0nkRQgghxGFFOi9CCCGEOKxI50UIIYQQhxXpvAghhBDisCKdFyGEEEIcVlq08zJhwgQURWmwZGVlxR4Ph8NMmDCBnJwc3G43I0aMYMWKFS14xkIIIcThafbs2Zx11lnk5OSgKAoffvjhPveZNWsWAwYMwOVy0alTJ1544YXd2rz33nv07NkTp9NJz549+eCDDw7B2TfU4iMvRx55JAUFBbFl2bJlsceeeOIJJk6cyDPPPMOCBQvIysri5JNPpqqqqgXPWAghhDj81NTU0LdvX5555pn9ar9x40ZOP/10jjvuOBYvXsy9997LbbfdxnvvvRdrM3fuXC6++GJGjx7N0qVLGT16NBdddBHz588/VC8DAKUlJ2acMGECH374IUuWLNntsXA4TE5ODmPHjuXuu+8GIBAIkJmZyeOPP84NN9zwPz5bIYQQ4rdBURQ++OADzj333D22ufvuu/n4449ZuXJlbNuNN97I0qVLmTt3LgAXX3wxlZWVfP7557E2p556KsnJyUyZMuWQnb9+yI68n9auXUtOTg5Op5NBgwbxyCOP0KlTJzZu3EhhYSGjRo2KtXU6nRx//PHMmTNnj52XQCBAIBCIfW7bNmVlZaSmpqIoyiF/PUIIIQ5f4XCYqqoqcnJyUNVDd3HC7/cTDAabfZxwOLzb3zan04nT6Wz2sefOndvgbzDAKaecwiuvvEIoFMLhcDB37lzuuOOO3dpMmjSp2c+/Ny3aeRk0aBCvv/463bp1Y/v27Tz88MMMHTqUFStWUFhYCEBmZmaDfTIzM9m8efMej/noo4/y4IMPHtLzFkII8duWl5dH27ZtD8mx/X4/7oQUMH3NPpbH46G6urrBtgceeIAJEyY0+9iFhYWN/g02TZOSkhKys7P32Cb6N/xQadF7Xk477TTOP/98evfuzciRI/nss88AeO2112Jtdu1RNtbLrG/8+PFUVFTEli1btgDwyteLGEMueu/LKMrbiN77Mq4ml20FhZSsX47R72rKFnzOk18v47W07oyc+BXv5h7J9+eO5B5nR/Tel1G4fTveE+7m25HD6XrTFG5S21H85b+5XW/PwPs/Qu99Gcs2buOGN+ZQ8OpfSDz+T9yut+ftH9ey4k+XU7bgc25Q2vGv5K44B4yh9Pv3STntIZ7+ehmJx/+J4s9f4bW07pwyaTpXk0tR/lYWjD6dwrf/xp1GB25Q2nHU3e/x6VEDcQ+6CeeAMXyzfBNdb5qy8xgbfmHVfdeQftZjvJEeeR0zV2zmBqUd5Uu+5efbf8+Pl57KlIwjKJ7+Ohu3FnA1uTz59TKOeeBjPujal8zf/YPld1xK6cx30Htfxpb8QqbMX0P2hf/kghe+YcD4D7ht6jxSz/grny/ZyLaCQvTel1G+aAbbXhyP94S7+enKM8kd/SoDxn/Ac8ldueSlWTw/cwU3KO2YtzqPq8llDLmUrZjDOEcH/hrfiY1P/IHX0rrz05Vn8tXQIcw5fyTv5PRkkrcL+S/fx59dnSie/jply75nDLkU5W1k3ZZ8riaX+WvyeHfBWm7X2/Pk18sY7+zIY57OXPDCN/wruSvv5PSk3z3v80nvAXS/eSodrn6d+RefQvaF/yT1jL+y7v9uIOmk8Wx94R4K3/4b7kE3RZ5rwec4+l5B2Yo56L0vi/z8bN0cW1+3JR9H3ysw+l3N/DV5OAeMYcayjXy4aD3xQ27l9TmreH7mCrwn3M0T05eSPPLPpJ7xV26bOo+Mc54g6/xJXPTit7S55F+c/vQMTvz7l3S89j8Me+QzBk34hO43T2XAfR9y5Nj/0vuP79L7j+/S7573GXDfh3S/eSqDJnzC0Ic/pdOYNxj++Od0uPp1cke/yimTptPmkn+Rc9FzXPTit2SdP4nLX5nNNa99T8Y5T3DDG3Ni38M/vfsjKac9RPIpD/DAJ4tIHvlnkk4azxPTl+I94W7+MePn2M/X8zNX8K9Zv+AZdjuvfL+Sf/+wKvY644fcinvQTbz941rcg27CPegmPly0HtfAG/h40Xo+W7wB18Ab+HzJRr76eWPsa+UcMAbngDHMXLEZo9/VGP2u5odVWzD6Xc2c1VuYtzov9vVdsDYPR98rWLA2j5/WRdYXrduKo+8VkfX1O9eXbIisL92wlWUbt8V+P5fXrUc/6r0v45d66ys35aP3vozVm/NZvXnn+potkfU1W3aur6v7qPe+jPV5Bbutr88rYOPWyPrGrQ3Xo20bW9+8rYDN2xpf35JfGPu9jO7X2PrWgkK2FjS+Hm1bfz36e7ytoHC/1nfdT+99GQWF22Mf97S+a9v93W9fxyjcvj32cU/ru7ZtbD+t50UAJCQkHOw/eTHBYBBMH3rPi2Ln0aSl50VUV1eTl5fX4O/e+PHjD9q5NvY3eNftB/p3+mBo8ctG9cXHx9O7d2/Wrl0buw5XWFhIdnZ2rE1RUdFuvbz69jRcFufxYCgqimaQmJiIohkYqCQmJuLUApHtnnjcVgJxqobDHU+cquFx6Djr76c78eg6mjMOp6KSGB/5qLviUTSDhIREnHEeEnUXim7hVFTiPAkkOCPHNxQVt6rFnk91uHB7ElB0J4nxcbHnjp6bx3CQGOfGqagYCmjOeOI1HUV3ooQVPAmJaM64ncdISCDoMlAdrtixPAmJGIpKYoKHBKeBbjiIUzUS4+NQEhMxUHHHJ6C74onXNFSHu+5842KvO84TRnW4cbg9aC4TZ5wH1eEiPiGBxMS6dgnx6HGR1+0xHKh2HJorHreiYcR5cHsSMBQ1cj6oaAokJnhwKiouRSPB7Yx8zQ0HAV3H44icp1vRSIxz4ar7emsJke9lYmIi4bATg8gx4ywHTiXyWlx1x3S4PbhVjThVQ6t7fZozDtXS8ZgOVMuN6giS4HKi6C4S3TZ6nDv2/dA88XWvzYOiGQCxnx+AhLp1RdXwJETW4xMSCQctFN1JnCcBu27dHZ+A4nChOlx1Xz83qm5guD2ohhuHOx5Ts1CNOHRXPKg2mjOM7nKjmUFULfJmoBkamqaiOSM/d5quoBpBdFc8qlFDWDVxuONRDTdhy4oc3+HGiPNg6Cqqw40zzoNTV+udi4uwbeGK96A4XGBbkfOtO2+HqkTWPQmoihJ7bfXXFd2JYluxdYB4TwKKbhCfkICmKDvXVWJfq+jX0tPIuichERVl57pCbF2pvx79fjSynpCQGDnPuvXofgmNfB/rryckJgL12tbbr8H2fRxDbWS/+j9Dja0n7tK2wX51r6VZx2jifgfl/H/1z+2IfPwf3GagOFyx82iKsKoBkdcQPf+DKSsra7cRlKKiInRdJzU1da9t9vZ3+mBo8bRRfYFAgJUrV5KdnU3Hjh3Jyspi+vTpsceDwSCzZs1i6NChLXiWQgghRPMpqtbs5VAaMmRIg7/BAF999RUDBw7E4XDstc2h/jvdoiMvf/rTnzjrrLNo164dRUVFPPzww1RWVnLllVeiKApjx47lkUceoWvXrnTt2pVHHnmEuLg4Lr300pY8bSGEEKLZmt0BCR/YvtXV1axbty72+caNG1myZAkpKSm0a9eO8ePHs23bNl5//XUgkix65plnGDduHNdddx1z587llVdeaZAiuv322xk+fDiPP/4455xzDh999BEzZszg+++/b/rr2g8t2nnZunUrl1xyCSUlJaSnpzN48GDmzZtH+/btAbjrrrvw+XzcfPPNlJeXM2jQIL766qtDei1SCCGE+C366aefOOGEE2Kfjxs3DoArr7ySyZMnU1BQELtPFKBjx45MmzaNO+64g2effZacnByefvppzj///FiboUOHMnXqVP785z9z//3307lzZ95++20GDRp0SF9Li3Zepk6dutfHFUVhwoQJB+WuaSGEEOLXRFGaOfJiH9i+I0aMYG+l3SZPnrzbtuOPP55Fixbt9bgXXHABF1xwwQGdS3P9qm7YFUIIIVoLRVNRtOZcNvpV3bb6P9VqXnnbh2/k8hHtGXL5FfQaP4shl1/BmNM6M+/o4Rz7/GrSjxjMRXMMRm95m2+La/j4rGRGzXyNF99dyQ1/OYX2Q8/itOfn468o4c2vNvD6HcdxyciOfJ16PAA/T/uUdkPO5Kr/LOLxUzrx7Z+mcNQZJ+OzwpzuzOPr5+cw39UTTYHjrj2GsG2xJnMwPYcfzeW90qgtzWflP1/jh9Ja7hrZjSyXjj39ZZZ+vAZ1xOUUByx6JTrZunwVPS4aSKimAm/bbryxcCv9+mUzqksqtaX52D9NY9P0FWR27cqyigDn9GvDkakOMp06vvlfsO2HDbQZfhS/VAag6yB+Kfbh0VVmrS6iKK+CjN4Z1JZuI23gkYTb9UbVDbZUBlmYtwNvZgYbt1bStm0ivdt48Zdvp53XibMqcqd5cMMKdqzJw52cxY6N5XiS4klKi6M4YNE100Ou10XQDpPijvyyGqpCuGQb8ZpKiqFRs62EBK+T2qJqakt8xGUkUuE3qTZttNRsaiwbPTULOy4ZKwxB3U11yEZTYIc/RLkvhKEqlNcGcWsqHl2htDqIR1cxPA6CPhNnopNgwCTkr8VIdGMFfVgBH454N7YZRIv3oMYlYIeCqK54FMNN2LYI667Yz1JY35lmC1rheus2iqoRMG1qQ1Zs3W9GtvuCFqrDQK3brqgaqm4QrFsPmjamaaMoCpZpY1k2qq5iWTaKCpquYlthVEWJrIfDKCqomhqJJqoKth0mbFsYeuRXO2xbaHWFtjRVQVOUum0KmrpzPWxbhK1667aFVpe2UBUFVVUI23bkOPVCGNF1tV4yI7otbFn1timxNpq6s71Wf79670bRzSqNJz72JwhyMLMiuz7f/r5xqgd4Evtqvz8JmAN9TiEORzLyIoQQQrQAtZk37IYPcdro10w6L0IIIUQLaHbaqBV3XlrNZSMhhBBC/DbIyIsQQgjRAmTkpemk8yKEEEK0AEVVUZozc/UhnPX61671vnIhhBBCHJZaTefl1U/WYrz2EV+f6GfrTzP45gyNlNc/5IOVxSz+YApf/u08Pn32ZZ688iWu+113Zgy/lGvn2Byd7Kbkqkd46+4RzJ/6DsdcdCFphk7PH56l/6vPM/bZuVxyVlfClsX91x7Noo+/oOTxsUzbVsm/LuvHabmJrLjvL8wr83H3f3/mlPZJtL39XjKOHMa9n/7C/WcfifXRk8Sn5zLnq434rDCD43YwrIOXFS9+yoJyP9Pz/HgdKv2HtGHHlpWkn3cZjngvbXp2Z9aCPH4/MJc2NRtRVI1tn3/DmuXF9OuTxfaAyYkdU9DX/kDPRIOt3y5iw6oS4gedyPaASaGaxA+by8h06qxZX0ZF/kayBnbBX1GCq/cQyo1UjHgvSwqqWLS5nORMD2UFVfRvn0zPdA/BmgqyXGGUwrVohhv/pvXsWJePOzmDyq2VeNPi6JqZQKVp0TktnjaJkYhxshGJSXt0FXP7FryOuqh0YSlxqW6qt9dQW+YjLiuVipBNtRmJSAftMHZ8CnZcMgDVQZvqoI2mKJT5TMr8IVyqSlFlALem4NZU/L4QLpcei0gb8Q7MkIUV8GEkxGMGfdhmEEdiHLYZQo1PRI1PJGxbqPEJ2IYbANvhjv0sherS0YqqEbTDsaHf2pCFomn468Wj/ZaN34xEpH1BK9bWF7JQdQNFi8Smo5Fpy7LRdJWwDWE7XBdRDqNpaiwKreoqiqJgm5G2sTb1Ys66qhC2rAZRaENTMXQV27Zw6pH12ON1sWatXs5WjcaqFXaLTcfWY/FnBdu2Ytt3HqPh72HYtmjMnrZDvdj0bnFlpcHjjbXZ9XwatG3wHHs+592Ot/eH90pSzAfX/2DuxEPq1z630a+ZXDYSQgghWkDkslFz7nlpNeMPu5HOixBCCNECmj09gNJ6R15ab7dNCCGEEIclGXkRQgghWoKmNWtuo/ABTsz4WyKdFyGEEKIFNPem29Z8w65cNhJCCCHEYaXVdF4mTLqQk67+O48MvZWJT93JxIHXctwfpnDnncfT//xLUf98Bdn9RhIKh8md/AEfbijnv8/9h0s/fpDzHv6Grl/8jbjUHD69aRBX3jmCqbe8wYSlNhu//5gjn32OPmeczeUpxQSqyvjkqe9IMTTaL5rK8Ecv5OOP1pLj0lnx9XcMe+AcPilL5ORTe/Hj9MWMiCth4aRpdBo0hOWVfnokONkx5Rn6Xnss837YSrVp889v1zE4xU2P0Sdgm0HyU3qR2qU/Jx2TS/7KVZzQwYtv9gfEpeawccZ61lQHOadPNlYYOijl7Jj1Jbn9s8j7IY/1NSHM9gPwWWEW5lfx7coiungclGzbga80H+9RR2GbQULZR7Ku3E9cag4/bS4nP6+C7u2TqCoqoHd2IrmJjsgszOVbCG1ehRHvpXxNHhWbK0lIcVNW6iM9LY6uWR6qTZsOSW7S3JGBPq2yEENViNdUggVbSTE04lLcVBVU48mMp7bUR1nQIj4rlYqQhd8OoyZnRqLSccn4wpH/NqqDdmwm6dLaIGXVQdyaQllNZCZpj64S8Jk44g0c8Q5CAROn14npq8YK+iLx6FAQ2wyhxSdgmcG6qHRCJHJsxBN2xEV+gBw7Z5X2m/Vnkt4ZlfabNmpdZDoSf3YQMO1YRNoXtFB3mUla1Y267XUzSZs2mhaZSdqybFQtsl1RFTRNxTZtVFVB0yOxaUVRYhHqaPR519mh60eidXVntDkai244A7USiy3HZoxW60WiFRqsR6m7ZFZj0et6EevorNGqojTYHm4sYr1LDDraZl+zTe/JniLSB7LfgTjYMzvvz2zSTTruITmq2F8SlW46uWwkhBBCtABV1VBleoAmaTUjL0IIIYT4bZCRFyGEEKIFNLdIXbPmRTrMSedFCCGEaAGSNmq61tttE0IIIcRhSUZehBBCiBYgIy9NJ50XIYQQogVI56XpWs1lo3viz8GT2ZEsl85Zn/8VgB15K/n5ysf57rrO/PPVJXzz2On88V+Xc+JfZ3LF8HY4Pck8ZfZn85xPeGncu4z744Xkj70MfdwkFpT7efHFz0ls240HFweZfN0xLL7lTjoeezpLK/ycc2IHvv/jS9Scfgd5vhCnn9Ce6u2b4IK7eeS9Zfx5ZBdK1y2i4Pm/M/PnIm46qweGqjDs2LYsffl7Un5/PSurAuS6Haz6aTO9ftcDY+QVxKfn8uHqYjr2zuXS/m2o3LoG99rvWP/RXNK6HsXPWyooC1oc285LiqFhL/2avG+WkTuiJ2u3VVEcMFlXGcZQFWauLWHbhnJyemdQlb+OYE0Fes/BKKrGtoDGooJKPJm5LN1URtn2avq1T8JXXki31DiSwjUA2HmrqFqzHqc3jR3rtlO5tZKk9HgK/RY9shPpnBKPzwqT7TFIsGsxVAWtshCPrpJiaFRt2Y7XpROfGUfN9hriMjxUVQSoCNk40jPx22F8lo0Vl0zQDmO5k6gM2gBUBixKaoOxOi9FlQHcmkppdQCPruJMMAj4QjgTDVyJTkJ+P0aCCyvowwz6MBLisII+bDOIEpdI2LJQ4xJQ3AkAhA03YYcz8jod7tjPUv3aLkErUvNF0TRqQ5HaLQHLJmDW1XYJWbE6L5HaL0ZkP9NCdUTWTdNG1VRsy8a2wpG6LaYdq+kSDofRNDWyPRxG1SK1XcLhMJquYuhqpOZOg9ouWr16LUqshku0TWzd2qW2S12BkrAdqT0Ttm00RUFTdm6LrSuRY9i21aDmi9ZIkRNNrV8fpl7dmP14B4oebtdSJ42VPqm/qX4NmabY9fiNneqB1F/ZU8t91YQ5VDVeDpZda/yI/RedmLHJi0zMKIQQQghxeJDLRkIIIUQLUJo5MWNz9j3cSedFCCGEaAFS56XpWu8rF0IIIcRhSUZehBBCiBYgaaOmk86LEEII0QKk89J0reay0X+ffpEVL17K1as+5+GHpjPuxxd5/G+3ceXY5/m075mc3iaRmj9czIe9xrDyy3fp//nnPHD/5Tzy1//QecS55PtD3JOdz6uTl3D+8/M5/4hUdmxazu+vOp2XX/6KdrNf4L8zNvL4dcdwdLKLfhMn8NnqEv74yUoGp7g56q934m3Xg0e+3cja72bTZuVn6G4P81+aR77f5PJeaQxPi+PIG8/h+w3lLFfaoCkKQ7unULJmAe0vv4glvgSyjhzAlFkbGX1cR3q6a7HNIEWffMCG2VvodGQGeb4QhqqQUrqKbh6DgukzyZufT/JxI9hUG8QKw/dbykkzNH5aXUz5tnyyB3bAV76dsG3hS+mE7vawvKiGOWtLSM1KoGRbFTXFW+idkUCgqpy2CQ70kg2oukFwwwrK1+QRl9qGHZsr2F4VpENmAuUhi66ZHtonuQBIdWtolQV4dJXQ1vUk1kWlq7cVE58ZjyczntoSH/HZqZQFLapNGy01i2rTJmiHseOSAagKWFQHbQxVoaQ2SEltCJeqUFwZoKwmgEdXqKoJ4jY0jHgHQZ+J0+vEmejECvgwEuMxgz7sUBAjMR7bDGHbFmpCEmHbQolPjESkbYuw7iTsiAPArPtVicSj7dibTm0oEoNWVS0Wjw6YNv66Nr5gJCqt6Qa+aFuHQW1dfFrTVSzLjnw0w1imHdtm22HUuu2KqqBpKmE7HIkr163Xjz87dZWwZcWi0NHYdDS6rNe1BWL7RdchEn/W6sWLtboIrKooqOrO9ShNVXaLyYYtq0H0d29x5V231z9UdH3XGLHaSOC4sahx/WPXP279N7z6EeR9xZV3dTAi0gfTgZ6/EIc7GXkRQgghWoCq7vzHoGkHaL29Vum8CCGEEC1AUSMFJ5uzf2vVai4bCSGEEOK3QUZehBBCiBagKEqzpn/4tU8dcSjJyIsQQgjRApS6e16aujT1stFzzz1Hx44dcblcDBgwgO+++26Pba+66qpYJ6v+cuSRR8baTJ48udE2fr+/See3P6TzIoQQQrQARVFi9700aWnCyMvbb7/N2LFjue+++1i8eDHHHXccp512Glu2bGm0/VNPPUVBQUFsycvLIyUlhQsvvLBBu8TExAbtCgoKcLlcTfq67I9W03k57YZrmNXlaPr9bQVXj+zISV8qXLLwOTSnm2+Laxm16FOenfoLY//8Gt1OOo/j/z6HGwPfY/pr+HD8CK65sAdfnHIbhqqw6IP3OPG/T9B5xLlMPDGDsg1LmXbrawTtMGfYKzj9zpOYoXTHUBWmf/ADo+4YweLUQfQ9eQhTP/qF2tJ8fn70JdodfQKzS2rJdTuwPnqSoy7vD6Oup9Bv8o9v19E/yUXvq48lVFNBbd8zeWneZgYd3ZaNi1dzRrc0rO/fxZ2cxbpPlrK0IsClg9rhs8LkuHRqv/uYrj3S2DJzLcsqAnDEMCpCNh5d5asVhXTxGGzfsoPq7ZtIH9IP01+NqhusLw8Ql5rDj5vLWb9pBx3aeanYXoK/fDudk12Y/mpcFVsJrvsZI95L+crNlK0rJSHFw46Cagr9Jke2SaTatOmcHEdmXOTKpLOmmHDRFuI1lVDBJlIMDW+Ki+qCKjwZccRnJVHhN/G0SaciZFNj2ejpbSIzSYchqEdmda4O2ZTWhtAUKK0NUlIdmUm6qCpAaXUQj64S8Jk4EyPx6GAgsm4kurGCPpxJHqxAZCZpLd5D2LawQ0HUuIRIvNYZj10Xjw4bcdiOyC9foG72aICgXTeTdF0sWtG0neuqSm3dTNKqw8AXtOq2R2LTO2eVttF0HU1TsUwbRVWwLBvLslFUCNthbCtcFzWOzB6tqGDVzUCtqAq2Hcaom1W6sdmho1Fpp65i2xaGru4+q7TdcL9o9Dls2/VmgabRdbVutunoethqGLGOnAf12tbbr972qF1j0NFzanT2aOXAghYH8kbX2PPtz/4H+97JQ3VJ4HC/0NCKr5QcFBMnTuTaa69lzJgx9OjRg0mTJpGbm8vzzz/faHuv10tWVlZs+emnnygvL+fqq69u0E5RlAbtsrKyDunraDWdFyGEEOLXpFmjLvUuG1VWVjZYAoFAo88XDAZZuHAho0aNarB91KhRzJkzZ7/O+ZVXXmHkyJG0b9++wfbq6mrat29P27ZtOfPMM1m8eHETviL7TzovQgghRAtQFaXZC0Bubi5erze2PProo40+X0lJCZZlkZmZ2WB7ZmYmhYWF+zzfgoICPv/8c8aMGdNg+xFHHMHkyZP5+OOPmTJlCi6Xi2HDhrF27domfmX2TdJGQgghxGEsLy+PxMTE2OdOp3Ov7Xe9JBkOh/frMuXkyZNJSkri3HPPbbB98ODBDB48OPb5sGHD6N+/P//85z95+umn9+MVHDjpvAghhBAt4GAVqUtMTGzQedmTtLQ0NE3bbZSlqKhot9GYXYXDYV599VVGjx6NYRh7bauqKkcfffQhHXmRy0ZCCCFECzhY97zsL8MwGDBgANOnT2+wffr06QwdOnSv+86aNYt169Zx7bXX7vN5wuEwS5YsITs7+4DO70DIyIsQQgjRSowbN47Ro0czcOBAhgwZwosvvsiWLVu48cYbARg/fjzbtm3j9ddfb7DfK6+8wqBBg+jVq9dux3zwwQcZPHgwXbt2pbKykqeffpolS5bw7LPPHrLXIZ0XIYQQogU0d2LGcBP2vfjiiyktLeWhhx6ioKCAXr16MW3atFh6qKCgYLeaLxUVFbz33ns89dRTjR5zx44dXH/99RQWFuL1eunXrx+zZ8/mmGOOOfAXtZ9azWWjF4wZzCvzsW7Wp3he/4g5r7/Gw2Pf5eN/XsfdfxnFiFfWcf4RqYRqK5lx/wks/mAKb1zwCJfcejnxz/6RLv9+n0+2VnLNrUNxJiTzYnVnXvzDMNbdfh25g85gRlENvxuQzbwbJ+Ad+w/ueX0hZw7IpmzDUlJvf5Rxby/hqfP7ULB4BkkdejFj1hauObcnACP6Z7Fw0jTa3/QHpiwvIsulM+f7zQw8tRNJ54/BnZzF+6tK+G7uFq4d3J7yTcvJLFrCpg9mkNKlPytWl1HoNxnZKQWvQ6V3iptNn/9I+5OOYNWaMrYHTDaH4tEUyHU7WL+ulPbdU9mRtwZ/RTFGn+EAuLxpzN9aQWJ2J+auLaG0sIpBnVOpKd5CsKaCdD0IQHjrSqrXrMbpTaNsTT4VmytJSo9jm8+k0rTpkhqPz7LJ9TrxqiEMVUGryI/Vd6nasp00p058RjxVBdV4chLwtEmnLGjhzMqixrLxWTZ2fGqspkqFP1LzozJgUVIbxK2pFNcGKaoM4NFVSqsD1NYEcccbBH0hDI8DZ6KTUCAYqfmSlIAZ9GEkxmGbQWwzhJqQjBUKErYt1PjESF0Uw03YiNSUCTvcBOvquwStMIoaqecSW9c0akMWmm6g6g5qQ5E6Lr66Oi+KqlEbtPDVbQ+akdovkfV69VqsMJqmYpt2pKaLpmJZNrYdRtUj66qmoOkq4XAYVVUitV3qarTou9RrCdsWhhZ53K63XYvuZ+2s+QLEPoZtC02J1HgB6tat2Jtr2LbqarQokePWr//SyJtotH3kWEq9tg3ruzSmsfdkFaXxmi8N9lP2Wh8msn3/3/Abe4M8kP331PJwn09vX9+/Q+G3WN9FUZu/NMXNN9/Mpk2bCAQCLFy4kOHDh8cemzx5MjNnzmzQ3uv1Ultby3XXXdfo8Z588kk2b95MIBCgqKiIL7/8kiFDhjTt5PZTq+m8CCGEEOK3QS4bCSGEEC1AJmZsOum8CCGEEC1AVWnmPS8H8WQOM9J5EUIIIVrAwarz0hq14n6bEEIIIQ5HMvIihBBCtABFaebISyu+56XVjLw8cP0bTJj5BPc+cgfDr/0nQy6/guFpcaQ+ch0LL3yQBW+/wfHzvuRP911D8Y0X0m7ImSytCPDCQJN/Pf4Npz0/n3Pae3Hf/wLnXnMe//fk5wz8ZSpT3vmFv940mL5eF0Nf+AvvztvK2E9Wsfqbzxk86U4S23bj8XnbWT59Jt22fIOqG/Q56Rg21Ya45Zi2DEt10++Oc5j5cxEr447gxS/WMLxrCoXLv6frdZfyi5JD1pHH8OrX6ylcsZBBKRZW0Efxh1NZ+8UGOvfJYk11AE2BrIq1dPMYdDyhPZtn5ZEx8iTW1wQJ2mFmbSoj3anTPT2O4o1baTOsE7Ul+YRtC19GdxzxXuLT2/H92mJSshMo2lpJZcEmjspOxF9Rgm0GcRSvi8R81/1M+crNxKW2oWxtGYU7/HTITqAkaFFt2nRJjcMKQ0acjlaxDbemENq6ntpNm0h3alRv2U58ZhwJOR5qttfgaZNOfJt0KkI2WnobfJZN0A5jxafGvn+VQRtNge3VQUprg7hUhcIdfoqr/Hh0hYrqIP7aEM5Eg4DPxJXswp3swvRV40xKwEiMwwr4cCYlYAX9WGYQNT4hFi0OO9w7PzriAAgpOgHTRlE1/HUfFVXDH4qsq2okKh3bbkW2+4KRqLQWi02bqA6D2rr4tKarmKFIBFnTVCzTRquLRFumHYlHm2Fs00bTVMJ2ONY2bIfR62LOYdvCqasYeuTX2NC12OupH12uH4nedT0Sj94ZL24Qi65bVxUFTdl5rPoxWU1VCFtWg+hv/bjyrnbdXv+9N7pev42iRCLSu2rs/X5Pz1n/Ta7+m/2B/s04GBHpg/lcB3z+TTyXhs/Zev9YHgoHa2LG1qjVdF6EEEII8dsgl42EEEKIltDMG3YP+2qHzSCdFyGEEKIFSNqo6eSykRBCCCEOKzLyIoQQQrSA5k7M2Jx9D3fSeRFCCCFagEwP0HRy2UgIIYQQh5VW03m55KQOnDwnlZtXvojqMPjmNDhj+Zc8+dIirhr3It1PPp9hkxZxtzKHF97+hc8nnMz1v+/JtGOvRlMU5k99h1HTnuLcf87lxVOzKFmzgM+ufY5q0+YCVnDevScz3d0PQ1X4+O1ZhG2LJdnHM+C043j17SXUFOex9MF/0nHIyfz9d73IdTuw33+CQVcfDaffSr7f5LGv17Bu/k/0vf54QjUV+Ab+jqdmb+DYYe3ZsGgVNcV5WLOn4k7OYvV7C1m0w8/oIe3xWWFy3Q5qvn2fHr0zaH/aIJbu8EPvE6kI2Xh0lU9/LqBHgkHbwTlUb99E5rABmP5qVN1gXXmAuNQcktvksGZDOV07JlO+rRB/+Xa6pbgx/dUABNcuwYj3UvrzekpWF5OY5qU0v5pCv0mf3CSqzUh9lsy4yICes6YYtm8iUdcIbVtPVV4R3hQXlVsrSMj2EJ+VRJkvhKdNOnp6G2osGz29DT4rjBUGv+YGQFOgtDaEoSqU1gbZXhXArakUVQUoqgzgdWj4a0MEfSbORCcBXwhnohMj0Y0V9OFM8sTqu2jxHsK2hR0KonqSYnVRbKcHgLARh+1wARCwwvitcOS125GPSrS2ixap7RKpA6PG6ruoDgNf0IrVdPEFLYJ1NWKCpo2m67HaLqquRmq6WDaKCmE7jG2F6+qkhLHDYRSVSFtNRVEVbDuMUVfbxQ4F6+q1qLHaLtHX49RV7Ho1X6If668bWuTXP1orImzbONSdbwnR7Zqyc71+zZcG9V5ix7DQ6r2raPX3U3fuF9111xou0Votjf1DqSgNwxX7+p/zQN7cGnu+/dn/QEft99X+UNR3gYNT46Ul/VYHGBS1+UtrJZeNhBBCiBYg97w0nXRehBBCiBYgUemm+9UMOj366KMoisLYsWNj28LhMBMmTCAnJwe3282IESNYsWJFy52kEEIIIVrcr6LzsmDBAl588UX69OnTYPsTTzzBxIkTeeaZZ1iwYAFZWVmcfPLJVFVVtdCZCiGEEAdHNG3UnKW1avHOS3V1NZdddhkvvfQSycnJse3hcJhJkyZx3333cd5559GrVy9ee+01amtreeutt1rwjIUQQojmi97z0pyltWrxzsstt9zCGWecwciRIxts37hxI4WFhYwaNSq2zel0cvzxxzNnzpw9Hi8QCFBZWdlgEUIIIcRvR4t2XqZOncqiRYt49NFHd3ussLAQgMzMzAbbMzMzY4815tFHH8Xr9caW3NxcAKr+Npm5b7zOA2Pf4/uXbmDiMdcz6MllXDa4DWHbYv6DJ7H0w6m8cN5jDE+LQ3noWrJf+C+fFlRxw19OIS41h8cKcljy0busuOYqOg0/hxlFNfz+hA7MHv1n9Jse586XF3D+iR0o27CUTsNO5Q+vL+TZC/tQsHgGqV36M+3bzdx6UW96ly9k5LC2/Pj4p+TePI5XlxSQ63bw3cz1VG5dQ+KFNxOfnsuU5UV8/8Nmbh7WkfJNy9EMN+umfEH6EQNZvLKUQr/JaV1SSDE0jsrysOHT+XQ8tTeuoWeR7w+xzu9CU6BDnIP1a0pp1yudtsf3xV9RjNHvBBRVw52cydy8HXjbdCaznZeS/EqGdk2jpngLgaoyMtRaAHSXh8rlK3AlZ1K2Jp8dG3aQkhnPNp9Jeciie7oHn2UD4FUCGKqCVp5HKG8N6U6Nyo0FVG0pJj4jnqqCahLaeklol0lZ0MaZlYWe1Q6fZWN50mOx5IpAJDZrqApFNZF4dGF1gIIdfjy6SlGlH39NCCPOgb8mSMAXwp3sIujz4Upy40xKIOSvxpnkwfAmYJtB1IRkrFCQsG3FotIAYSMSyw473ATMyOvwm2GCVrguEh35qGiRqLSmG6i6g9qQhaobqA6Dar8ZiVIHLXx124NmJDat6gaBoLUz8myF0TQVTVewTTsSn7bsWITasuzIdl0lHA6j1cWjozFnvS7ybOhabLuhRT7au8SmDV0lbEU+anX/qWmqUhfJttCUSEwaqFu3GqxHYs5K5Lj1YtORY+z+O1g/Qr2zbcPtu4o8TyPHQmk8Nr3L8+0tYh3ZXv+c9ngadc+59/335xiNHvcw/yd5b9+/Q+W3flVEUZTYTbtNWn7rX6C9aLHOS15eHrfffjtvvPEGLpdrj+12/eaEw+G9fsPGjx9PRUVFbMnLyzto5yyEEEIcLNHaS81ZWqsWi0ovXLiQoqIiBgwYENtmWRazZ8/mmWeeYfXq1UBkBCY7OzvWpqioaLfRmPqcTidOp/PQnbgQQgghWlSLjbycdNJJLFu2jCVLlsSWgQMHctlll7FkyRI6depEVlYW06dPj+0TDAaZNWsWQ4cObanTFkIIIQ4KtZmjLq35ht0WG3lJSEigV69eDbbFx8eTmpoa2z527FgeeeQRunbtSteuXXnkkUeIi4vj0ksvbYlTFkIIIQ6a5l76saXz8ut011134fP5uPnmmykvL2fQoEF89dVXJCQktPSpCSGEEKKF/Ko6LzNnzmzwuaIoTJgwgQkTJrTI+QghhBCHioy8NF2L13n5X7nkpr9z7X23c2H/bEovPBOAFdP+S9cvvuJff7+Bn0acSO8zLyLfH+KCmc/x7AsLGPnYLC4b3Iayqx5l3B8vZOI//osnqwOvf7qWV+84lhPS4xj48iTeW17E6DeXsG7Wpwx89jFSu/TngasG8MuM6bRd8AbOhBROOGsQ+X6Ta4+I45e//oOjxl/NjNWlzLHa8vInKxnRP4vty2ajuzzMqU6g7VHH8OqXayhYNpc+RjlW0Edyh1788vUm+g5sw/qaIIaqkLZ9Cb0SnXQe1Zl1328lfdTpFCd1wQrD9PUl5LgcHNE2kZKNG2l3/BEkDD2JsG1RkdQZI96LJ7MjM37ZTnpbL0O6plFZsIGBOV78FSWRGG3hajTDjcubRumKjcSnt6NkVRn5lQG65yZREjSpNm26pMRhhSPxWr08D4+uEtqyhuqNWyJR6U0FVOZVkdg2ger8ajxt0onPyaAiZKFntgNvBkE7jB2fGvueVQVsDFXBUBVKakO4VIXCHX6Kq/x4HSpVNUH8tUFcyS6CPpOAL4Qr2YXpq8aZnIAzyYMdCuJMTkBNSMIKBVETkrDNSFQ6bLh3RqUdcQAEFR1/XTw6aNmxiHRtKDJLtFpvXVE1aqLb9chs0ppu4AtZ+IImqsOIxKaDFlpd/FnT1dis0pquoiiRmaJVXcUyw4TtSIQ6bIex661rqhKZSdoM4twlNl1/pmigwUzSQIN4dGy/erNDO+qmew7bVuwaejQiHd2vwazSqkLYikSb688aHX2+Xe26XVF2zia9p+DgrrNNR55j18/3/JwNn2//Y9J723efbQ/s0E1ywOd/UJ6zZf5AtoYUsKSNmu5XNfIihBBCtBa6CnozOiDhVjP8sLtW/NKFEEIIcTiSkRchhBCiBcg9L00nnRchhBCiBajN7LxYrbjzIpeNhBBCCHFYkZEXIYQQogVoioqmNn0MQVNa7/hD633lQgghRAtqqaj0c889R8eOHXG5XAwYMIDvvvtuj21nzpwZmf16l2XVqlUN2r333nv07NkTp9NJz549+eCDD5p0bvur1XRe2hx1HI9V/Jf2X3zFm99tYdyPLzLyhjEM/uOnHPfZX3nrx3zm3D2EPz5yFrf+kkyPBCcrpv2XQZ9/wHmPfMM92fnUluZzy63nkuzQOGrhvzn7xet4aouHXLeDOe9Ow4j3MrUyh4suPZ7zE4sIVJUx555/0+PEkTxxZg/6el2UPv8g0z9dR/FR51EWtPjLxyvY9OMc+t1xDrYZJOPIYTw+fQ3nndSZjT8twl9RTO2nr5CQ3ZlO/Y9g0Q4/Y4Z2IGiH6eYxKPn4HboPbkPuWSextMKP2fNEZm+uIMXQ+HjhNvomuWg/vD3V2zeRdtwwzI7HoLs8LCuqxZPZgbR2mWxYX0bvLqkM7pBCbWk+nZNdWEEfiqoRWDEfI95bV9+llKT0eIqKaij0W/TJ9VJt2lhhyIqPDOJ5dBUrfx3JDg3/5vVUbirAmxFP5dZKqgqqSWyXQUnAJKFdJlpmO2osGzWjHVZCJlYYajCASL2Y7TUBDFXBrank7/Dh0VWKqgKUVAZIdGj4qoIEfCbuZBcBfyhS3yXJgxX04Uzy4ExOwAr60RKSUBOSI/VNEpIiNV5sC9vwxH4+LD0ys7nfDBMwwwAErDB+047VedEcBqpuUBuyUXVHrLaL6jAi9V+CVmxb/XXLtHfWdtHUSE0Xy0bVFDRdxbYi9Vxs066rBaNgmTa2ZaPrKpZpYuhqrL5KpF6LRti2MHQVQ4vUbnHqKnZd3RNDr6vdYu2s+aKpClpdbRRH3bwoYdtuUMejfu2U6BujWq8mTORz6r5HSqy9Vu+dJFr7pf52VVFidTtUlAY1PKLP11hdD0VpWNuksbfq+vupDbbv/Y29qXVEDvTvxb7a708tmab8jTqca7yIQ+ftt99m7Nix3HfffSxevJjjjjuO0047jS1btux1v9WrV1NQUBBbunbtGnts7ty5XHzxxYwePZqlS5cyevRoLrroIubPn3/IXker6bwIIYQQvyYtMfIyceJErr32WsaMGUOPHj2YNGkSubm5PP/883vdLyMjg6ysrNiiaVrssUmTJnHyySczfvx4jjjiCMaPH89JJ53EpEmTDvj89pd0XoQQQogW8L/uvASDQRYuXMioUaMabB81ahRz5szZ6779+vUjOzubk046iW+//bbBY3Pnzt3tmKeccso+j9kccsOuEEIIcRirrKxs8LnT6cTpdO7WrqSkBMuyyMzMbLA9MzOTwsLCRo+dnZ3Niy++yIABAwgEAvznP//hpJNOYubMmQwfPhyAwsLCAzrmwSCdFyGEEKIFROYYa/p9RdF9c3NzG2x/4IEH9jqh8a73WYXD4T3ee9W9e3e6d+8e+3zIkCHk5eXx97//PdZ5OdBjHgzSeRFCCCFaQHOL1EUnUs3LyyMxMTG2vbFRF4C0tDQ0TdttRKSoqGi3kZO9GTx4MG+88Ubs86ysrGYf80DJPS9CCCFECzhY97wkJiY2WPbUeTEMgwEDBjB9+vQG26dPn87QoUP3+7wXL15MdnZ27PMhQ4bsdsyvvvrqgI55oFpN52X+Hd2497o3GHLDq/z5r2dw0pcKHw+pZvuy2Txx/zSuOaUTPww9mYVn3svrkyZz1acPkTvoDE6ZvIqN33/MF6fcxqDfX8QDXau54rZhvHP9KyztdyX/eO5rfn/rUHzl2+l31mk88PICHj+lE8vvGk+7waczbWUJf7+8P6nfv8rJl/dh9pPfsqY6yF+/WU//JBcrv1tMbWk+nH4ryR16MejYjvz83UquPbotlVvX4PKms+L12bTt04/RIzpREbI5oV0knt3niFTWfriIrucNQT3mLIoDFgsKavlwyTZ6JBhsWV1C7rFtaTNyMIGqMtTeI1hTaROXlsOsDaUktW1Hp84plG4r4bguafTOiCdUU0GSbzsARryXsp9XE5eagzczjZKNO8hok8g2n0l5yKJnugefFYkVewJluDWFRF0jtGkl6U6NinXbqNxchifHQ2VeFWWVARLaZVIWtDDatMeR0wGfFcb2pBEwEgDYEbDQFDBUhcKqAG5NIV5TKajw43WoFOzwUVsdxOk18NeG8NcEcSW7CNbWYPqrcSYlEPJV40pKwJGUhG0GUT1JqJ5IRFqJ88aiuWFnfOznw2/aAARMG78ViUf7TZvqoImiaVQHLRRVQ1FVakORddVhUO030fRIhNoXisajTXxBC03XMU0bM2Sh6ipmyEbVVTRdwTZtNF3dGaHWVexwmLAdRtVUwuEwth2OxZwNXcWpq3Xr2s74sxJ5A7PtnZHoaCw6bFmxfcO2haGpseirqu4crtaUnXHlhpHouuMqdRFqa2fcOvp4/Sh0/f12XVeUSES6vrBt7TEC3Ghsut76gUZ4G8StG9m1sTfCXYe8m/IP8sGo3n6gxzjcw82Szj60xo0bx8svv8yrr77KypUrueOOO9iyZQs33ngjAOPHj+eKK66ItZ80aRIffvgha9euZcWKFYwfP5733nuPW2+9Ndbm9ttv56uvvuLxxx9n1apVPP7448yYMYOxY8cestchl42EEEKIFqCrCvr/eG6jiy++mNLSUh566CEKCgro1asX06ZNo3379gAUFBQ0qPkSDAb505/+xLZt23C73Rx55JF89tlnnH766bE2Q4cOZerUqfz5z3/m/vvvp3Pnzrz99tsMGjSoya9tX6TzIoQQQrSA5s4q3dR9b775Zm6++eZGH5s8eXKDz++66y7uuuuufR7zggsu4IILLmjS+TRFq7lsJIQQQojfBhl5EUIIIVpAS428/BZI50UIIYRoAdEb7puzf2sll42EEEIIcViRkRchhBCiBRysInWtUasZeXm2/0VcOjSXQFUZ7wz/I3Nef42nj72Vux76A4NT3HR851PeWVbElePfxJ2cyWO1ffn4gZOZ++ZbdBp+Dp9sreSLG49h5rk3E3/f88wr83HtUz9QvGoeaQ+8QNcTzuHfVw5gy7xplDw+lo8+WcufruiPFQ4ztHYJP4z/Dx3vvp/ZJbXkuh1Mm7aC4y/sSdmGpcSn5/LqkgK6D+3LXSd1o2TNAnLy5qAZbjJ6DmLB4u2cdXxHfndEGimGhjLnHQZke+h6bj9+Xl5M/Inns85MxFAV3l2az7LlRXTpk0HZxl9oP7IfzmNOQVE1tirJzNpURlJuN75ZXkhOx2RO6JFBVf46jm6TSLZWC0B4wyJ0lwd3ag7FSzeTmNWW1GwPW2pN+rdPpiRo4rNsOia5gEhNFr1sM16HFqnvsn4b6XEOKjZtZ8fmCrztkthRUktxwMKVm0ulaePI6YCVmEXQDmN50in3R+qMlPssDFXBrakUVgeI11Q8eqS+i9ehUVMVjNR2SXIR8IUI+ny4ktyYvmpC/mpcqYlYQR/OFC9qQjJWKIjmTUVNTCFsW9j1arvYxs51nxmO1HaxwgTq1qsCFrUhC003qA1ZqLoDVTeoCZqoDgNF1agNRmq7ROu7qA6D2qBFIGihaipmyMK2wmiaWlfbRYnUdrFsNE1F1SPrqhap/WJZkZovlmk3qNGiqwqGrsW2GZraSP2XyMewtXMddl4XVxWlrqaLjaYoOOrqwkTfACN1VyLr0fousf1ix9j5O6WpO+ut1B++1tSddWMaoyg7j1N/1HvXOjDR59u1vkv02Ps7Yr6vGi+7n1/j59Fo2/14zv19jl+TA62jc7D8yr8sB11LzCr9W9FqOi9CCCGE+G2Qy0ZCCCFEC5C0UdNJ50UIIYRoAZravA6I1oqvnUjnRQghhGgBMvLSdK243yaEEEKIw5GMvAghhBAtQEZemq7VjLykOTWS/vsZX708lnvG/YMhl1+BzwpzV9m7nL/kA4697ytuPK871YWb+MeEy/jHo6/hfeGPpHTqy0f3ncA57b2sveZ83llWxHn/ms+5nZJZN+tjsvqewOVvLWXSDYPInP4Ucak5fPLUd+T7Ta5pb3J61xQW3/UIX6wp5fPqDDy6yikj2rF92Wy63nUXjngvnQYN4eVPVvLH047gKPII2xabX32VtG5Hc+yxHVhTHeDKAW1J2fQDRye72PTOJ3Q7uwcZ51zImuoA2xI68/HK7eS6HfywJJ/ta1fT6dQ+1BTnkTjiDEoTO+FMSOH7LRV8uayQrPbJFG7awYlHZjKkbTK+8u10SNBQNy9BM9zULluIOzmThMz2lKwuJa1NIr07pFASNOnb1ku1aWOFITtOxVAVvA6V4IYVpBkaWS6divXbSGybwI5NFVQV1pDYMZvigEVFyMKR3YFq0yacnIOdkAFAecBmhz8SkS6oDmCoCvGaytYyHx5dxetQqawKEO/W8VUH8NeGcCdHotKmrxp3qpeQvxor4MOZnIBthlC9qWjeVGwziJqQhO2Mr4tKJ8R+JkJ1P/6KqhGw7MhHMxyJRTsi8eiqoBWJRIciH1WHQbXfRNUNNN2gOhBZj0akI5Fpq17kOYxl1q3Xj0ebYVRdrRehjjwetsNodRFm2wxGYtB10efodkNTMXQV27Zw6mosFq2pCmGrYTw6bFs41LrHFXDUXSRXFSUWka4fP9ZUBbtuPRaDVpVY9FdTlAbtd7Xr9mj0VUXZub6n2HH92HQjberHdxtGrOsfQ6nXvvHnaWy/A/W/+JPRUn+XWiIm3doi0lHqXiLQ+7NInRchhBBCiMOEXDYSQgghWoCmKM2an6g1z20knRchhBCiBah1l2Gbs39rJZeNhBBCCHFYkZEXIYQQogVoEJtDrKn7t1bSeRFCCCFagNrMxJCkjVqB83+ZxXFXT0K95SLaDx7FN6fBn6ZN4KErX+X417ex8fuPSXnpPa750xgu3DAFRVV57rFvePb+c4l/9o+MmvYUr/53JYNT3Cx8/wNGvvsoiW27cd9Nw5k59VOGF33LV3dMYch5p7C0ws/IjHg2/OVOjn38Sj79djNBO8yD7yzl1CPT6XPfDSiqxkKjO+2OPoGbzurBph/ncGqWzfbXniO5Qy9+fnsZPYd05Q/HdUJTFDrVrCV/6hSOOK0zqz9ZQ9sLzmVH7jH4rDCfry3lk/l59MnxULBmM9WFm0g96VRsM0hN2/7M21ZFYptufL68gI1rShnaM4OyLes5tkMK3VOd2GYQI38Z/uXzcHnT2P7TKjxZHUnJTmBbUS1d2yXRv30SFSGbIzM8WOHIfwuO0g14dJVkh0bNurVkuXQS2yRQvmEHSR28VG6totBvktghm7KgRaVpo6S1xWfZWAmZVJiRH79yv0VBVQC3prCt0h+bSbqwwofXoeJ16dRWB3Elu/DXhAj6QsSlxRGqqdg5k3TAhxX0o3lTscwgqjcV1ZsamWXZnUi4bgbpsNMT+5nwhSLxaEXV8NfNJF0bsqgOWqiqRnXQxFc3m3RkVunI7NHVfhNtl5mkoxFpTY/MJG0GI7FoM2RhhixUTcEy7YbxaE1F0xVsOzLrdNgOY5lmJAZtBmMRadsMYuhag0i0ru6MMUdj0YYe+Xo2mGG6/uzQ6s6Ys0PbuR593K43q3T02GHLQlV23hgYmWGanevK7nHr+seIzhQdnUl61xmhG5tJuj5ll2PvKSK9L/u6NWBPszw39rehOX8u9nc26QP9m3Qw/oS15vsnxOFHRl6EEEKIFiBpo6aTzosQQgjRAiRt1HTSeRFCCCFaQOSSbPP2b61azT0vQgghhPhtkJEXIYQQogVI2qjppPMihBBCtAC556Xp5LKREEIIIQ4rrabz0u/2D3F503np07X8PGEQE4+5njFbu9MjwcmCt9/guKuv5oR7v2RShzyevuol7rv/KuI1lZOXvsS/Hv+GxwpyyHE5uPjfN+Fwe3jd6snVY07nurTt+CtKmDXmcb7cXsOrl/Slf5KLkf93Dh+/tZyiYddQ6Dc5LTeRNbNmMvjBy9jS7TSy+p7A+I9XcPmZR3B5rzRqS/OpffcZfv73PLoO7sMPpT7GndSVvnoxPRKclPz33/zyzhI6XnwmC8p9mP3PZvqGctKdGlPnbiZv1TY6j+rCjryVmP5qzB4j0F0eFuRX8+nyQjI7teGXVcWUbt7ECV3TqCnOo1dGHO6i1ZE6J0u/Z/uPv+DJ7Mj2pYWktUmhU/skttSGGNQphb6ZiQTtMLkJDjQFPLqKtWUlyQ6NNm6dHWvzSMmMJ6l9IpVbq/B2yKS4NkRZ0MLRtguVphWp7+LNxgpDjeKiPBCpMZJfFWBblR+3ppK/w4dHV/E6VLbv8JNs6LiTXfiqgsSlxRHwhwjWVOFKTYi8Tl81rlQvVtCPZQbRvKnYoSBaQhK4EgjbFrbhwXYlAmDprtjPhM8MA6CoGjVBC0XVqAiYVAXMSD2XoEVV0IzVdlEdBppuUBvcWfMluq4bDoJBC01TsUwby4rUdLEsG8u00XQV2wrX1XaJbNd0BVVTsS0bXVexTHNnjRbLqlvXYtsMLVK7xamrsTouRnTd2rW2S726K6pC2LZxqOrOmi+KElt3aDvfBqLbovVdItvq1YRRd/63t+v2qPr/DDb2j6GiNF7fpf4IeGP/T+6pxsu+aqfs+vD+vukd6Ij8vtrvT42XVnwVoFXSlOYvrZVcNhJCCCFagFw2arpWM/IihBBCiN8GGXkRQgghWkD96T2aun9rJZ0XIYQQogXIZaOmk8tGQgghhDisyMiLEEII0QKamxhqzWmjVjPyUlO8mWUvX8ndfzyOt7ueAMA7T/6Ly5e+S5+zL+ar3yWxdcEXvDryj1SaNrdZc/jDs5fwzPX/QVMUJv7jv1w38QJmdruYM676HX95cjp/PdrF3Cv/yBEnn82Hq0vp5jFw/GcCZ911EtYlf2Z9TZBb31vGyIx4jn3od/jKC9kx/Fru+2wlZ5/WneXfLuSWQW2xPnqS+PRcFj0zg9lbKxl7aneqTZsT0k2qPnyF/kdns+KN+fyYX4Uy7CKKAxYzNu7g9bmb6Z/kYuPyAnZsWk7bM0cSqqlAM9wsKQ7gyerApyu2s2jFdnr3yKB441aqt2+iX7aHUE0F3oqNBJf/gMubTtGPyyhcuI3U3AwKNu4gt30SQ7umUR6y6JedSIckA4D4mu24NZVkh4Z/zXKyXBqp6fGUry0muVMS3o4ZFFf48XZpQ3HAotK00XM64bPCWGGo1T0AlPosCquCuDWVgqoAW8t9JOoqm0trSTE0UgyNmsoArmQX7jQ3/togcWluAlWVhGorcKV6CfmqsYJ+HMnJmEFfJCKdnI5tBsGTiu1KACDsSsA24gDwmTYQiUf7TBtF1VBUjaqgieYwqAqYVActFFWlNmTFItLVfhOtLh5dXRel1pxufEELTdfRNBUzZKHqKmbIxgxZ6A4VM2hhW2F0hxaLTOuOSJxarYtN23Y4FnO2Q0Gc0XUzuDMSrak4dRW7fjzatmLXzKPrsXh03XByNCINkTc6Xdu5Hm2jKgp2vf3CllXXZufj0Si0Wm8W3F23R+0agw7bkdh1tEn9x/c16q0qygFFpBvErRs59v684e3pNoKm/p3Yn4h0k457SI76v9OKr3jEKHWXjZq6HKqfrcOBjLwIIYQQLUBu2G26VjPyIoQQQgh47rnn6NixIy6XiwEDBvDdd9/tse3777/PySefTHp6OomJiQwZMoQvv/yyQZvJkyej1I0E1V/8fv8hew3SeRFCCCFagErkMmWTlyY859tvv83YsWO57777WLx4MccddxynnXYaW7ZsabT97NmzOfnkk5k2bRoLFy7khBNO4KyzzmLx4sUN2iUmJlJQUNBgcblcjR7zYJDLRkIIIUQL0OrdQ9bU/Q/UxIkTufbaaxkzZgwAkyZN4ssvv+T555/n0Ucf3a39pEmTGnz+yCOP8NFHH/HJJ5/Qr1+/2HZFUcjKyjrg82kqGXkRQgghDmOVlZUNlkAg0Gi7YDDIwoULGTVqVIPto0aNYs6cOfv1XLZtU1VVRUpKSoPt1dXVtG/fnrZt23LmmWfuNjJzsEnnRQghhGgBzUka1S9wl5ubi9frjS2NjaAAlJSUYFkWmZmZDbZnZmZSWFi4X+f8j3/8g5qaGi666KLYtiOOOILJkyfz8ccfM2XKFFwuF8OGDWPt2rVN/Mrsm1w2EkIIIVqAptJgRvam7A+Ql5dHYmJibLvT6dzrfrtGrMPh8H7FrqdMmcKECRP46KOPyMjIiG0fPHgwgwcPjn0+bNgw+vfvzz//+U+efvrp/XkpB6zVjLx8+9IfmNdzMN9f+ijra0KM+/FFuhx/Nse9Xsic23vzwcDfM+qGa1hZFeAPE07l1fMe4Ycht1AesrjhL6dQW5rP2lPv5Ka/z2Ty2e0p+uUHll5zPW/P3crkW4aS43Jw3s1D+OSBT/GO/QdjP/qFwSlu5n4yixH3nYp13l2kdTuaP3+xhu8+X8j4EztRtmEpzm9eZuGkaXQaNITZa8soC1qc2d5FhzgH/o9eYNnk2fS65iTmrisn32/yXWEIj67y2rzNrPm5kK4ndaB8w1L8FcWox5yJqht4sjrw0fJCMjp344elBRSu28qpR2ZSVbCeQFUZabX5AJjLf6Bk3iLi03MpWLCJonVl5LRLYmNNiGFd0+if7cVnhemU5CTRX4JHV1G2/kKyQyPLpVO2ajNZ6XEktU+kfOMOvB3TSO6WS6HfwtW+M+Uhi2rTxvTmELTDAJT4LDQFCquDbKv049YUtpTXsrm0Fq9DpWCHD69DJS7Fja86UtslLi2OYG0N7lQPpr+akK+auIxkrKAP2wyiJmVgh4KRGifxyZEaKa4EwnV1Xiynh9pQpL5LTWhnbRe/aaPqBoqm1dV20fCFLKqDZqyeS7S+S5U/sk3VDXzBSO0XTdfx+U10hxar76I7NMyQhV1X08W2wrH6LpZlo2oKqqYSDofRdBW3ocXquURfg6FrhG0rUrtFidRucTZW28VqWPPFqHsnC9sWDk0hbEdes6ZEtumaGltXFQWHFqkbEy10FbasBtFLVYnWj9lZx0Wr999e/e2Nve9F67s09nis5kv9uiz1Ho8+d33NebNqbN8DqZGxp5b7Sqruz3McaNr1YIZjW3N5+d+SxMTEBsueOi9paWlomrbbKEtRUdFuozG7evvtt7n22mt55513GDly5F7bqqrK0UcffUhHXlpN50UIIYT4NVGV5l46OrDnMwyDAQMGMH369Abbp0+fztChQ/e435QpU7jqqqt46623OOOMM/b5POFwmCVLlpCdnX1gJ3gA5LKREEII0QLqV6xu6v4Haty4cYwePZqBAwcyZMgQXnzxRbZs2cKNN94IwPjx49m2bRuvv/46EOm4XHHFFTz11FMMHjw4Nmrjdrvxer0APPjggwwePJiuXbtSWVnJ008/zZIlS3j22Web/Nr2pUVHXp5//nn69OkTG+oaMmQIn3/+eezxcDjMhAkTyMnJwe12M2LECFasWNGCZyyEEEIcHAfrht0DcfHFFzNp0iQeeughjjrqKGbPns20adNo3749AAUFBQ1qvvzrX//CNE1uueUWsrOzY8vtt98ea7Njxw6uv/56evTowahRo9i2bRuzZ8/mmGOOaf4XaQ9adOSlbdu2PPbYY3Tp0gWA1157jXPOOYfFixdz5JFH8sQTTzBx4kQmT55Mt27dePjhhzn55JNZvXo1CQkJLXnqQgghxGHp5ptv5uabb270scmTJzf4fObMmfs83pNPPsmTTz55EM5s/7XoyMtZZ53F6aefTrdu3ejWrRt//etf8Xg8zJs3j3A4zKRJk7jvvvs477zz6NWrF6+99hq1tbW89dZbLXnaQgghRLNF00bNWVqrX81LtyyLqVOnUlNTw5AhQ9i4cSOFhYUNiuk4nU6OP/74vRbTCQQCuxXsEUIIIX5tWuKy0W9Fi3deli1bhsfjwel0cuONN/LBBx/Qs2fP2E1BB1pM59FHH21QrCc3NxeALWecxezt1dx42z+495vHOelLhUX/N4KF/32T6X1PZXZJLR+dpHHHnSPYeMlDrKkOMObBj7jlj8dTdtWjDPr9RVzy2LfkL/ySlddfSfuhZ/HmVxtIMTS6L/g3F1/Tj+z7n2ZemY8/fraaL9+bzWl3j6Ry6xrirvsrD0xfz4gzBvLlJ4soWbOApLlv4vKms+iJ/zLz5yKuP7MHhX6TXLcD87PnGNornWWvfMuc5cU4Tr6KPF8kIv3q3E309Tr5eWE+JWsW0em8E/CVF6LqBqsD8XiyOpDR+QhmLi2ge8908tduo3LbGobmevFXFANgLpuNMyGFkrkLyJ+/gZTcXAp/KWFddSQiXRK0GNDGS5eUyLwUSaHyWEQ6sGYxbdw62aluylcXkNwxieSu6RSX+Eju1g5Xh0hE2tGuG9WmTdAOE4hLBSKR3aKaIIaqkFfhY8sOH4m6xubSWraW1ZJiaFTu8ONJdhGX5qa2KkB8ZjzxGQmEaipwpycTrKnACvowUlOwgn7MgA/NmxqJR5vBWDw67ErAdkbWfSEbX11E2mdGPqq6QYXfRNE0tLpYtKo7qApaVAdMNKebitpQLCJdHTDRDDeqIxKb1g0nqqZihixUXY1FpDVdwTZtzJCNpqlYph2LSNtmJEqt6ZHt9WPOzrqPkai0Gnkt0fX68Wh7l3i0rsYi0tF4cdi2YzcBhm0LVVViX//om52jLjYd28+KxJId9eIL0f/q6t9UqCoKmrozbh2LPNcL8Cp1cezoelS0zb7eb+tHpPfUtn4EuUHcupH2u77JNRZf3lNq41BGpJvicP9T1Yr/1oqDrMU7L927d2fJkiXMmzePm266iSuvvJJffvkl9viBFtMZP348FRUVsSUvL++QnbsQQgjRVIrS/KW1avGotGEYsRt2Bw4cyIIFC3jqqae4++67ASgsLGyQFd9XMR2n07nP6oJCCCFES1NRGoxaNmX/1qrFR152FQ6HCQQCdOzYkaysrAbFdILBILNmzdprMR0hhBBC/La16MjLvffey2mnnUZubi5VVVVMnTqVmTNn8sUXX6AoCmPHjuWRRx6ha9eudO3alUceeYS4uDguvfTSljxtIYQQotmae+lHLhu1kO3btzN69GgKCgrwer306dOHL774gpNPPhmAu+66C5/Px80330x5eTmDBg3iq6++khovQgghDnuR6QGat39r1aKdl1deeWWvjyuKwoQJE5gwYcL/5oSEEEII8av3q7vn5VCZsa6Mh776PzJ6DuO0BRnMef01vjnyWIZcfgXTtlVyxx3H8uagK9l+00Quuu9Dbht7LCVrFlA79inOe+QbvrjxGLbM/ZR2Q87kPx+s5l93HIvXoXHZVUfx2Q0vk/voi/zxy8309br4YMpMdmxaTuJtfyOpQy/+/NU63n9/IY+e0YOiX37AmZDCokffoOPg4/l6YQH5fpMre6eR49I5rnc6S579kr7Xn8icJdvZVBtifqUbt6bQ1+tk4YJt9D6+HcWrF1Jbmo/juAtiM0l/sLyQ9C496doznW1r8jn3qDZUbluDv6KY7FAkJu1MSKHkh3l4MjuQP38925cXk9UhiXXVIbYHTIa0T6batOmRFkeqWY5bU1C3/UKwLiJd+vN6slPdJHdKonRtOcld00k5ogOFfhN3564YHY6g2rSxknN3ziRda6IpYKgKWyr8eHSVLTt8bCiuwetQ2VxSE4tI11YGiM+IJy7NTaCmmviMBNzpyYT8O2eStoJ+tOQMzIAvEtn1pmGbQQBsd2SuDduZQK0Zef7akE1NXUTaF9o5k3RFIDJjdDQireoGlf4QFbWh2EzS0Yh0tT+E6jDQDSeBoIWqqeiGFptJOhqRjkSmbSzTRjc0LMuObbesyOzS0Si029Bw1s0mHZ1J2jaDGFokBm3Xi1DH4tGNzCQdjUjXn0naoe6clTm6vutM0tGI9K4zScPOKDTsfSbpXW8YrD+TdH0qSoMh7ths0/Xb1JtJumHEeqc9RaQb01IzSe+P1jaTdGu+vLE3kjZquhZPGwkhhBCtkaSNmk46L0IIIURLaO7oSevtu7Sey0ZCCCGE+G2QkRchhBCiBUjaqOmk8yKEEEK0AIXmXflpxX0XuWwkhBBCiMNLkzov0eJyOTk56LqOpmkNFiGEEELsnVpXhqA5S2vVpM7LVVddxaJFi7j//vt59913ef/99xssv0YPfPoXTl2Sy8qnzuC7f/+bIZdfwed5lXz7Ow933nk82//wFIt2+Pnd+PcpXjUP35+eZfCll3HWQ1+z8fuPWXvN+bQbciav/HE4Hl1lyC9TuOyqo2j3xMvMKKph7Jebefetbzn37pMo27CUlE59uefztZxwzrH8992f2L58NpkL38aZkEKnoScy48d8bv7dkeT7TXLdDuxPnmZEnwyOunEk3y0uJO7s61lfE8StKTz//Qb6J7noc2IHtq/8iW6XnkxtaT6qbrDKSsaT1YH0Lj35fMFWevbO5Pz+bdmxaTnD2yfhKy8EwF42E2dCCp7MDmybs47U9h0oXFrE6qogJ/TIYHvAjNV3AUg1y1G3/UKaoRP45UdKlqwhO9VN6cptJHdKIrV7BkVFNaQc0QF3566UBC2MDkfE6rv449MB0BQoqA5iqAoeXWVTWS2JusaG4ho2l9SQ7tSo3OGP1XeprQoQnxmPJ9tLqKYCd3oycRnJmL5qnBlpWEE/ZsCHlpwRq4sSre0CkfouALVmmNpQpLZLjWnH6rtU+E0UTUPTDaoDJqruiNV30ZxuKmpDVPlN1LrHo/VdqvwmuuFE1VSCARPd0NAdGrZpo+lKrL6LpqlYpo1l2ahapPZLtL6LZdoYuorbiNR0cepqrL5LtPZLbN22YusN6rxE17XIr26kdkukvotWv0aLunu9FoemotW916mKEqvv4qh34VxTidVaiR4v0nbnsaLvl/VjmorCHmq0KLtvU3Yf7o7uu6v9qfGyP+/fjdV32dP9Ak2t77I/NWSaco/Cwfrz1FJ/6Frx39d9UmhmnZeWfgEtqEn3vHz//fd89913HHXUUQf5dIQQQggh9q5JnZfc3FzC4fDBPhchhBCi1VBp3o2nrfmm1Sa99kmTJnHPPfewadOmg3w6QgghROugKEqzl9aqSSMvF198MbW1tXTu3Jm4uDgcDkeDx8vKyg7KyQkhhBBC7KpJnZdJkyYd5NMQQgghWhcpUtd0Teq8XHnllQf7PIQQQohWpbkzQ7fiq0ZNv9/Hsizee+89Hn74Yf7617/ywQcfYFmNxx1/Dc6cn8EPr03m844DOeG6a/nmNLj7L6N4ecDlrBvzd373p7f40/2jKFmzgGOvvIIz/vIVX13Xh81zPqHT8HN49b8rmXLPCRz900tcceMxvHvN8+RO/A83fLKBo5NdvPufryjbsJS42/5Oapf+nHr+cbz7zlwmnnMk25fPxuVNZ8EDr9LluJO4/YJe5PtNrurppUOcgxH9s/hp4if0v+1UXOfezKbaEN9VuPHoKkcnu/lxXh5HndyRbpeOorY0H/2ES1F1g4Sczry9JJ/MrkfSp28WW1fmceGAtpzQIQl/RTFtggUAuLzpFM38noTszqR17Ej+siLadEpmdVWQ7QGTYzukUG3aAKSFSvHoKmrecvzL59HGrVO8ZC3Fy7eS2iWZktWlpPXIIvXITmzzmcR17Y7R6UiqTRszpX0sIl1ca6IpYKgKm8p9eHQ1FpFOMVQ2l9RQucOPJ9lFbWUgFpEO1FQTn5GAOz2ZYE0F8dmpsYi0lpyBGfARti2UpAxsMwjQICpdY0ZuJK8N2dSYkai0L2THItIVARNNN1B1B1VBC1U3YhFpTY9EoqsDJprhptofikWkA0EL3aGhG5FItO7Q0HQFMxTZHo1I64aGZdnYZqSNZdmxiLRtBnEbGk5dxQ4FMXQttt3QIjFouy5CHa4flbZ2iUprKqqiELYtHJoSizQ7VCUWOW6wrkWi15oSicuGLQtNVWIR6WiEtn6suv52Td25riiNR6T3GDuORbN3/pdYv2n0ddRvC02PSDf2hrbrfQFN+W/1YPyHe6DHOJh/l1oiJt2a/7DuL/UgLK1Vk0Ze1q1bx+mnn862bdvo3r074XCYNWvWkJuby2effUbnzp0P9nkKIYQQQgBN7LjddtttdO7cmby8PBYtWsTixYvZsmULHTt25LbbbjvY5yiEEEL85kjaqOmaNPIya9Ys5s2bR0pKSmxbamoqjz32GMOGDTtoJyeEEEL8VskNu03XpJEXp9NJVVXVbturq6sxDKPZJyWEEEIIsSdN6ryceeaZXH/99cyfP59wOEw4HGbevHnceOONnH322Qf7HIUQQojfJKUZS2vWpM7L008/TefOnRkyZAgulwuXy8WwYcPo0qULTz311ME+RyGEEOI3J3rZqDlLa9Wke16SkpL46KOPWLt2LatWrSIcDtOzZ0+6dOlysM9PCCGEEKKBZsXEu3btyllnncXZZ5/9q++4LPrgv1x6563MK/PxSe88Jh5zPd+d+xfyfCF+P+4VKrauYeWlD3POLdfwxcVt2bZgGgvPOpcep1zAh/eeQI7LQdcv/sYbN75G0kMv80Opj9+/uZSPX/+UC/52ARVbVpLRcxg3vbec838/nIlnHUHxqnkkz3iWuNQcup9wEl8tLmT8xX25vLNB53iD2jce44Shbel3+1nM/LkIx9m38VWhgteh8vcZaxmc4qbfmV0o+mU+Xa88F+2E0WiGm6XVbhLbdiOre0++/DGP/v2zuWhAW3ZsXs7x7ZPI8uWhqBqhhV/h8qaTkN2ZvO/Wkt6xPR26pLC6KsgpvbPI94eoNm16pscB4NFV2LSENEPDt2wexYvX0iYjjpJlWylZXUpqzxwKi2tJ7dUJd5cjKA9ZODoeiZnSjqAdptaVwvaaSH2XvIoAbk3F69BYV1JDskMjxVDZUFRNulOnosxH9Q4/nmwPNZV+/JUVeLK9BKvKiMtKxdM2HdNfgzMjHS01GzPgQ0vNwjaDkaVebRfLlRhbrwlFartUh2yqAzaqblDmC8Xqu1T4TVTdgWa4Ka8NojndaLrBjtoQmuGmwhdiR932Kr+Jz2+iOzSCARNVV9EdWl1tl+i6Xa/+S2S7GbQwQxaaHqn9Eq3vYoeCOHUVQ9di29wOjbBtEWdo2HW1XdxGZFu0vku03ouhRX5dVUXBoSmEbRuHqsZquuh1j9ev1xKt71J/P4jUgYnWWNFUYrVWojVj6m9XFWVnvZZdarzEarc0qNGyh7os9X4fo8+9r9ouB1LjZVeNJTH2WI9mD8fY13+2+5P2aMp/xwfrH+qWqO8CUuNlf0naqOn2e+Rl3Lhx/N///R/x8fGMGzdur20nTpzY7BMTQgghfsskbdR0+915Wbx4MaFQKLYuhBBCCNES9vuy0bfffktSUlJsfW+LEEIIIfauOUmj5iSOnnvuOTp27IjL5WLAgAF89913e20/a9YsBgwYgMvlolOnTrzwwgu7tXnvvffo2bMnTqeTnj178sEHHzTx7PZPk+55ueaaaxqt81JTU8M111zT7JMSQgghfutURWn2cqDefvttxo4dy3333cfixYs57rjjOO2009iyZUuj7Tdu3Mjpp5/Occcdx+LFi7n33nu57bbbeO+992Jt5s6dy8UXX8zo0aNZunQpo0eP5qKLLmL+/PlN/trsS5M6L6+99ho+n2+37T6fj9dff73ZJyWEEEL81kVnlW7OcqAmTpzItddey5gxY+jRoweTJk0iNzeX559/vtH2L7zwAu3atWPSpEn06NGDMWPGcM011/D3v/891mbSpEmcfPLJjB8/niOOOILx48dz0kknMWnSpCZ+ZfbtgDovlZWVVFRUEA6HqaqqorKyMraUl5czbdo0MjIyDtW5CiGEEGIX9f8WV1ZWEggEGm0XDAZZuHAho0aNarB91KhRzJkzp9F95s6du1v7U045hZ9++il2H+ye2uzpmAfDAXVekpKSSElJQVEUunXrRnJycmxJS0vjmmuu4ZZbbjlU59os9z48lmfCn/Lgx/fwfyffB8ANdzzLve/cTqimklvvvYFL7/oPU46DmSMuYMCFl/H67C18Of54El64k+smXsBL495l0Q4/5728gLPaJvLtWx9TU5xH8dl30+bo07n+qmP5cupX/P30rqhTHiYhuzOz75lC71NO4uHfH0VxwOK89BrKX/orI0d2YP7fv6TvHy9BOes28v0mH2yo4R9frua4tHiWz1vPURceSacrL8ZfUQzDL+P74jBJ7Xrw0rzN5PbqybHHtGXr8jVcOjCXER2SCFSVkbFjLcF5n+FOziR/+nd4c3uQ1aUdm34p4Ygj0ji9Tzb5fpPjO6Tis8JoCiRX5+F1qGQ6dWoWz6NjvIOihasoWrqV9J5plKwuo6DUR1qfzmzzmbi79ULv2IuKkIWZ2oFqRxIA22tNtlT4cWsq68tr8egqibrK2u1VpBgamS4HVWU+POlxVO/w46sOkJDtIVBVSai2grisVII1FXjapGOkZ2AGfWip2ehpkYi04s2IRXrtuOTY97Y6aAOgqBq1oUg8uipgUe4PoTki8ehyXwjNcFERiESiVYdBRW0I3XCjGW6qA2ZkP3+Iar+Jbjjw+c26+HO9SHRdFFqLxqZj65FYtKqp2JaNZZqReLQZxA4FiauLP7sNPRaFNjQVp65i2zsj0dFYdNiqt17XNhqJdmgKDjXyq6spoGuRNpqyM/Ls0CLHhbpYshXZLxqFVhWFumR1g6HnaIS5/nZF2Rl/jv63F4lQ7/zPb9cI9a52jUkfyH+M+4pI78+bWFNSGXvbZ38jqgf6vAcjPKLW+x7/r7Xi5G6TKOFwsxeA3NxcvF5vbHn00Ucbfb6SkhIsyyIzM7PB9szMTAoLCxvdp7CwsNH2pmlSUlKy1zZ7OubBcEBF6r799lvC4TAnnngi7733XoOJGQ3DoH379uTk5Bz0kxRCCCF+c8J2ZGnO/kBeXh6JiTvrXTmdzr3utmvnOxwO77VD3lj7Xbcf6DGb64A6L8cffzwQuYGnXbt2rbpAjhBCCPFrkJiY2KDzsidpaWlomrbbiEhRUdFuIydRWVlZjbbXdZ3U1NS9ttnTMQ+G/b5s9PPPP2PbkV5eRUUFy5Yt4+eff250EUIIIcTeKWG72cuBMAyDAQMGMH369Abbp0+fztChQxvdZ8iQIbu1/+qrrxg4cCAOh2OvbfZ0zINhv0dejjrqKAoLC8nIyOCoo45CUZTY0FF9iqJgWdZBPUkhhBDiN+cgXTY6EOPGjWP06NEMHDiQIUOG8OKLL7JlyxZuvPFGAMaPH8+2bdtiyeEbb7yRZ555hnHjxnHdddcxd+5cXnnlFaZMmRI75u23387w4cN5/PHHOeecc/joo4+YMWMG33//fdNf2z7sd+dl48aNpKenx9aFEEIIcXi5+OKLKS0t5aGHHqKgoIBevXoxbdo02rdvD0BBQUGDmi8dO3Zk2rRp3HHHHTz77LPk5OTw9NNPc/7558faDB06lKlTp/LnP/+Z+++/n86dO/P2228zaNCgQ/Y69rvzEn1hu64LIYQQognC4cjSnP2b4Oabb+bmm29u9LHJkyfvtu34449n0aJFez3mBRdcwAUXXNCk82mKJhep++yzz2Kf33XXXSQlJTF06FA2b9580E7uYDp/xhPce8W/uSy/Lz0SnIz78UWc3jTuC5/I03+7if+LX4ivNJ//DrmK91aVMvO2gZye5aF87CW88OgM1p56J/n+EJcek8OP77zHqHcfIlhTQZfjz+byf81nwg2DGH+Um6qC9ZQ+MZYZ93/E0HNPZNqmHfzz0n6crG9iWKqbvH88xJyJ39Br/M3M2FJB1dDLmbykkFy3g6c+W8XquSvoe80xFK+cR4drr8F/9Hk4E1L4fEMl//p+I+37HsEP8/IYNbgdl/Zvy44tKzm+vZfkgsWoukHt7A/Z8vn3JHXozeZvN5DTtQ1HHZnJmuogp/fOZnj7FIJ2mB5pLjQFvA4Na9V8clwOOsY72P7TKjLbedm+JJ+SVaWk9+nAljIf23wmru5HURK00DociZnWCSsMO5R4CqpNDFVhQ5mPtaW1dfHoapIdGulOjbziGtLjHMRnRiLSnpy6maQryonPTiFQXRaLSJv+GozMbLT0NtihIFp6G6iLSNvxO9NtId0NROLR1UELVTdQdYMdfhNF1SIzSftDqLpBZcCkKhiJQlfUhtCcbnTDzQ5f5HHN6Y7NJF3tNwkEIjNJhwIWoYCFbmiEAmZkJmmjfmxaw7Ls2AzTtmVjGBqWaWKbkXi0HYrMgh2dSTo6O3TYtupmmFYbRKLrzyQdjUiHbQuHquDQ1J0zSWs7Z5KOzhodjUfXn0k6GpGGyIzR9WeSjkZpd51JOqp+DDq2vstM0moj4d7GZpJW6z33vmaSrn+MPcWkVRp/89qfmaSVXT7uq/3+PMeBHkMIYOdlo+YsrVSTOi+PPPIIbnfkj8fcuXN55plneOKJJ0hLS+OOO+44qCcohBBCCFHfAUWlo/Ly8ujSpQsAH374IRdccAHXX389w4YNY8SIEQfz/IQQQojfpEihuaaPnijNueR0mGvSyIvH46G0tBSIxKFGjhwJgMvlanTOIyGEEELsQi4bNVmTRl5OPvlkxowZQ79+/VizZg1nnHEGACtWrKBDhw4H8/yEEEKI36YWiEr/VjRp5OXZZ59lyJAhFBcX895778Wq7C1cuJBLLrnkoJ6gEEIIIUR9TRp5SUpK4plnntlt+4MPPtjsExJCCCFaBRl5abImdV4AduzYwSuvvMLKlStRFIUePXpw7bXX4vV6D+b5CSGEEL9NYRts6bw0RZMuG/3000907tyZJ598krKyMkpKSnjyySfp3LnzPgvZtJQnn/qB8/pl8ck//8UF62Zz0pcKM569lucefoYzfpjE02c/zD0PXM/sklouG9yGH08+nTNn/YvnX1+Gpihc8ti3XHNhD4a9+y8cbg9THQPpe9bveO6WIfz82ceMTipg/fjbyBlwCh8/OYsZRTU8e0Fv3JpCzy1f88tfHmTEjUP55tUFfFtcy+ZOJ+GzbJ76YTP/+vgXRvTPYt38RZRtWErOtbdgBX0Utj+W/64oIu2IwTw7cz0Lf9zK+cd1oOCXxVzavw3HpGtYQR/GL19T9sX7eLI6sOHT+Wz6djNtu2bwy+YKhvXN5vQjMykOmAxrl0SXhDCGquDK/5kUQyPX7aBs/nw6ewyyu6awfWkhWUdlsn11KRsrA6T0PYJ8v0lJ0ETJ7YHPsgmldqLEjMxpkV8dYk1pDR5dZU1pDb/kV5Lu1FlZUEmWSyPd66KqzEdCjofEtonUVNaS2DaRQEUxgeoyEtplEKqpwPTX4MjIwQz40NLboKe3wTaDhBPSsNzJANhxybHvZ1XQRlG1SJ2XUGRddRiU+0LoTjcVAZNyXwjNcFFaG6SsOlLHpbQ6iG640Qw3FbVBdLcHzYjUd3E4Dfy+UKS2iyNS28UMWegONfLRiNR2iWzT0A01tu6sq+/iNiL1XKL1WmwzSNi2cDu02Mc4Q8Myg7gNLdI+FKn/YoeCDWu+1NV3gUgNl2idFIe2s2ZKtP6LbVuxui3R9uG6aTqidVxURYnVfHGoKg5VrWsbOVa0TVS0hotSV9sl2qax+i9KI7Vdosfbdb/IvvtvX/vtqe5KY7VWmlrbZW/P01wH46jqITq3/SHz84qW0KSRlzvuuIOzzz6bl156CV2PHMI0TcaMGcPYsWOZPXv2QT1JIYQQ4remKZMr7rp/a9WkzstPP/3UoOMCoOs6d911FwMHDjxoJyeEEEL8Zsk9L03WpMtGiYmJDSZuisrLyyMhIaHZJyWEEEIIsSdN6rxcfPHFXHvttbz99tvk5eWxdetWpk6dypgxYyQqLYQQQuyP6MSMzVlaqSZdNvr73/+OqqpcccUVmKYJgMPh4KabbuKxxx47qCcohBBC/CbJZaMmO6DOS21tLXfeeScffvghoVCIc889l1tvvRWv10uXLl2Ii4s7VOcphBBCCAEc4GWjBx54gMmTJ3PGGWdwySWX8M033/D000/Tp0+fX33H5fabBpE97UsGXTqa3n/+njmvv4Z656W0PXoUj/3xA2osm3G+6dw2pj/HfPkJb87bxm3LEuiR4OT6+05my9xP6fLv97lzkcLIy8/h3idn8PZNgxm88RM0h8H86+7knTeXc/eYQSyt8JPj0kmb/k/O6JfFgjsn8flHa8m6/QEWlPvRFHjwy9UMTnHz32mr2fTjHPrdfhYVW1YCsMrVhcS23Xh14TZembGOvse0Zc1PGyle9RMX9cqievsmjlDLYP4HOBNS2P7px2yYtpis7r3Y/F0ey7bXcNrANmyqDXHmkZkMzfVihaGdUoG2fj6ZTh3/T1/TOd6gU0YchfPXktEnnawBbdi6YQeZA45gY02I7QETZ89jKAta+Kww/qR2WGEoqLXZXOHHrSmsKqlhdXE1yQ6NFdsqWVUXkd5eVENKqpvEtglUlvlIbJuAt30ygYpiEnIzCdZWEKqpIK5NNqa/hpCvGj2zHbYZREvNwY5PJWxb2PGp2PGRCs41tgaAompUBq1IPFo3KKsNoToMND0SlVZ1B+W+UGTdYVBWHYxFpCt8ITTDjeZ0s6M2hG440R0aAV8I3dAIBizMoBWLQUdj02bQxnDq6A4V27RxOCOxacu0cUYjz2aQuLros20GcRs6thlZN3QVywzi1NVYFNqot+42tHrx6GhU2o5Enm27LhIdjTkrDSLUsXVVIWxZhK2dEetIm50R6/qxaU2NxJ/rx2y1euuKEn2cBpHo+hHqqMaixtGI9G5t67WpHz+uf4zGIti7vlk1Fl3eU+S5qWne6HPsT0x6f+LWzT2fXwuJRx8c0YkZm77IZaP98v777/PKK6/w+9//HoDLLruMYcOGYVkWmqYdkhMUQgghfpPkslGTHdDIS15eHscdd1zs82OOOQZd18nPzz/oJyaEEEL8psms0k12QJ0Xy7IwDKPBNl3XYzftCiGEEEIcagd02SgcDnPVVVfhdDpj2/x+PzfeeCPx8fGxbe+///7BO0MhhBDit0guGzXZAXVerrzyyt22XX755QftZIQQQojWQqYHaLoD6rz8+9//PlTnIYQQQgixX5pUYfdw9NnZ9zH0qn/y7agQW36cztArruSZt1bw0xOn0tfr5M5Xr2LiRZPg4cmc+NwSftc1hdeeep2r37mTHWMep92QMznt+flMfv593rqwO9uXzybu9fuZcfWTDDzvbN77Lo/tAZPr2tbQP8nF787txrd/msKgR2/kswX5rK8J8tYWhSyXzqi2iXz7+VKGje7H1sWzqC3NRznrNhzxXlI69WXirPV06N+P975ez8aFy7n1+M6UrF6Ar7yQ3IpVqLpBaOZU8j78nOROfVn7yc+s/qmAPn0yWbLDT54vxFk9Mqk2bY7O8ZBeswWvQ4WV31M15xu6eBzkf7eY9u0SyToqk8Il28ke2JHMY3qysSZEfJ/+bA+YlAUtzMzuBO1IHG9rVQhNgfXlfn4pqsbr0FieX8mKbRVkuTTW5ldSVlxDUraHqjIfiW0TSWybgG9HGd6OGSS0yyRQVUZCu0xCNZWE/DXoWe0wAz5sM4iSko1tBrE8aVh18eigkUBlMPLfRXQmaVU3qPRbqLqB5jAoqY3OFO2izB+JQpdUByirDuJweSitCVJWE0B3e9hRG5ld2uFy4fOb6A4Nw6nvjESHLIIBE4dTj8WmHU49Nrt0tI1RLx7tNrRYRLp+PDo6k7RtW8TVRaGjM0mHrch6LDat1ZtJWlMI25HX7FAjMWe9LhIdeVyNzSStKTtnkI5GqWHnbM71o9CasnM2ak3dOROx0khEORqRjm6LxqPrzyStKjvjwfuaSVpl55uNoigHNEPzgcwk3ZimziR9sOPRezuXA6HWi7v/r0lE+iCz7eYvrVSTKuwKIYQQopmaW+K/Fdd5aTUjL0IIIYT4bZCRFyGEEKIlSNqoyVp05OXRRx/l6KOPJiEhgYyMDM4991xWr17doE04HGbChAnk5OTgdrsZMWIEK1asaKEzFkIIIQ6O5k0N0Lyk0uGuRTsvs2bN4pZbbmHevHlMnz4d0zQZNWoUNTU1sTZPPPEEEydO5JlnnmHBggVkZWVx8sknU1VV1YJnLoQQQoiW0qKXjb744osGn//73/8mIyODhQsXMnz4cMLhMJMmTeK+++7jvPPOA+C1114jMzOTt956ixtuuKElTlsIIYRoPrls1GS/qht2KyoqAEhJSQFg48aNFBYWMmrUqFgbp9PJ8ccfz5w5cxo9RiAQoLKyssEihBBC/OqEw82c20jSRi0uHA4zbtw4jj32WHr16gVAYWEhAJmZmQ3aZmZmxh7b1aOPPorX640tubm5AEy450kcbg+PD7mFv036E1+fEubqkR1ZNHQEly6cyusdL8cKhznl3s9Z8PYbnDj7v2iGi9e8I/ndX79hyj0nMH/qO/jKt7P2xstoN+RM3npgGp9srWTK1QMwVIWzOiaz/PZxnPnHE+j56CNM21bJxiPPpSJk0SvRyd/f/plThrRhyL1nULxqHu3H3k2opoL49FwmLykkq9ex9BjWi5kzNzB6VFe2LJpP+ablDE8PY/qr0V0eSj96k4Sczqx5ZyZrP11Nx965LF9bxvLKAL8fmMv2gEnQDtPdY2GoCvFbFxFaOJ0OcQYls2axdfbP5PZIY+u8PHKObkP2Md1YX+4n7Zi+uHoNpiRoonbqR0XIxgpDkeUCwFAVVtTVdvm5sJIleRVkOnWW5u1g47Yq0r0udhTXUFXmI6m9l+odNSR1TMbbMQt/ZTEJ7TJIaJ9DqLYSo017Qr5qrIAPPbMdthkkbFtYCekA2PGp+NTI81YFbSqDdqS2SzBS20WN1nZxRmq7lPtCaIYLzemmuDKA5nRTWh2ktCaIZripqA2yozZS/2VHdRCH00B3aAT9kXouDpdGKGBiOCMf69d8MUMWuqHW1XnRcLt0LNOM1XaxAj4SXI5YfRe3I1L7xTKDkcejNV9idWDq1XbRVVx6pP5LpIaLTdi2cdWr+eKsa+tQlVhtF4eq4KgrMOLQ1FitEa2uCEe0VkyUQ1Xr2iqx7apSr14LO/dTFRqp0bKzrbJLbZfos9SvK1O/Fkj9N5j6dVOix6hfJ6Wx/Xbdf1/HqK+xzfVr0+zJoajvsqfzOVAtWdtFarwcAmEL7GYsYaulX0GL+dV0Xm699VZ+/vlnpkyZsttju76ZhMPhPb7BjB8/noqKitiSl5d3SM5XCCGEEC3jVxGV/sMf/sDHH3/M7Nmzadu2bWx7VlYWEBmByc7Ojm0vKirabTQmyul0Npg4UgghhPg1io62Nmf/1qpFR17C4TC33nor77//Pt988w0dO3Zs8HjHjh3Jyspi+vTpsW3BYJBZs2YxdOjQ//XpCiGEEAdPcy4ZRZdWqkVHXm655RbeeustPvroIxISEmL3sXi9XtxuN4qiMHbsWB555BG6du1K165deeSRR4iLi+PSSy9tyVMXQgghRAtp0ZGX559/noqKCkaMGEF2dnZsefvtt2Nt7rrrLsaOHcvNN9/MwIED2bZtG1999RUJCQkteOZCCCFEM/2KR17Ky8sZPXp0LPwyevRoduzYscf2oVCIu+++m969exMfH09OTg5XXHEF+fn5DdqNGDEidvN9dPn9739/wOfXoiMv4f2IeSmKwoQJE5gwYcKhPyEhhBDifyRsWYStpndAmrPvvlx66aVs3bo1Vo/t+uuvZ/To0XzyySeNtq+trWXRokXcf//99O3bl/LycsaOHcvZZ5/NTz/91KDtddddx0MPPRT73O12H/D5/WrSRoda37PPZ8mrY0h3avzuq8eYeMz1ZE39hHeWFXHS+zu4594XuXPKLRQsnkGbo0/ntHcLGHf3aO57eAobv/+Yrl/8jbjUHAZecAGT31nJs7cfy8qqAL0SnSj/uocLT+vMiGdv4r3P1pH8pyf5b0U6KYbGTVOWcFpWAqeO7suGOTPo/5frcV1+H5rhZlYoh9Qu/elw9GBe/GQlo0Z24U+julG4/Hsu75NFTXEkKWV+/TpxqZG2K6fOp12f3vwyO48F22v4/bAOrKkOUhGyGd4uEQCvQ0VZNoNct4Oq7z4n/+s5dG+XSN6sVWz7sYC2QzuzaeMOco7tQ9KgIeT5Qhi9j8VufxQ+K0ylpw0AmgJrynx4dJUUQ+Pn/ErSDI3Fm8tZua2C3DgHRYXV7CiuIblTElXlPmrKyvF2TMNfXkhix2y8ndsQqqkkvkN79JyOmAEfjpwOWEFfJFKckBmL5pruSH2fSkujImCjqBoVAYtyn4mqGxTXROLRuuGmpLYuHm24KakJojndOFweSquDOFweymqClFYH0N2RbdW1IQynTjBgYjh1HHXrDqdWF4u2cTh1Qv5IPNpwapjByLrTqWMGQ8S5dNyxeLSOx+WIxaCjEek4Q8Ouiwy7jUgMOmztXDd0NdbWpak4tEhE2qlrQCSmrNeLSju0aMxZjcWjVSUSeQ5b0di0GotHR7+W9WPTmhqNQe+MPGuKEmujNBKPhp0R6vrxaFVpGPmNRqT3FY9uLOK86/PtSWPJwgOJR++t/d6eY0/HOJCYdP04eVOpitIiEWmQeHRrtXLlSr744gtefvllhgwZwpAhQ3jppZf49NNPd5vCJ8rr9TJ9+nQuuugiunfvzuDBg/nnP//JwoUL2bJlS4O2cXFxZGVlxRav13vA59hqOi9CCCHEr4ptN3+B3QqzBgKBZp3W3Llz8Xq9DBo0KLZt8ODBeL3ePRaIbUxFRQWKopCUlNRg+5tvvklaWhpHHnkkf/rTn5o03c+vIiothBBCtDq23bz7Vuo6L9FirFEPPPBAs261KCwsJCMjY7ftGRkZeywQuyu/388999zDpZdeSmJiYmz7ZZddFksSL1++nPHjx7N06dIGqeL9IZ0XIYQQ4jCWl5fXoIOwp1pnEyZM4MEHH9zrsRYsWAA0fil1bwVi6wuFQvz+97/Htm2ee+65Bo9dd911sfVevXrRtWtXBg4cyKJFi+jfv/8+jx0lnRchhBCiBUTvbWvO/gCJiYkNOi97cuutt+4z2dOhQwd+/vlntm/fvttjxcXFeywQGxUKhbjooovYuHEj33zzzT7Pq3///jgcDtauXSudFyGEEOJXL7zzvpUm738A0tLSSEtL22e7IUOGUFFRwY8//sgxxxwDwPz586moqNhrgdhox2Xt2rV8++23pKam7vO5VqxYQSgUalBFf3/IDbtCCCFEC4iOvDRnORR69OjBqaeeynXXXce8efOYN28e1113HWeeeSbdu3ePtTviiCP44IMPADBNkwsuuICffvqJN998E8uyKCwspLCwkGAwCMD69et56KGH+Omnn9i0aRPTpk3jwgsvpF+/fgwbNuyAzlE6L0IIIYRo4M0336R3796MGjWKUaNG0adPH/7zn/80aLN69WoqKioA2Lp1Kx9//DFbt27lqKOOalB4NppQMgyDr7/+mlNOOYXu3btz2223MWrUKGbMmIGmaQd0fq3mstHnwyv4qf+xXL3yM/6QfRKd4w2OHfsus8YO5YjXXyMhuzNPec/k2CsH8vSFfRlw9l18/tzpPFaST6fh5/DSuDsZ99Gn3DKoLf+4X+H49e+hHpHKUdcP560HpnHLyg/5RcmhIvQo4z5dzew5W/j3iPY8+8VMjn/sfNwjL8E86+9s6Xk2nywspE3/E3jwoxX0OaEf5w1oy933vcQ7t99Ne72KUE0FCUs+xhHvJSGzAysnf0rWkbeS3TGZxR+VctqE9ix93E9FyOaqbmkstcO4NYWELT+S6dTJcukUfTWdXmluNn25kLK1ZeQOa8fS91eyzWcy4rhjWP/kbOIGjsBObU9FyCaY3ZOiGhOA9eUB3JqCW1NZuK2CNEMjxdD4ZHM5V8Y5eH9bFdUVPpI7JbGjuAZ/TS0pXVKpLd1OsLYC76A2+JcV4+3SHi05nZBvMY42nVGT0rHNrwmntI39x2Al7LyjvdxvoagaO/wWPtOuq+0SoiJgojvdFNUE0QxX3fYAWl3Nl4IdfhwuT2R7lb+utkuAWr+Jw2ng94VidVyCPhOHS0PTVEIBE6fLgaqr1FQGSEx1Y4YsLCvS1gxZsTouthkkwaVj6FrdNh1DUyPrDg2nrmLVaxut7WKHIv9xGHqkFotL12K1XaK1W+rXaAnbVqyei123Hv2oqXW1XbRIbRcAh7azBkh0W3R7rOaLWq9eyy61W6J1YKLUelVJlEZquyjsrO1Sv27MvuxPbZcDqQ9T375qu+xxvwMoYnIgtV32dk4HQmq7tALNrZJ7CCvspqSk8MYbb+y1Tf1Csx06dNhn4dnc3FxmzZp1UM6v1XRehBBCiF8Vu5n3vMis0kIIIYQQhwcZeRFCCCFawK95bqNfO+m8CCGEEC3hIFXYbY3kspEQQgghDisy8iKEEEK0hF9x2ujXrtWMvDx82v3M2FJB/4mruPKE9oz78UXKNi5l7Y2T6HnaBTz/2NX89YF/8eUFGcRP+gNp3Y5m6ml3c9YNl/PhvSeQ7w9xT3Y+JfdcxZVjBvDRVf/khKmPErzqYVZWBXhynYPrXv6Rc49M54MpM9nwwxcMmPgXqrdvIvi7u3i32ENm7+Hc+fEKXnp/BZee3YMVM+fx8Jk9ubJPBrWl+XQqWYjvg+dIyO7MupfeJKPnMDoO6MXC7/M44bgOXHd8J9ZUB7i8fxsqQjaaAm0q15FiaHSIMyj78iN6e50c0S2FzV+vov3wduR9v5VVGytoc+Ix/FIZIM8XQu8znLKghdmuH6WuSFR5XXmAZUU1eB0qP+VXkOzQyHHp/LSxjHZxDnK9Tgq2VZKWm0hpYRUVJbWkdE2muqSI2tJtJHXLxVdeSLCqnPhOnQjWVOJo1w1Hu25YQR9kdsBKzCJsW1iJWbHvy44QKKqGqhvs8FuoukGpL0RBVQDd6aagOkBRTQDNcLG9OoBmuNHdHgrr4tG6y0NZTQDd7cER76W0OojD5aKqJkjAZ2I4dQI+k4AvhNOtE/CHcDh1DLeDkN/C4dRxunXMoIVRF482gyE8Lh0r6MMORSLSVsCH29BjUWi3QyPO0LDMIHGGFotFuw0tch3bjkSlw3Ykbu3SolFpFYcaWXfqKnrddmfdR9u2cGgqdt2bkkNT68Wjd0aiNZVYXFmry7ZGI8zR7VHR+LOmKPX2qxeFbiQeHTle3TYaj0dHotfRY1DvGI3Hmxscex/71ddYTFmh6XHk/Y1Iq8qBRaQPZjy6JWLSEpH+3wvbdrOX1qrVdF6EEEII8dsgl42EEEKIliCXjZpMOi9CCCFESwg3s/MSls6LEEIIIf6HmnvfitzzIoQQQghxmJCRFyGEEKIlSJG6Jms1Iy/D23l5+LP7WPvtJ3jf+oSTvlT4v0dv5ZLbX2D++KGcOPMfuJIz+XLQhTwz8TvefOgc5pX5eHNkPAkv3Mk1F/bgi1Nu49WXFpL5xGt8W1zLm2YPLn11Aee09/L0y9+z/KtpHPuvuynbsBTVYTBD60lat6O567PVPDplKb875yjmfT6fzfNmcMf/t3ff8VFV+f/HX3PvnTslkwzphd5BKT+KYrAAgiC7qCuuirjorohtLYiuK7K7YMVeVsS2rOKKX1wLlhURVEAUUQSRKqJ0SAglmbRJZube+/tjkskEEkMSZQj5PB+P++DktjlcSk7uve/POb0Nhbt/oF/oJ8z3nsSdnMW2mTP59tlPaH9Kf1bN/5HsM9ryx6GdWF9YzrXZbTmvazIAnUJ78NoV2rl1Cj+cS2+vg14dW/Dj+6vpcFYb2p/Tne837qfNiFP4rqCM7aUBHKcMZ195iOKQic/bHsOCbSU21ueV4tEUvtpVwIrth8hy2ln+4wHax9lpF6ezfaePzFbxJHVOIn9fMSldkyjK20/J/p0kdW8biUd7OnciUFpI0F+MvV13jIAfJaM9RouWFfHoTMrjUgHwGeExs01ROeQPx6MVTSenuCIeXVRObnE5mjOOvJJycnxlkXi0HufF7vSQ4/Njj/OiuTzkFZaju+PQHRplJUEcTjtlJUHKy4LoLo1yf5Byfwi7QyNYZuBw2tEdKoHyEHaHisOhESwP4HZqhALlGOV+4p0aZjCAEfDjcdoxQwE8Do14h4ZRMcO0x6lhmQaeqHZllDo6Hm2ZJg5NDc8krdhwVMwwbVdsOLXK2aGr4tFOTYmUDY+OR1fOFF1tVunD1ldSbdXjz4fHo1WbrdoM05Wio8HREeza4tF1zf4cfRxRx9Wlthmh60rz1jWD9NHEpGMVj44FiUfHWOULu41ZmqlmM3gRQgghxIlBHhsJIYQQMSATMzacDF6EEEKIWDDNxr23Iu+8CCGEEEI0DXLnRQghhIgFqbDbYDJ4EUIIIWKgciLVxhzfXMljIyGEEEI0Kc3mzkvHj+dzwYfb+et9p3PWhJkU7v6BdxOyuF9xsaTf2Xy6vYBX1n7Ju92foUeCg25v3c3V53fhk7PH8flOH3cVbOAZVzdSHSq/+9c33JoZzw1PL+Hgj6s5++U72HfrJ2hOD1+mDSK5Uw4ZXTpy5yurGHVRNu+/8TmFOT/x6ZT7efaeJ7ApKvpHM8O1XZ56lL1f76L92X/j6xfeZltJkPEzurL6/jJuG9KZjok6d5oWPZT9mN9+Szu3TsmCOfT2OsnI9PDD21/R9czWJHXJ4tMXVnDRE5fiaN+NHx5ejGvgKPaVv0TAtPAld8GwQLXBd/tK8GgKy3cVsONgKVlOjaU/7GfvIT+3enRe25rPn7LiiUuP42BuEaknpeBO81KyczfJ/dtRunQPRqCMhG6dKX9vG2YwgN5uKCH/GizTwExui2UahBJbY9jdABTipNRvhGu7lBmouguboobruThc4TovReHaLrnF5fj8QTSXhz2H/BSVhdDd3nC9F6cH1eGK1HZRVYXSkgAOpx1FU6rVdjENC5dHp+iQH8MwcbjiOFhR20XXVYLlATxuO7qmRGq7GOX+cE0Xpx0j4McyjWq1XXRNqdiuoSo2zGAAl66iq+H1cXb1iNoulmngqDjOoSnY1fDPDA5NRbWBWVHzpTI5UFnbpbJ2S+WvlXVX7IqCqlRtj67dUqmytgtAxccdsb2mWiqVTcs0IudoaG2XaIf/lFTXOarte9ivh+8vtV3qR2q7HD9keoCGazaDFyGEEOJ4YpkWltGYwYv1C/amaZHBixBCCBEDlmE2bvDSiGObOnnnRQghhBBNitx5EUIIIWJA3nlpOBm8CCGEEDEgj40aTh4bCSGEEKJJaTaDl8ETnmHxi7O4ecssAAZecSX3Xvcab/zzGt778RDt3Dq935zG9Rd3Z8Jbd/LU3+fT+bV3mbflEAAjn/2K81ol8KfrB7Div28z/I2/k/vdYizDYFnL4aR0OYXuw37DLbO+5qIxZ/HgFf3Y/OmHPDiyCwd/XE3IX4xrwQzcyVmk9zyLbx5+m47ZZ7B8zncsXrOP684/iRWH/OzyBxnbM52AadFb3Yf72/foGKdT/P5L7Py/N+nfOZHv537ByUPa0vn8nqxflUPHC7LJGDWSTUXluM/6HfQahi9oUpDSjYBpodpgdW44Hp3q0Ph0ywGynBqLv8/js015dIl38MPWfPbt9JF6cgoH9haS1jOV1B6ZFOZsJ7V3B1J6dcKfv48WJ3elvDifQIkPvVMvQv5ijIAfM61jJJobTMgEwvHo/aUhbIrK/tIQeSVBVN3F7sJwPNru8rDLV4bmjENzxrEzvxTN5WHnwVJ2H/Kju73szi8lx+fHHuclp8CPIz4Bh8tBSVE4Hq277JSVhuPRzjg7AX8Ihyv8a7k/iMOlESgPESwP4XTZI/FoT0UsuoXbTgu3jhHw46341QwFIvFooyIWXRmP9ji1SDza49Ai8WiHptQaj7ZMA6dWFY+2KzbMinUOTcEyqrZHx6MhHIuu/LVabLoi26vaqiLUldFmyzQi8WgAhartlesV25ER5PBxVZ99NPHoyohxbfHb6P9gjmU8+ufUdY7a+tPYhLFis8UsIm2zSUT6eFR556UxS3Mlj42EEEKIGLAMA1NmlW6QZnPnRQghhBAnBrnzIoQQQsSAZTUybWTJYyMhhBBCHEOSNmo4eWwkhBBCiCZF7rwIIYQQMSB3XhpO7rwIIYQQMWCZVqTKbsOWX29ixvz8fMaNG4fX68Xr9TJu3DgKCgp+9pg//vGP2Gy2astpp51WbZ/y8nJuuukmUlJSiIuL4/zzz2f37t317l+zGbw4W6Rz4S3X8Y+b3+Dzf93AJyMsTkl00ubJP3PrDacyYfGTPDJtASnPv8kzjrNI0BSGPvo5l/RM4/q/Deeruf/l3I/+ifeef6FqOm97Tiej9xD6XPA7bpr5JROuGsJzEwaw+ZMPePQ3nRlmbAzXzfjvA8RndiSr3wiWT3uD7mcP4dzf9GTRd/u4bXQPVhzys7csxJW90jAsiyRdxfHFa3TxOMh//Tm2zp7LgJNT2PDKMr5/exNdL+rL6pU5dL54CBkXXMCmogCusy/G6j0CX9Bkf2JnNpfa0RUbX+4uwmtXSHdoLPw+j3ZuO108Oss27qN7sotNWw6Su6OAjP+XRt5uH/l79pLetw2FOVtJ79+FtP7dKMvfh7fHSTi79aa86BCObv2qarukdojUBCnzpANgU1RyS4Iomk5eSYic4gBaRT2XbQV+7C4PO33+SG2XHYdKscd50eO87M734/AksTu/lN35pejxSeQUlHHQV4Yzzhmp7eKM0ykrDeCMs+OMs1NWEsQZp+Nw2in3B8P1Xg6r7RIsK6WF235EbReP0x6p1xLv0AgF/IQC/kgdGDMYiNR28Tg1XHY1UtvFbVcjtV3C7SNru5imgWkaOFTliNoudsWGXVFqre0Svb622i6qEq5XcnhtFzWqqEdNtV3UipojtdV2gep1WaJro0TXdqn8mOjjjqa2S001VmorQyK1XerHVsOfjzg+mYbZ6OXXMnbsWNasWcOCBQtYsGABa9asYdy4cXUed+6555KTkxNZ5s+fX237xIkTmTdvHnPnzuXzzz+nuLiYUaNGYdQz9i2PjYQQQggRsWnTJhYsWMCKFSsYMGAAAC+++CLZ2dls3ryZrl271nqsw+EgIyOjxm0+n49Zs2bxn//8h2HDhgHw6quv0rp1az7++GNGjBhx1H1sNndehBBCiOPJL1Vht7CwsNpSXl7eqH59+eWXeL3eyMAF4LTTTsPr9bJ8+fKfPXbJkiWkpaXRpUsXJkyYQF5eXmTbqlWrCAaDDB8+PLIuKyuLHj161Hnew8ngRQghhIiBX2rw0rp168i7KV6vl+nTpzeqX7m5uaSlpR2xPi0tjdzc3FqPGzlyJHPmzOHTTz/lscceY+XKlZx99tmRwVRubi66rpOYmFjtuPT09J89b03ksZEQQgjRhO3atYuEhITI1w6Ho8b9pk2bxt133/2z51q5ciVQ87tklmX97Dtml156aaTdo0cP+vfvT9u2bfnggw8YPXp0rcfVdd6ayOBFCCGEiIFfqsJuQkJCtcFLbW688UbGjBnzs/u0a9eOtWvXsm/fviO27d+/n/T09KPuX2ZmJm3btmXLli0AZGRkEAgEyM/Pr3b3JS8vj4EDBx71eUEGL0IIIURMHOs6LykpKaSkpNS5X3Z2Nj6fj6+//ppTTz0VgK+++gqfz1evQcbBgwfZtWsXmZmZAPTr1w+73c6iRYu45JJLAMjJyWH9+vU8/PDD9fq9NJvBy5qZlxG/7BW+7pvJoUtG8fiqXCZuXchN6YP54/bV3L1sK0O9Ts7+xyL2bfqGvf++in88+H+cueh1tljJuJc/zYM5Wcx/bwWDxv6OO574lHtvGsxZbVvQbcQk/jZ5Cub699EcLg4+PJE9X26hw+m3svRvk+kz5TnO6prKgpcOcu/Fveie4ubecoO/t1VYqdhItKuY7z1J3xZOWia62DTzdbIHtuS7f33Owf0lDPvHb5k9+V3ygwYjfz+WTVMXYB90CeWedIpDJrucbdh3MIhLtbFkewHbD5XS2mXnvXU59I/TSXWoPLphH+dkeXAnu8ndXkBmvwzyduQRLPGROaATRd/8SMhfTNrokyn7ch8JvfugJqYRKHkHvdspmC4vZugDQqkdI7HaImf4H4FNUcktDqFoOoqms8tXjupwsa3AT2FZELszju2V7bgEtu4vwRGfhKLp7DhYisOThGLX2XGgBEd8C/Ye8hMKGjjjdPzFAUIBA2ecHX9xOa54HVVVKCksx5viRlFtFB4oxZkVj6oplPuDuF12An4/lmmQ7NEJ+YuxTAOvWydUVozXpaNrCqGAnxYue6Ttddsxg4GKfe2YoQAAcXo4Su22q6g2G5Zp4rarKBURY7ddicSO3XYVs+L6OFQlMuurXbVhGeHItKoQiVVXRpodqgpUra+8xtHxaFtUJLqybbOBQtU+laIjwdHx6MrPiL5DW1O0Ofociq16/Lam46LVdo5q+xz26+FqPa6G6HVt6hOPttXSbqhYxaOF+CV0796dc889lwkTJvD8888DcM011zBq1KhqSaNu3boxffp0LrzwQoqLi5k2bRoXXXQRmZmZbN++nbvuuouUlBQuvPBCALxeL+PHj+e2224jOTmZpKQkbr/9dnr27BlJHx2tZjN4EUIIIY4nx3OF3Tlz5nDzzTdHkkHnn38+M2bMqLbP5s2b8fl8AKiqyrp163jllVcoKCggMzOTIUOG8PrrrxMfHx855oknnkDTNC655BL8fj9Dhw7l5ZdfRq34Ae5oyeBFCCGEiAHTNDEb8c5LY46tS1JSEq+++urP7mNZVRV+XS4XH330UZ3ndTqdPP300zz99NON6p9EpYUQQgjRpMidFyGEECIGjufHRsc7GbwIIYQQMRAevNRvTp/Dj2+uZPAihBBCxEDl7NCNOb65ajaDl3e7ncGmwiAT961jekoPOsbpZD+zmUf7ZXL+xJcp3P0Ds796iWsufgGnN5W3Oo8js89nXDAvh582fMek2y7m8cfeoGT/LnxLHiH+2Re5XEun6I1P8aS3Y/OtfyZndS69rnyE9x68gb1lQf7+UF8+erKYZy7pRVuPyh2mxdnqDgJfraZHgoOCfz/EkNQ4Ett5WfXk/xh4QRdadGnNOw99wpWzrubRPzxHYcjkoguv4qeb38CwIC+jLwHTYlMoke3bfXjtCh9sOcDWvBI6xum8sWo3B/L93JniYu66XP7QJQl3sou9Ww/R6rSWxGUkc2jjFloOPomiD3/ECJSRPKAfpQvWY4YCOHuPJFDyMmqXfphOL2bovwTTOhNQdAAOKfHYFBWbopJTHELVXSh2na354RmjFU3np/xSdHcCPx0qwVcaRI9P4qe8Ygr8QZwJqWzdX4wen4Squyri0QlodpVCXzkuj4OSwnJCQQO3R6eksAwjZOFp4aTwQCmJ6R40XeFgbhHpbi+aplDuL8fr0XFoCpv9xdXi0UlxDoyAHyMUIDlOJxTwk+zRURUbRnk4Hq1rCmYwEIlHW6ZBXMUs0ZWxaDMUDMejbTbMUACXXQ2fIxTAoanYFVtk1ujKn6ScWtXs0E6tavboysiyXamKTUfPKh0debYrtmrx6Mrt0e2aIsGqzVYtbl0Vq67Yl9pnfK4UHY+OPq5qu63Gdl0zRtcVjz7i2AbEo4/GLx2JPlbx6Jpi60I0N81m8CKEEEIcTyyzke+8yJ0XIYQQQhxTjXxhl2b8zotEpYUQQgjRpMR08PLZZ59x3nnnkZWVhc1m45133qm23bIspk2bRlZWFi6Xi8GDB7Nhw4bYdFYIIYT4BZmG2eiluYrp4KWkpITevXsfUXK40sMPP8zjjz/OjBkzWLlyJRkZGZxzzjkUFRUd454KIYQQv6zKtFFjluYqpu+8jBw5kpEjR9a4zbIsnnzySaZMmcLo0aMBmD17Nunp6bz22mtce+21x7KrQgghhDhOHLfvvGzbto3c3NzIpFAADoeDQYMGsXz58lqPKy8vp7CwsNoihBBCHG8qK+w2Zmmujtu0UW5uLgDp6enV1qenp7Njx45aj5s+fTp33333Eev3lhmcd1IqA66Zxep7R5I++lKmXP4KPT/7hLLz7ibt5NMZvQROufQPDOuTxa1TX+XDZ67lrIv/hhHwc+f1Lbjv4F7iUluz9cbLaXnKFSwZO4Wte4u46Pm5vH7FvzkUMJj1p/48ObkM1QYXe/ez1q7Sds0bFG1Yz5BUNz88cB8Hvz/I2Rd04evHP6X/n8+kRdf2PHrFC0z+51+xZXZk/ZQPUEdMYG9Z+HHaJiUL1WbDo9l45/v9ZDk1Xl21mx0HS7jA6+T55TsoKSzn/M5JPLNuH2UlAdoNaUvujztpM7gz7tQWFCxcR+sr+6MlplKyZBPe0wbhf21RuMbIyX8gVLYCgGBWDyzTwJ/UgZKgiU1R2VOm4A8GUXUX2wvK0VweFEVlw/5i9Dgvimbn+wPF6J5EVIeL73OKcHpT+T6niOKyIE5vKj/sK6K4LITTm8jeg6W4vQmoqkJxQRlxCU40u0Kxz48rXqeksBwzZJKY7uHA3kKMkElaqwTydhYQF6/j0lW2lpaQktAKXVNYX+Ij2eNA1xRCZcXVaru0cNsJlhVjGUa4jktFPRddUzBDAbwue7hGSyhAvK5hhoJYpoHHEdXWwzVfInVeKtqqYsMyDNx2JVxXxTBwqEqkvopDi2qraqTei1JRg0XXbCgV1Ua0igIllTVdDq/5okUVMFGjfuRQbUTOF13bJboGiBLZN/ocP1/bJfq42mrCRB9b7RzU3K7p8xpa2+VozlHtfD/Tn6NVWcflWNVzqWSr5c9HNH2WYWEZVt07/szxzdVxe+el0uH/eVmW9bPFqiZPnozP54ssu3bt+rW7KIQQQohj6Li985KRkQGE78BkZmZG1ufl5R1xNyaaw+HA4XD86v0TQgghGsM0G5cYMpvxC7vH7Z2X9u3bk5GRwaJFiyLrAoEAS5cuZeDAgTHsmRBCCNF4lmk1emmuYnrnpbi4mB9//DHy9bZt21izZg1JSUm0adOGiRMn8sADD9C5c2c6d+7MAw88gNvtZuzYsTHstRBCCNF4pgGm0vABiNnwCambvJgOXr755huGDBkS+XrSpEkAXHnllbz88svccccd+P1+brjhBvLz8xkwYAALFy4kPj4+Vl0WQgghRIzFdPAyePBgLKv2UafNZmPatGlMmzbt2HVKCCGEOAYsw8RSGjExo0SlT3w3r3qNgtSTKB/7KG8PvYM3P9rJWeOvov9t83nowes5u0MSvUbeRtHH98L3X/Bo/j7a/nca8VkdSWrTkQUjbmbApKfJ7pLKyxfO4unvzuTNF28HYMaIVtweNEl1aLT9chZ9Wzhp6dHZOPkuzju9FV/+9QUO7i1m0J3DeW3qfPKDBn9/bhovvnY1I66+nVCLluwtm0lOx7PZXxJCV2z8b1sJSbpKgqbw/PIdnBzvINWh8uTSbfy9dQJ3rtiJvzjAnae3YvuGvQRLfHQ89yT2f7eRkL+YthOyKXzmBzL+dBZacgb+19/HfdpVmC4vobKV0DUbM/QBAEWJHQGwKSq7/AqKprOtIICvLITm8rBxfynFgRCO+ETW7yvC4UlEsetsyCnE4U1B011s2FOIKzED1eFiU04hzsQMfthbSChoEJfoZf+BUkIBA08LJ0WH/MQlONDsKsUFZcQnudDsCnk7fbRIjSN/XzFGyKRtl2R2bS7BMg1SElqyuaiAzBbtw5FofzGp8U50TSFYVkxavKOqneAg6C/GMg2SPDpmMIBlGiS6dYyAnxYuO6rNhhkK4nVoKEq47dFVzFAAALddxYhqm5GoNFhGOPJsV22RCHVlTNltVwEi7crosq6FE3J2RcFmC2+3K7ZIOzoeXVMsWo0K2FVGnsPHRe1bwz7hc1RGfKuitrZq+1a1o1+Ci44r1xSJVmx1R5BrOu7wcx++7mjPUZfGxqOjI9HHKh5d05+POHFZhoXViMdGEpUWQgghhGgims2dFyGEEOJ4YhpWI1/Ybb53XmTwIoQQQsSAvPPScPLYSAghhBBNitx5EUIIIWLAtCzMRhSaM38mrXuik8GLEEIIEQuGhWVrxACkGb/zIo+NhBBCCNGkNJs7L6c9vY1Duxbz4Yu3MOSSv2ME/PjnXIHnP4sZu2ENe59fS1a/K/jsjPPYkVPE5c/N5ekrLuKx5Us5pWUCT/6zkAXXnYp932b+brMxbNf/+CneQRu3xtbbrub3/TNJ6pTIJzf8m1G3DaFF1/Y8esULTP7scSYPnIjfMDlvwjQ2TZoHwMbk/hgWfJTvYdf2XLKcGjO+2MGOgyVckOjinwt/4M5MD64UN3/+Ygd/HNiSuPQ4tq3ZQtcLe7Jn4yaCJT46X3kG+S9+R6jcT/rE31D80SIs08Ax8GbKH3wCtc85GC4vZugtSjJ7UhI0sSkqu814FE3Hpqh8f7AMzelBtet8l1uEIz6Jb/b6KCoP4UxIYfXuAorLQrgSM1i1swB3cksUu853uwqIS22Dqin8sMeHJyUFVVXYn1dCQpIb38FSQsFwbZfCQ37MkEliuocDewtJb+NFs6sc2FtIyw6JuHSVHRt2k56YxU9FhZihAJktOrKmxIdlGmR4XYSi6rkESn2kJYTbRrmf1Kh2Uly4ngtAslvHCJRhmQZep1ZRz0XDrtgwQwE8Dg3FZsMIBfDoGqZpYBkGHl3FMsJ1V9x2FTMYwG1XUKLquVS2nVrVel2zHVGvpbKmi2UaaCooFZVHVJstUs/DXkO9Fss0IvVj1Khz1VXPBarXRKnc5fB6LnXVWomur1JTzZTo9uH1Xw5Xn3ouh392berqU30c65outhr+fETzYxompq0REzM24xd2m83gRQghhDieWI18bNSci9TJ4EUIIYSIARm8NJy88yKEEEKIJkXuvAghhBAxIO+8NJwMXoQQQogYsCwLqxF1XqxmXOdFHhsJIYQQoklpNndedq/+DE9mJxy3jKHtaX8mo10LZpx2DRP/bx4PXDgKX9BkyYFzmZ5yO7pi49leRdxrs/H7nW9w6P3vOa9VAluuuogD3x/kyit7879xjzP27t/g6dyJhy5+gr+vfB4zpR0zMocy9PYn2F8aYm/ZTNZmnIVhWXjtKs98e4B2bjteu8pd72/kyhQ3D763kZLCMv75/9KZ8PGP+ItKuOfCbmz5ehM9/tAfd0YSe975ipMmDENJTCN/6ne0uv0SCq97PxzTHXI7ZY8+BIDZ+1zM0AcA5Ho6YFNUdpBIaaGJqrtYk1uKrzyEIz6Jr/cU4vSmoGg6y3fm407JQtF0vth6iLjU1qzYeojisiCejPas3HaI0rIQnrSWbNyRT3xaKqqqkLO3CG+KG1VVKNhfgjfZjaKF2ylZ8eTt9GEYJu1OSuPAnlzMUIDOJ6Wyc9NuMpIzceka3/sO0jalI7qqsKLoEC0T3ZQXH8IyDVolugiU+rAMg1ZJLgIlPjJaOCOR6PQEJ3bVRqishGS3jl2xYQTKSHTZMUNBLNMgwaFhhgKYpoHXoWGEAngdGqpiwwgG8OhqOI5sGMQ7wpFogHhdi0Se4x0qlmng0cP/XCzTwGlXIpFnXVVQbOH1uloVla6MR0NVbNqu2CJxXk0hErGujERDOPIcaUdizjVHotWoHz+iI7c1xaltRxGrjj4uOopcU3S5ehy75ih0TbHoumLVh/slItGV8WeJRIvjiWlYmMjEjA0hd16EEEKIGLAMKzw5Y4OXX2/wkp+fz7hx4/B6vXi9XsaNG0dBQcHPHmOz2WpcHnnkkcg+gwcPPmL7mDFj6t2/ZnPnRQghhBBHZ+zYsezevZsFCxYAcM011zBu3Djef//9Wo/Jycmp9vWHH37I+PHjueiii6qtnzBhAvfcc0/ka5fLVe/+yeBFCCGEiAHLsLAa8djo17rzsmnTJhYsWMCKFSsYMGAAAC+++CLZ2dls3ryZrl271nhcRkZGta/fffddhgwZQocOHaqtd7vdR+xbX/LYSAghhIgB07AavfwavvzyS7xeb2TgAnDaaafh9XpZvnz5UZ1j3759fPDBB4wfP/6IbXPmzCElJYWTTz6Z22+/naKionr3Ue68CCGEEE1YYWFhta8dDgcOh6PB58vNzSUtLe2I9WlpaeTm5h7VOWbPnk18fDyjR4+utv7yyy+nffv2ZGRksH79eiZPnsx3333HokWL6tVHufMihBBCxIBlmo1eAFq3bh15sdbr9TJ9+vQaP2/atGm1vlRbuXzzzTdAzUlBy7JqnVj1cP/+97+5/PLLcTqd1dZPmDCBYcOG0aNHD8aMGcObb77Jxx9/zOrVq+tz6ZrPnZf/zJjEKR0yeCD5n6zNz0bdu5H77zL4e9l8Xo3Tae11UHD9xfxpWHuSOiXz5lnXcuNDF/DCFc9xIBBi2upZ3N7rT/gNi8eXL+WJF7txyhX3sr0kiC/4KG+Z3dmxuZR2bjuT/reZHQdKuCotjpvnrOYfHRLxpMVx1X/X8fq5HYhLi+feT9bw7HUD+f7zrwj5i+k36QL2PL0MIxig44zxHLrufVo+cC2my0vprIfQhj+A6YgnVPYFxV2HYIbewqaobNOyIrNDr8wtwx7nRdV0lu0swJWYzidbD1FYFiQurTUfb9lPQWkQT3o7Fm3Kw5PeHtXhYvGmPLwtO6OoCqt+PIA3qyUbth4iFDBIzGjB3l3hyHNSuoeDOcW0SI1Dsyvk7fSR2SERza7y09pcuvfNQtcU9vywlz59Mtm2ZiuWadAhrQMbffsxQwE6pPZiuW8/HVL7oWsK5cWHaJvsDs8UXeKjbYqbkL8YyzTIauGqaJukeRwYgTIyPA6Uikh0ituOYgvPDp3osoej0qEAiU47RiiAZYRnkjYq4s9ep4YZDOB1apFostdhR1UIzzAdFY922pVIW1eVSAzaFhWJVrBF2pWiZ4fWKn40qDar9GEzTVftW8s5Ks4dHYmubXboumaYbujs0Iqt7hmoo9V3duia+lFTP49GTVHoXzMSXVsUWmLR4mj8UlHpXbt2kZCQEFlf212XG2+8sc5kT7t27Vi7di379u07Ytv+/ftJT0+vs1/Lli1j8+bNvP7663Xu27dvX+x2O1u2bKFv37517l+p2QxehBBCiOOJZTbyhd2K6rwJCQnVBi+1SUlJISUlpc79srOz8fl8fP3115x66qkAfPXVV/h8PgYOHFjn8bNmzaJfv3707t27zn03bNhAMBgkMzOzzn2jyWMjIYQQQkR0796dc889lwkTJrBixQpWrFjBhAkTGDVqVLWkUbdu3Zg3b161YwsLC3njjTe4+uqrjzjvTz/9xD333MM333zD9u3bmT9/PhdffDF9+vTh9NNPr1cfZfAihBBCxEKjCtSZ8CtOzDhnzhx69uzJ8OHDGT58OL169eI///lPtX02b96Mz+ertm7u3LlYlsVll112xDl1XeeTTz5hxIgRdO3alZtvvpnhw4fz8ccfo6pqvfonj42EEEKIGDANC7MRkyuajZjUsS5JSUm8+uqrP7tPTRNDXnPNNVxzzTU17t+6dWuWLl36i/RP7rwIIYQQokmROy9CCCFEDFiGVePdi6M+/le883K8k8GLEEIIEQOm1cjHRo04tqlrNoOXlL9NYPOOQu6YOJDXOw0it8zgr+9P4e7f3sd9G+YQatGKm1PP4KHiTewvDfHZ8z3JGnUX2ye9jUdTeDAniyynnSRd4fxZ33Bzhocxz39FaWE5z57emiuf/4rykmI+/VMfznx9GYESH6/dewHj537MmdPHoiZnsvOvi+jxwm3YElI4MHoG6U9NoXDk/QDYzvsb5Q/cAMDe9oOwzHfYEt8Nf9BEc3r48pCGr7wIpzeV+VsO4U4O13d5Z9M+4rM6omo6b363F2/LLqi6i3nf7sHb5iTe/XYP/rIQye268enaHEJBk+S2bVi7eT8pbTPR7Co7t+aTkpWApqvk7S4kJSue3O0FWJZFu5PS+GltLmYoQP8zOvL14k306nUyuqaydfUWupzZDpddZf3iVfRo2R1dU/giP5fO6aewsHA/lmHQOd1DmW8/lhluB0t8tEuJw67aCJYU0ibRjWqDUFkJmfFOQmUlWKZBWpyOESjDrGiHAn6S3TqqYiNU7ifFraPYKtsVNV+CAZJcdsyK2i5JLjtmKNyOd4TruMTr4b/2lmngsisoFbVbHJqtqs6LWlXnxamFi3boqi1Sd0RXbCgVtWLsSlT9F6XqHPaodmUdl5pquEDtdVzUOmq4VGtH1xmJ+rtfuV6tof7K4eeoqY6LzWarsV5Lbe1oddWVOZpyKDXVa6mt/UuSGi5CHL+azeBFCCGEOJ4YloXRiLsnjTm2qZPBixBCCBEDhhVeGnN8cyVpIyGEEEI0KXLnRQghhIgBeWzUcDJ4EUIIIWJAHhs1nAxehBBCiBgwG3nnRaLSzcCrH/2ER1U55bWH+eHZQSRoKjcc+H8MjLMzcqFBYcFm7j0plbPuXUxpUTlv/r47590zn0+vOwV3RjI9H3uDnx7/PXpyEhOfeocR/zeZS//6NpZpMOC/T7Jj9AwA2iyewcEhdwJgXDYD/7M3cmDIXfhDJpa5jPUpp+IrC2GP8/LhoTic3lQUzc7La3LxpLdDdbh44atdJLbrwcwvduDzB0nu1JeZy7ZSVBYitdupvLh0K+nd+6NoCm9/sYOMriehajY+X72HrG4dUFWFDevzaNUlg22bDxAKGLTqnMyuHw5ghEx69G/J2hXb6H9GRxyawmcfrWXYb/ugawrvr9rI0DPOYsuKtZimwWnndee7RV9imQZ92/ZnycE99Gl7Brqm8O7BPfRp0wLFZqPMt59umfHYFYVAUT5d0zwEivIB6JDkJlhaCEBrr4tAaSFtvE4Um42gv5jMeAf2ivhzpsdB0F8MQGa8g1C5H4C0OAdmMEC6JxyPNkMBktx2FMJxZK/Djs0GZihAvEONxKNd9qrIs0tTKuLR4ayrZRq4tKrca3Q8Ojo2XRl/1hUbtop4tK4qkXiwrtoi8dno+LMe1bZXvF2mVYs2V0WQ7XXEn6vHp2tu1xV/ttUSt64r/qzY6hd5ritJfDSR55ri0Y1ReZqjiT9LFFqI41+zGbwIIYQQxxODRj42+sV60vTI4EUIIYSIAcOyMJAXdhtCotJCCCGEaFLkzosQQggRA4bVuEc/kjYSQgghxDElg5eGk8dGQgghhGhS5M6LEEIIEQPywm7DNZvBy9RnLiO5TRtSbnqM/E+moyam4R43kxe+fZ3rf/cUAGd++RGbBt0OQOaSN8kdfBP2j/9NQdCk9P3JbDn3PnzlIeAh/pc8BD1uA4pm59ncROIzO6LYdaZ+cYDkTn1RNJ2J724ko/cQbn1nA8VlIVqdMpxb31hLsNyg3aln88Db62l76llodoUX3t9Ex+zTUFWFeYt+pNOpvfhoyVaMkEmnfh34ZsUujFCInqe2idRocesqiz/8jhHn9UXXFN55fRmXjB2EQ1N4+d8f8buhI3n+uf9hmQbjf38RjyxchmUaDLuyL5+/uYBhJw3ErtiY/8o2zupyDnZF4fWDexnYIYlXDu7FMg36tPJSlr8PgN5ZXsoKD9AjPR67qlBelE/XlDhUm41ASSGdk+JQFQiU+MK1XSrqtbRrUdVu43VilPtp5XWiEK7XkhXvAMAI+EmNs0dqtCS7qtpeZ7h2S4JDBcI1Wjx2BaWi7opHV7DZwuvdUbVd4rQj67w4Kmq0WKaBU6s6zlGt5suR9VocmlJtXXSdl0rR9Vq0Gtr2qHud0e3oei1aDTVf1NrqwNTSrlbzpYZaMfWp12Kj5rorx6peS0NrtNR0nBDHE7ORj43M5jt2kcdGQgghhGhams2dFyGEEOJ4Io+NGk4GL0IIIUQMSNqo4WTwIoQQQsRAePDSmDsvv2Bnmhh550UIIYQQTYrceRFCCCFiQB4bNVyzGbzcqp5LMNdF2knp/ObbDEJBg/ZnjGLQ3EN0GToaVVM4/bGV9Bx1CYqmMPT+JfS7+HJ+c/9iQkGDAWMu4bIHF2OZFgPHXMhtTy1jyNjzcWgKD81cym/GjkDXVGa/spSLxpyFrim8+vJCxl99Lv964QMs0+Dmmy/kqSfeBGDK5DHcd/8c7pk6Drui8NcpL/LoAxOwqwo3/+VZ7nz0Bv486WkA7r/2Fsa//R4AEyYN4rJX/suf/jIUu2Lj7efmcFm/81FsMPuxHxjdawyKzcbTe3/iN93TeCx3OwBDO6Zwd0X8eVC7JPz5+zi9TSI2G5T5DjCgZQtsNigvOkTfrATKiw4B0CPdQ6DEB0C3FDchfzGdk90otnC0uV0LJzbC7TbeqshzVryOEfADkB6nRcWfw+0kp4ZiAzMUINFZFX9u4VQj0WavoyrmnKCH2/F6VFS6WrvqJmJcVAbZHdV2VUShXVpVzNlVSzw6OhZd2dYPi0/XFJWuFpuuoV1TfPrwdnSkuaaYs2qz1av9S8Scf8m4ssSchQiTF3YbTh4bCSGEEKJJaTZ3XoQQQojjiQWYjTy+uZLBixBCCBED8tio4eSxkRBCCCGaFLnzIoQQQsSApI0aTgYvQgghRAzIY6OGazaDl3eemYVN1fF9ORNv9g0Akbbvy5kANbYr913zSFV7/ZMz8b44i00zKvZ96jleeHFMuP3Q0zzy2z8BMGPa4/xj6A089rdNAPzlzLbcd8dPAFx/Skvu3Led8X0yAbjp4F4u75UOwIT8XH5/UgrjffsBOK9LUiS6fE6HFgRLfAxt7wUgWOLj9NbxAITKijm1pQcIx5X7ZMRF4so90lyRdtdkJ2YoQOekcLTZDAXokKhH2m29eiSi3Cahqt0y3o5lGrSMtwPhiHKmp6qdFlf11ynVXdVOjmonudRqvwK0cFa1vY6qdnwN7ehIdHTbXVs8uoZ29LqaItFQc/y5tkj00cSftQbODl3TrNISVxZCNHfNZvAihBBCHE/ksVHDyeBFCCGEiAF5bNRwMngRQgghYsBs5J0Xs/mOXZpGVHrmzJm0b98ep9NJv379WLZsWay7JIQQQpyw7r//fgYOHIjb7aZFixZHdYxlWUybNo2srCxcLheDBw9mw4YN1fYpLy/npptuIiUlhbi4OM4//3x2795d7/4d94OX119/nYkTJzJlyhS+/fZbzjzzTEaOHMnOnTtj3TUhhBCiwQzLavTyawkEAlx88cVcf/31R33Mww8/zOOPP86MGTNYuXIlGRkZnHPOORQVFUX2mThxIvPmzWPu3Ll8/vnnFBcXM2rUKAyjfvegjvvBy+OPP8748eO5+uqr6d69O08++SStW7fm2WefjXXXhBBCiAYzqHhpt6HLr9i3u+++m1tvvZWePXse1f6WZfHkk08yZcoURo8eTY8ePZg9ezalpaW89tprAPh8PmbNmsVjjz3GsGHD6NOnD6+++irr1q3j448/rlf/jut3XgKBAKtWreLOO++stn748OEsX768xmPKy8spLy+PfO3zhWdEtowgAIWFhVhGoFq7sLCwYp8j24fve7TH/RLnkM+Wz5bPls+Wzz7Wnx3+XmEdg5dhA42a2ajq+Mq+V3I4HDgcjkadu762bdtGbm4uw4cPr9aPQYMGsXz5cq699lpWrVpFMBistk9WVhY9evRg+fLljBgx4ug/0DqO7dmzxwKsL774otr6+++/3+rSpUuNx0ydOtUiPF+VLLLIIosssjRo2bVr16/2vc3v91sZGRm/SD89Hs8R66ZOnfqL9fWll16yvF5vnft98cUXFmDt2bOn2voJEyZYw4cPtyzLsubMmWPpun7Eseecc451zTXX1Ktfx/Wdl0q2w6pkWZZ1xLpKkydPZtKkSZGvCwoKaNu2LTt37sTr9f6q/TyRFBYW0rp1a3bt2kVCQkKsu9MkyDVrGLlu9SfXrGGO5rpZlkVRURFZWVm/Wj+cTifbtm0jEAg0+lw1fT+s7a7LtGnTuPvuu3/2fCtXrqR///4N7k99vl/XZ5/DHdeDl5SUFFRVJTc3t9r6vLw80tPTazymtttlXq9X/pE3QEJCgly3epJr1jBy3epPrlnD1HXdjsUPuk6nE6fT+at/TrQbb7yRMWPG/Ow+7dq1a9C5MzIyAMjNzSUzMzOyPvr7dUZGBoFAgPz8fBITE6vtM3DgwHp93nH9wq6u6/Tr149FixZVW79o0aJ6/0aFEEKI5iwlJYVu3br97NLQAVX79u3JyMio9v06EAiwdOnSyPfrfv36Ybfbq+2Tk5PD+vXr6/09/bi+8wIwadIkxo0bR//+/cnOzuaFF15g586dXHfddbHumhBCCHFC2rlzJ4cOHWLnzp0YhsGaNWsA6NSpEx5PeA69bt26MX36dC688EJsNhsTJ07kgQceoHPnznTu3JkHHngAt9vN2LFjgfAdrfHjx3PbbbeRnJxMUlISt99+Oz179mTYsGH16t9xP3i59NJLOXjwIPfccw85OTn06NGD+fPn07Zt26M63uFwMHXq1GP+5nVTJ9et/uSaNYxct/qTa9Ywct2O3j/+8Q9mz54d+bpPnz4ALF68mMGDBwOwefPmSKIX4I477sDv93PDDTeQn5/PgAEDWLhwIfHx8ZF9nnjiCTRN45JLLsHv9zN06FBefvllVLVqIt6jYbOsZjw5ghBCCCGanOP6nRchhBBCiMPJ4EUIIYQQTYoMXoQQQgjRpMjgRQghhBBNygk9eJk5cybt27fH6XTSr18/li1bFusuxdRnn33GeeedR1ZWFjabjXfeeafadusYTmfeVEyfPp1TTjmF+Ph40tLS+N3vfsfmzZur7SPXrbpnn32WXr16RQqBZWdn8+GHH0a2y/Wq2/Tp0yPR00py3Y40bdo0bDZbtaWyWBrINTuh1WsygSZk7ty5lt1ut1588UVr48aN1i233GLFxcVZO3bsiHXXYmb+/PnWlClTrLfeessCrHnz5lXb/uCDD1rx8fHWW2+9Za1bt8669NJLrczMTKuwsDCyz3XXXWe1bNnSWrRokbV69WpryJAhVu/eva1QKHSMfzfHxogRI6yXXnrJWr9+vbVmzRrrt7/9rdWmTRuruLg4so9ct+ree+8964MPPrA2b95sbd682brrrrssu91urV+/3rIsuV51+frrr6127dpZvXr1sm655ZbIerluR5o6dap18sknWzk5OZElLy8vsl2u2YnrhB28nHrqqdZ1111XbV23bt2sO++8M0Y9Or4cPngxTdPKyMiwHnzwwci6srIyy+v1Ws8995xlWZZVUFBg2e12a+7cuZF99uzZYymKYi1YsOCY9T2W8vLyLMBaunSpZVly3Y5WYmKi9a9//UuuVx2Kioqszp07W4sWLbIGDRoUGbzIdavZ1KlTrd69e9e4Ta7Zie2EfGwUCARYtWpVtWm3AYYPH87y5ctj1KvjW13TmQN1TmfeHFQWZEpKSgLkutXFMAzmzp1LSUkJ2dnZcr3q8Oc//5nf/va3R1QbletWuy1btpCVlUX79u0ZM2YMW7duBeSaneiO+wq7DXHgwAEMwzhi8sb09PQjJnkUYZXXpaZrtmPHjsg+uq5Xm1Crcp/mcF0ty2LSpEmcccYZ9OjRA5DrVpt169aRnZ1NWVkZHo+HefPmcdJJJ0W+Icj1OtLcuXNZvXo1K1euPGKb/D2r2YABA3jllVfo0qUL+/bt47777mPgwIFs2LBBrtkJ7oQcvFRqyNTczd2xms68KbrxxhtZu3Ytn3/++RHb5LpV17VrV9asWUNBQQFvvfUWV155JUuXLo1sl+tV3a5du7jllltYuHDhz06MJ9etupEjR0baPXv2JDs7m44dOzJ79mxOO+00QK7ZieqEfGyUkpKCqqpHjJyjp+YW1UVPZx6ttunMa9vnRHXTTTfx3nvvsXjxYlq1ahVZL9etZrqu06lTJ/r378/06dPp3bs3Tz31lFyvWqxatYq8vDz69euHpmlomsbSpUv55z//iaZpkd+3XLefFxcXR8+ePdmyZYv8XTvBnZCDF13X6devX7VptwEWLVpU72m3m4tjPZ15U2FZFjfeeCNvv/02n376Ke3bt6+2Xa7b0bEsi/LycrletRg6dCjr1q1jzZo1kaV///5cfvnlrFmzhg4dOsh1Owrl5eVs2rSJzMxM+bt2oovFW8LHQmVUetasWdbGjRutiRMnWnFxcdb27dtj3bWYKSoqsr799lvr22+/tQDr8ccft7799ttIfPzBBx+0vF6v9fbbb1vr1q2zLrvsshpjha1atbI+/vhja/Xq1dbZZ599QscKr7/+esvr9VpLliypFscsLS2N7CPXrbrJkydbn332mbVt2zZr7dq11l133WUpimItXLjQsiy5XkcrOm1kWXLdanLbbbdZS5YssbZu3WqtWLHCGjVqlBUfHx/5f16u2YnrhB28WJZlPfPMM1bbtm0tXdetvn37RuKtzdXixYst4IjlyiuvtCwrHC2cOnWqlZGRYTkcDuuss86y1q1bV+0cfr/fuvHGG62kpCTL5XJZo0aNsnbu3BmD382xUdP1AqyXXnopso9ct+quuuqqyL+71NRUa+jQoZGBi2XJ9Tpahw9e5LodqbJui91ut7KysqzRo0dbGzZsiGyXa3bislmWZcXmno8QQgghRP2dkO+8CCGEEOLEJYMXIYQQQjQpMngRQgghRJMigxchhBBCNCkyeBFCCCFEkyKDFyGEEEI0KTJ4EUIIIUSTIoMXIQSDBw9m4sSJse6GEEIcFRm8CCGEEKJJkcGLEEIIIZoUGbwI0cyUlJRwxRVX4PF4yMzM5LHHHot1l4QQol5k8CJEM/OXv/yFxYsXM2/ePBYuXMiSJUtYtWpVrLslhBBHTYt1B4QQx05xcTGzZs3ilVde4ZxzzgFg9uzZtGrVKsY9E0KIoyd3XoRoRn766ScCgQDZ2dmRdUlJSXTt2jWGvRJCiPqRwYsQzYhlWbHughBCNJoMXoRoRjp16oTdbmfFihWRdfn5+fzwww8x7JUQQtSPvPMiRDPi8XgYP348f/nLX0hOTiY9PZ0pU6agKPJzjBCi6ZDBixDNzCOPPEJxcTHnn38+8fHx3Hbbbfh8vlh3SwghjprNkofgQgghhGhC5F6xEEIIIZoUGbwIIYQQokmRwYsQQgghmhQZvAghhBCiSZHBixBCCCGaFBm8CCGEEKJJkcGLEEIIIZoUGbwIIYQQokmRwYsQQgghmhQZvAghhBCiSZHBixBCCCGaFBm8CCGEEKJJ+f+Qq/cSuyrCXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_positions = 50\n",
    "d = 512\n",
    "\n",
    "pos_encoding = positional_encoding(num_positions, d)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu') # we take pos_encoding[0] (we remove batch dimension)\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, d))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014364d",
   "metadata": {},
   "source": [
    "Each row represents a positional encoding - notice how none of the rows are identical! We have created a unique positional encoding for each of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d29500",
   "metadata": {},
   "source": [
    "Finally, let's look at an example where we add the positional encoding to the word embeddings.\n",
    "\n",
    "<img src=\"images/PosEncoding.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c679e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput embedding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[0.1 0.2 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]], shape=(3, 4), dtype=float32)\n",
      "\u001b[1m\n",
      "Positional Encoding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[[ 0.     1.     0.     1.   ]\n",
      "  [ 0.841  0.54   0.01   1.   ]\n",
      "  [ 0.909 -0.416  0.02   1.   ]]], shape=(1, 3, 4), dtype=float32)\n",
      "\u001b[1m\n",
      "Sum of the embedding with positional encoding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[0.1   1.2   0.3   1.4  ]\n",
      " [1.341 1.14  0.71  1.8  ]\n",
      " [1.809 0.584 1.12  2.2  ]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let' say that we have an embedding with 3 tokens and 4 dimensions\n",
    "\n",
    "\n",
    "# input text: \"I am happy\"\n",
    "embedding = np.array([[0.1, 0.2, 0.3, 0.4], # corresponds to \"I\"\n",
    "                     [0.5, 0.6, 0.7, 0.8],  # corresponds to \"am\"\n",
    "                     [0.9, 1.0, 1.1, 1.2]]) # corresponds to \"happy\"\n",
    "\n",
    "embedding = tf.constant(embedding, dtype=tf.float32)\n",
    "\n",
    "# Positional encoding for a sequence of length 3 and embedding size 4\n",
    "pos_encoding = positional_encoding(3, 4)\n",
    "\n",
    "# sum of embedding with positional encoding\n",
    "encoded_input = embedding + pos_encoding[0]\n",
    "\n",
    "# Results\n",
    "print(\"\\033[1mInput embedding:\\033[0m\")\n",
    "print(embedding)\n",
    "\n",
    "print(\"\\033[1m\\nPositional Encoding:\\033[0m\")\n",
    "print(pos_encoding)\n",
    "\n",
    "print(\"\\033[1m\\nSum of the embedding with positional encoding:\\033[0m\")\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5b752",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <font color='red'> <b>  4. Masking </b> </font>\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: \n",
    "\n",
    "- The *padding mask*\n",
    "- The *look-ahead mask*\n",
    "\n",
    "Both help the softmax computation give the appropriate weights to the words in your input sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a71e83",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### <font color='orange'> <b> 4.1. Padding mask </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991f41c",
   "metadata": {},
   "source": [
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, they zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by a small negative number (for example -1e9) and adding it to your sequence, you mask out the zeros by setting them to close to negative infinity.\n",
    "\n",
    "After masking, your input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when you take the softmax, the zeros don't affect the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283a014",
   "metadata": {},
   "source": [
    "Let's remember that in in a softmax function, when x approach to -infinity, the value tends to 0.\n",
    "\n",
    "<img src=\"images/softmax.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c2a64",
   "metadata": {},
   "source": [
    "We will use [tf.math.equal](https://www.tensorflow.org/api_docs/python/tf/math/equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8e4a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1 0 3]\n",
      " [4 0 6]]\n",
      "\n",
      " Mask:\n",
      " [[False  True False]\n",
      " [False  True False]]\n",
      "\n",
      " Mask Float:\n",
      " [[0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example sequence\n",
    "seq = tf.constant([[1, 0, 3], [4, 0, 6]])\n",
    "print(f\"Original data:\\n {seq}\")\n",
    "\n",
    "# Check where elements are equal to 0\n",
    "mask = tf.math.equal(seq, 0)  # Result: [[False, True, False], [False, True, False]]\n",
    "print(f\"\\n Mask:\\n {mask}\")\n",
    "\n",
    "# Convert to float32\n",
    "mask_float = tf.cast(mask, tf.float32)  # Result: [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]\n",
    "\n",
    "print(f\"\\n Mask Float:\\n {mask_float}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c57fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] # shape (n, 1, 1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "797089cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's try our function\n",
    "\n",
    "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c93471",
   "metadata": {},
   "source": [
    "We can see that in every place where there was a number other than 0, it was changed to 0, and in the places where there was a 1, it was changed to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618dca1",
   "metadata": {},
   "source": [
    "Let's multiply this by a small number (1e-9 in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d592bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[-0.e+00, -0.e+00, -1.e+09, -1.e+09, -0.e+00]]],\n",
       "\n",
       "\n",
       "       [[[-0.e+00, -0.e+00, -0.e+00, -1.e+09, -1.e+09]]],\n",
       "\n",
       "\n",
       "       [[[-1.e+09, -1.e+09, -1.e+09, -0.e+00, -0.e+00]]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask(x) * -1.0e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb306f16",
   "metadata": {},
   "source": [
    "We will now see that the difference between the softmax of x and that of x added to this mask is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed902c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[7.288e-01, 2.681e-01, 6.645e-04, 6.645e-04, 1.806e-03],\n",
       "       [8.444e-02, 2.295e-01, 6.239e-01, 3.106e-02, 3.106e-02],\n",
       "       [4.854e-03, 4.854e-03, 4.854e-03, 2.650e-01, 7.204e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.softmax(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b8030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 3, 5), dtype=float32, numpy=\n",
       "array([[[[7.297e-01, 2.685e-01, 0.000e+00, 0.000e+00, 1.809e-03],\n",
       "         [2.447e-01, 6.652e-01, 0.000e+00, 0.000e+00, 9.003e-02],\n",
       "         [6.648e-03, 6.648e-03, 0.000e+00, 0.000e+00, 9.867e-01]]],\n",
       "\n",
       "\n",
       "       [[[7.306e-01, 2.688e-01, 6.662e-04, 0.000e+00, 0.000e+00],\n",
       "         [9.003e-02, 2.447e-01, 6.652e-01, 0.000e+00, 0.000e+00],\n",
       "         [3.333e-01, 3.333e-01, 3.333e-01, 0.000e+00, 0.000e+00]]],\n",
       "\n",
       "\n",
       "       [[[0.000e+00, 0.000e+00, 0.000e+00, 2.689e-01, 7.311e-01],\n",
       "         [0.000e+00, 0.000e+00, 0.000e+00, 5.000e-01, 5.000e-01],\n",
       "         [0.000e+00, 0.000e+00, 0.000e+00, 2.689e-01, 7.311e-01]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.softmax(x + create_padding_mask(x) * -1.0e9, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af314c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Padding Mask\n",
    "    - A mask that indicates which elements in the sequence are padding and which are not.\n",
    "- Usage in Attention.\n",
    "    - It ensures that padding values do not affect the attention scores and, therefore, the result of the softmax.\n",
    "- Effect: \n",
    "    - Padding values do not influence the calculation of the output representations, allowing the model to focus only on the relevant parts of the sequence.\n",
    "This is crucial for proper training and for preventing the model from being \"confused\" by the artificially added zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e3e34",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <font color='orange'> <b> 4.2. Look-ahead mask </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e4c7",
   "metadata": {},
   "source": [
    "The look-ahead mask (or causal mask) is a special type of mask used in the Transformer, especially in the context of the decoder. Its main function is to prevent the model from using future information when predicting the next word or token in a sequence, ensuring that the model is autoregressive.\n",
    "\n",
    "Why is it used? The purpose of the look-ahead mask is to prevent the model from seeing the future when generating an output sequence. This means that when the model is predicting the token at position i, it should not \"see\" tokens at later positions (i+1, i+2, ...). This is crucial during both training and inference because, during inference, the model does not have access to future sequence information.\n",
    "\n",
    "How does it work? The look-ahead mask is a lower triangular matrix (with 1s in the lower part and 0s in the upper part), which is applied to the attention scores. This mask ensures that the attention values corresponding to future tokens in the sequence are masked.\n",
    "\n",
    "During the attention calculation, the attention scores for elements at future positions are set to a very negative value (e.g., negative infinity), so that after applying softmax, these positions do not influence the probability calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7997dd",
   "metadata": {},
   "source": [
    "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765213c1",
   "metadata": {},
   "source": [
    "For the implementation we will use [tf.linalg.band_part](https://www.tensorflow.org/api_docs/python/tf/linalg/band_part).\n",
    "\n",
    "More specifically:\n",
    "\n",
    "tf.linalg.band_part(matrix, -1, 0):\n",
    "\n",
    "This is a function that extracts the lower triangular part of a matrix, with the specific options given:\n",
    "\n",
    "- matrix: The matrix to extract from, which is the matrix of ones in this case.\n",
    "\n",
    "- -1: The number of diagonals to include below the main diagonal. A value of -1 means all diagonals below the main diagonal.\n",
    "\n",
    "- 0: The number of diagonals to include above the main diagonal. A value of 0 means no diagonals above the main diagonal are included.\n",
    "\n",
    "So, it creates a lower triangular matrix, meaning it keeps the elements below and on the main diagonal, and sets all the elements above the main diagonal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f2b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c27b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71b172",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Look-ahead mask: \n",
    "    - A mask used in the decoder to prevent the model from seeing future tokens when making predictions in a sequence. \n",
    "\n",
    "- Purpose: \n",
    "    - Ensure that the model is autoregressive and generates tokens in sequential order, without accessing future information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e22d2",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <font color='red'> <b>  5. Attention</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd9cd1",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### <font color='orange'> <b> 5.1. Types of Attention </b> </font>\n",
    "\n",
    "The 3 main types of attention in the transformer model are:\n",
    "\n",
    "- Encoder-decoder attention.\n",
    "- Self-Attention\n",
    "- Masked Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af0693",
   "metadata": {},
   "source": [
    "### Encoder decoder-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48295983",
   "metadata": {},
   "source": [
    "The words in one sentence attend to all other words in another one. \n",
    "\n",
    "That is, the <font color='blue'> queries</font> come from one sentence while the <font color='red'>keys </font> and <font color='#5b2c6f'> values </font> come from another.\n",
    "\n",
    "It is crucial for tasks like translation, where the decoder generates words based on the source sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffa698",
   "metadata": {},
   "source": [
    "<img src=\"images/encoder_decoeder_attention.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b0f61",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n",
    "**The queries, keys, and values come from the same sentence.**\n",
    "\n",
    "Every word attends to every other word in the sequence. \n",
    "\n",
    "This type of attention lets you get contextual representations of your words. In other terms, self-attention gives you a representation of the meaning of each word within the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac48e00",
   "metadata": {},
   "source": [
    "<img src=\"images/self_attention.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183d57c",
   "metadata": {},
   "source": [
    "### Masked self-attention\n",
    "\n",
    "Queries, keys and values also come from the same sentence, but **each query cannot attend to keys on future positions.**\n",
    "\n",
    "This attention mechanism is present in the decoder from the transformer model and ensures that predictions at each position depend only on the known outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d38f3",
   "metadata": {},
   "source": [
    "- The scale dot-product attention requires the calculation of the softmax of the scaled products between the queries and the transpose of the key matrix. Then for mask self-attention, you add a mask matrix within the softmax. \n",
    "    - The mask has a zero on all of its positions, except for the elements above the diagonal, which are set to minus infinity (or, in practice, a huge negative number).\n",
    "- After taking the softmax, this addition ensures that the elements in the weights matrix are 0 for all the keys and the subsequent positions to the query.\n",
    "- In the end, as with the other types of attention, you multiply the weights matrix by the value matrix to get the context vector for each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ff48",
   "metadata": {},
   "source": [
    "<img src=\"images/masked_self_attention.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd9a19",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### <font color='orange'> <b> 5.2. Scaled dot-product attention </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c4195",
   "metadata": {},
   "source": [
    "In scale dot-products attention:\n",
    "\n",
    "- We have queries, keys and values.\n",
    "- The attention layer outputs context vectors for each query.\n",
    "     - The context vectors are weighted sums of the values where the similarity between the queries and keys determines the weights assigned to each value.\n",
    "- The softmax ensures that the weights add up to 1.\n",
    "- The division by the square roots of the dimension of the key factors is used to improve performance.\n",
    "\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode.\n",
    "\n",
    "\n",
    "The scale dot-product attention mechanism is very efficient since it relies only on matrix multiplication and a softmax. Additionally, we can implement this attention mechanism to run on GPUs or TPUs to speed up training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d4450",
   "metadata": {},
   "source": [
    "To get the query, key and value matrices, you must first transform the words in your sequences toward embeddings.\n",
    "\n",
    "- Let 's take the sentence \"Soy feliz\" as the source for the queries. \n",
    "    - You'll need to get the embedding vector for the words \"Soy\" and \"feliz\".\n",
    "    - The query matrix will contain all of these embedding vectors as rows.\n",
    "    - The matrix size is given by the size of the word embeddings and the length of the sequence. \n",
    "        - If the word embbedimgs size is for example d=4, then, in this example, the size will be 4x2 (because the length of our sequence (\"Soy feliz\") is 2).\n",
    "- To get the key matrix, let's use the sentence “I am happy”. You will get the embedding for each word in the sentence and stack them together to form the key matrix.\n",
    "    - The size will be: sequence length of the keys x dimension of the embedding (3 x 4 in this example).\n",
    "\n",
    "You will generally use the same vectors used for the key matrix for the value matrix. But you could also transform them first.\n",
    "\n",
    "Note that the number of vectors used to form the key and value matrix must be the same. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcfaf8",
   "metadata": {},
   "source": [
    "<img src=\"images/scaled_dot_product.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b88fa",
   "metadata": {},
   "source": [
    "Now, we can implement the formula.\n",
    "\n",
    "- First, you compute the matrix products between the query and the transpose of the key matrix. You scale it by the inverse of the square of the dimension of the key vectors, dk and calculate the softmax. \n",
    "    - This computation will give you a matrix with the weights for each key per query. Therefore the weight matrix will have a total number of elements equal to the number of queries times the number of keys.\n",
    "        - For example, the third element in the second row would correspond to the weights assigned to the third key for the second query.\n",
    "- After the computation of the weights matrix, you can multiply it with the value matrix to get a matrix that has rows and the context vector corresponding to each query. \n",
    "    - The number of columns on this matrix is equal to the size of the value vectors, which is often the same as the embedding size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba115e3",
   "metadata": {},
   "source": [
    "<img src=\"images/math_scd.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a52c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_v0(q, k, v):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0306cc0",
   "metadata": {},
   "source": [
    "**Note.** About the ellipsis in the docstring.\n",
    "\n",
    "The ... (ellipsis) represents the batch dimensions and any additional dimensions (e.g., for multi-head attention or hierarchical models). In simpler cases, this is just the batch size. For example:\n",
    "\n",
    "- In a single-head attention scenario, the shape might be (batch_size,).\n",
    "- In a multi-head attention scenario, the shape might be (batch_size, num_heads).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bb019ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery shape:\u001b[0m (2, 3, 5)\n",
      "\u001b[1mKey shape:\u001b[0m (2, 4, 5)\n",
      "\u001b[1mValue shape:\u001b[0m (2, 4, 6)\n",
      "\u001b[1mOutput shape:\u001b[0m (2, 3, 6)\n",
      "\u001b[1mAttention weights shape:\u001b[0m (2, 3, 4)\n",
      "\n",
      "\u001b[1mAttention Weights:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[0.259 0.257 0.217 0.266]\n",
      "  [0.25  0.215 0.209 0.326]\n",
      "  [0.274 0.267 0.188 0.272]]\n",
      "\n",
      " [[0.289 0.262 0.24  0.209]\n",
      "  [0.28  0.29  0.24  0.19 ]\n",
      "  [0.29  0.25  0.239 0.221]]], shape=(2, 3, 4), dtype=float32)\n",
      "\n",
      "\u001b[1mOutput:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[0.556 0.206 0.584 0.392 0.253 0.557]\n",
      "  [0.532 0.189 0.579 0.375 0.248 0.602]\n",
      "  [0.573 0.205 0.582 0.372 0.238 0.548]]\n",
      "\n",
      " [[0.36  0.586 0.605 0.499 0.489 0.295]\n",
      "  [0.363 0.594 0.611 0.501 0.47  0.301]\n",
      "  [0.359 0.581 0.604 0.502 0.499 0.291]]], shape=(2, 3, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: Query, Key, Value tensors\n",
    "batch_size = 2  # Number of sequences in the batch\n",
    "seq_len_q = 3   # Length of the query sequence\n",
    "seq_len_kv = 4  # Length of the key and value sequences\n",
    "depth = 5       # Embedding size for query/key\n",
    "depth_v = 6     # Embedding size for value\n",
    "\n",
    "# Generate random tensors for q, k, v\n",
    "q = tf.random.uniform((batch_size, seq_len_q, depth))   # Shape: (2, 3, 5)\n",
    "k = tf.random.uniform((batch_size, seq_len_kv, depth))  # Shape: (2, 4, 5)\n",
    "v = tf.random.uniform((batch_size, seq_len_kv, depth_v))  # Shape: (2, 4, 6)\n",
    "\n",
    "# Apply scaled dot-product attention\n",
    "output, attention_weights = scaled_dot_product_attention_v0(q, k, v)\n",
    "\n",
    "# Print shapes and results\n",
    "print(\"\\033[1mQuery shape:\\033[0m\", q.shape)\n",
    "print(\"\\033[1mKey shape:\\033[0m\", k.shape)\n",
    "print(\"\\033[1mValue shape:\\033[0m\", v.shape)\n",
    "print(\"\\033[1mOutput shape:\\033[0m\", output.shape)  # Should match (batch_size, seq_len_q, depth_v)\n",
    "print(\"\\033[1mAttention weights shape:\\033[0m\", attention_weights.shape)  # Should match (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "# Print the tensors (optional)\n",
    "print(\"\\n\\033[1mAttention Weights:\\033[0m\\n\", attention_weights)\n",
    "print(\"\\n\\033[1mOutput:\\033[0m\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a9771",
   "metadata": {},
   "source": [
    "Now, let's generalize the previous function to include an optional mask that will allow us to implement masked self-attention.\n",
    "\n",
    "Mathematicaly:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df77c5c",
   "metadata": {},
   "source": [
    "<img src=\"images/math_scdt_mask.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe40ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c85e37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery shape:\u001b[0m (2, 3, 5)\n",
      "\u001b[1mKey shape:\u001b[0m (2, 4, 5)\n",
      "\u001b[1mValue shape:\u001b[0m (2, 4, 6)\n",
      "\u001b[1mMask shape:\u001b[0m (2, 1, 1, 4)\n",
      "\u001b[1mOutput shape:\u001b[0m (2, 2, 3, 6)\n",
      "\u001b[1mAttention weights shape:\u001b[0m (2, 2, 3, 4)\n",
      "\n",
      "\u001b[1mAttention Weights:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[[0.314 0.457 0.229 0.   ]\n",
      "   [0.329 0.433 0.238 0.   ]\n",
      "   [0.252 0.548 0.2   0.   ]]\n",
      "\n",
      "  [[0.307 0.309 0.384 0.   ]\n",
      "   [0.297 0.306 0.397 0.   ]\n",
      "   [0.335 0.272 0.393 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.339 0.    0.248 0.413]\n",
      "   [0.332 0.    0.24  0.427]\n",
      "   [0.301 0.    0.239 0.46 ]]\n",
      "\n",
      "  [[0.327 0.    0.408 0.265]\n",
      "   [0.324 0.    0.433 0.242]\n",
      "   [0.349 0.    0.41  0.241]]]], shape=(2, 2, 3, 4), dtype=float32)\n",
      "\n",
      "\u001b[1mOutput:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[[0.444 0.679 0.782 0.446 0.248 0.502]\n",
      "   [0.461 0.675 0.781 0.454 0.25  0.493]\n",
      "   [0.377 0.698 0.784 0.414 0.24  0.538]]\n",
      "\n",
      "  [[0.273 0.261 0.646 0.733 0.605 0.488]\n",
      "   [0.273 0.265 0.646 0.736 0.612 0.492]\n",
      "   [0.261 0.26  0.668 0.719 0.601 0.51 ]]]\n",
      "\n",
      "\n",
      " [[[0.782 0.568 0.713 0.368 0.426 0.58 ]\n",
      "   [0.782 0.566 0.712 0.362 0.431 0.586]\n",
      "   [0.78  0.568 0.705 0.337 0.441 0.614]]\n",
      "\n",
      "  [[0.338 0.319 0.688 0.503 0.531 0.63 ]\n",
      "   [0.324 0.321 0.697 0.52  0.546 0.638]\n",
      "   [0.323 0.312 0.701 0.512 0.532 0.633]]]], shape=(2, 2, 3, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: Query, Key, Value tensors and Mask\n",
    "batch_size = 2  # Number of sequences in the batch\n",
    "seq_len_q = 3   # Length of the query sequence\n",
    "seq_len_kv = 4  # Length of the key and value sequences\n",
    "depth = 5       # Embedding size for query/key\n",
    "depth_v = 6     # Embedding size for value\n",
    "\n",
    "# Generate random tensors for q, k, v\n",
    "q = tf.random.uniform((batch_size, seq_len_q, depth))   # Shape: (2, 3, 5)\n",
    "k = tf.random.uniform((batch_size, seq_len_kv, depth))  # Shape: (2, 4, 5)\n",
    "v = tf.random.uniform((batch_size, seq_len_kv, depth_v))  # Shape: (2, 4, 6)\n",
    "\n",
    "# Generate a mask (e.g., padding mask)\n",
    "mask = tf.constant([[0, 0, 0, 1], [0, 1, 0, 0]], dtype=tf.float32)  # Shape: (2, 4)\n",
    "mask = tf.reshape(mask, (batch_size, 1, 1, seq_len_kv))  # Reshape to broadcast shape\n",
    "\n",
    "# Apply scaled dot-product attention with masking\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "# Print shapes and results\n",
    "print(\"\\033[1mQuery shape:\\033[0m\", q.shape)\n",
    "print(\"\\033[1mKey shape:\\033[0m\", k.shape)\n",
    "print(\"\\033[1mValue shape:\\033[0m\", v.shape)\n",
    "print(\"\\033[1mMask shape:\\033[0m\", mask.shape)\n",
    "print(\"\\033[1mOutput shape:\\033[0m\", output.shape)  # Should match (batch_size, seq_len_q, depth_v)\n",
    "print(\"\\033[1mAttention weights shape:\\033[0m\", attention_weights.shape)  # Should match (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "# Print the tensors (optional)\n",
    "print(\"\\n\\033[1mAttention Weights:\\033[0m\\n\", attention_weights)\n",
    "print(\"\\n\\033[1mOutput:\\033[0m\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f192f3",
   "metadata": {},
   "source": [
    "<a name=\"5.3\"></a>\n",
    "### <font color='orange'> <b> 5.3. Multi-head Attention </b> </font>\n",
    "\n",
    "You can think of as computing the self-attention several times to detect different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed0072",
   "metadata": {},
   "source": [
    "- You need word embeddings for the query, key and value matrices and scale dot-product attention. \n",
    "- In multi-head attention, you apply in parallel the attention mechanism to multiple sets of these matrices that you can get by transforming the original embeddings.\n",
    "- In multi-head attention, the number of times that you apply the attention mechanism is the **number of heads** in the model. \n",
    "    - For instance, you will need two sets of queries, keys, and values, in a model with two heads.The first head would use one set of representations, and the second head would use a different set.\n",
    "- In the transformer model, you get different representations by linearly transforming the original embeddings by using a set of matrices, WQ, WK and WV for each head in the model. Using different sets of representations allow your model to learn multiple relationships between the words from the query and key matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a421c",
   "metadata": {},
   "source": [
    "It works this way:\n",
    "\n",
    "- The inputs to multi-head attention are the value key and query matrices. \n",
    "- First, you transform each of these matrices into multiple vector spaces. \n",
    "    - The number of transformations for each matrix is equal to the number of heads in the model. \n",
    "- Then, you will apply the scale dot-product attention mechanism to every set of value, key and query transformations. \n",
    "    - The number of sets is equal to the number of heads and the model.\n",
    "- After that, you concatenate the results from each head in the model into a single matrix.\n",
    "- Finally, you transform the resulting matrix to get the output context vectors.\n",
    "\n",
    "Note that every linear transformation in multi-headed attention contains a set of learnable parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef90da",
   "metadata": {},
   "source": [
    "<img src=\"images/MultiHead.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a634b",
   "metadata": {},
   "source": [
    "For the MultiHeadAttention layer, we will use the [tf.keras.layers.MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "474e04d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 5)\n",
      "Attention Output (attn1): (3, 5, 4)\n",
      "Attention Weights (attn_weights): (3, 2, 5, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5, 4), dtype=float32, numpy=\n",
       " array([[[-0.11 , -0.003, -0.206,  0.349],\n",
       "         [ 0.007,  0.064, -0.195,  0.46 ],\n",
       "         [ 0.024,  0.068, -0.237,  0.51 ],\n",
       "         [-0.028,  0.052, -0.269,  0.525],\n",
       "         [-0.015,  0.059, -0.263,  0.532]],\n",
       " \n",
       "        [[-0.115,  0.012, -0.315,  0.794],\n",
       "         [-0.042,  0.041, -0.281,  0.511],\n",
       "         [-0.016,  0.069, -0.183,  0.512],\n",
       "         [-0.036,  0.057, -0.236,  0.572],\n",
       "         [-0.08 ,  0.036, -0.274,  0.635]],\n",
       " \n",
       "        [[-0.284, -0.07 , -0.477,  0.555],\n",
       "         [-0.253, -0.044, -0.468,  0.688],\n",
       "         [-0.147,  0.016, -0.323,  0.609],\n",
       "         [-0.098,  0.034, -0.333,  0.618],\n",
       "         [-0.083,  0.055, -0.317,  0.644]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 2, 5, 5), dtype=float32, numpy=\n",
       " array([[[[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.494, 0.506, 0.   , 0.   , 0.   ],\n",
       "          [0.333, 0.335, 0.332, 0.   , 0.   ],\n",
       "          [0.246, 0.253, 0.255, 0.247, 0.   ],\n",
       "          [0.197, 0.201, 0.201, 0.2  , 0.201]],\n",
       " \n",
       "         [[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.539, 0.461, 0.   , 0.   , 0.   ],\n",
       "          [0.353, 0.312, 0.335, 0.   , 0.   ],\n",
       "          [0.259, 0.234, 0.247, 0.259, 0.   ],\n",
       "          [0.211, 0.184, 0.196, 0.216, 0.193]]],\n",
       " \n",
       " \n",
       "        [[[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.489, 0.511, 0.   , 0.   , 0.   ],\n",
       "          [0.346, 0.327, 0.327, 0.   , 0.   ],\n",
       "          [0.256, 0.24 , 0.247, 0.256, 0.   ],\n",
       "          [0.207, 0.187, 0.19 , 0.208, 0.208]],\n",
       " \n",
       "         [[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.485, 0.515, 0.   , 0.   , 0.   ],\n",
       "          [0.322, 0.366, 0.312, 0.   , 0.   ],\n",
       "          [0.236, 0.29 , 0.224, 0.251, 0.   ],\n",
       "          [0.187, 0.238, 0.178, 0.199, 0.198]]],\n",
       " \n",
       " \n",
       "        [[[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.488, 0.512, 0.   , 0.   , 0.   ],\n",
       "          [0.333, 0.341, 0.326, 0.   , 0.   ],\n",
       "          [0.249, 0.252, 0.252, 0.248, 0.   ],\n",
       "          [0.198, 0.203, 0.194, 0.204, 0.201]],\n",
       " \n",
       "         [[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "          [0.54 , 0.46 , 0.   , 0.   , 0.   ],\n",
       "          [0.363, 0.328, 0.309, 0.   , 0.   ],\n",
       "          [0.281, 0.25 , 0.224, 0.245, 0.   ],\n",
       "          [0.236, 0.203, 0.183, 0.195, 0.184]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MultiHeadAttention layer\n",
    "mha = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=4)\n",
    "\n",
    "# Create dummy input tensor with shape (batch_size, seq_len, d_model)\n",
    "x = np.random.rand(3, 5, 4).astype(np.float32)  # Batch size = 3, Seq_len = 5, d_model = 4\n",
    "\n",
    "# Create a look-ahead mask (in this example, it is a simple upper-triangular matrix)\n",
    "look_ahead_mask = create_look_ahead_mask(5) # (5,5)\n",
    "\n",
    "# Add batch dimension to the mask\n",
    "look_ahead_mask = look_ahead_mask[tf.newaxis, :, :]  # Shape (1, 5, 5)\n",
    "look_ahead_mask = tf.tile(look_ahead_mask, [3, 1, 1])  # Tile to match the batch size: shape (3, 5, 5)\n",
    "\n",
    "\n",
    "print(look_ahead_mask.shape)\n",
    "\n",
    "# Call the MHA layer\n",
    "attn1, attn_weights = mha(x, x, x, attention_mask=look_ahead_mask, return_attention_scores=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Attention Output (attn1):\", attn1.shape)  # Should be (3, 5, 4)\n",
    "print(\"Attention Weights (attn_weights):\", attn_weights.shape)  # Should be (3, 2, 5, 5)\n",
    "\n",
    "attn1, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af501b",
   "metadata": {},
   "source": [
    "**Explanation of the Example:**\n",
    "\n",
    "- x\n",
    "    - The input tensor with shape (3, 5, 4) is a batch of 3 sequences, each of length 5 and with a model dimension of 4.\n",
    "\n",
    "- look_ahead_mask: \n",
    "    - A mask is created to prevent looking ahead. The mask is (5, 5) for each sequence and is tiled across the batch dimension, making it (3, 5, 5).\n",
    "\n",
    "- mha(x, x, x, attention_mask=look_ahead_mask, return_attention_scores=True):\n",
    "    - This computes the self-attention for each of the sequences, applies the mask to prevent looking ahead, and returns both the attention output (attn1) and the attention weights (attn_weights_block1).\n",
    "\n",
    "- Output:\n",
    "    - attn1.shape: The output of the attention mechanism after applying the weights will have the shape (batch_size, seq_len, d_model). In this case, (3, 5, 4) because we have 3 sequences, each with 5 tokens, and a model dimension of 4.\n",
    "    - attn_weights_block1.shape: This shows how each token in the sequence attends to every other token. Since we have 2 attention heads, the shape of the attention weights will be (3, 2, 5, 5).\n",
    "\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- The query, key, and value tensors are passed to the MHA layer.\n",
    "- The look_ahead_mask is applied to prevent attending to future tokens in the decoder.\n",
    "- The attention mechanism computes how much each token attends to other tokens, and the results are returned as both the attention output and the attention weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd9289",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## <font color='red'> <b>  6. Feed Forward Network </b> </font>\n",
    "\n",
    "I will contain 2 Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c26b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77041a87",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## <font color='red'> <b>  7. Encoder </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a899",
   "metadata": {},
   "source": [
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. You will implement the Encoder by pairing multi-head attention and a feed forward neural network.\n",
    "\n",
    "\n",
    "<img src=\"images/EncoderLayerV2.png\"/>\n",
    "\n",
    "    \n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20305f22",
   "metadata": {},
   "source": [
    "<a name=\"7.1\"></a>\n",
    "### <font color='orange'> <b> 7.1. Encoder Layer </b> </font>\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training.\n",
    "\n",
    "The function call() should perform the following steps: \n",
    "1. You will pass the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K should be the same.\n",
    "2. Next, you will pass the output of the multi-head attention layer to a dropout layer. Don't forget to use the `training` parameter to set the mode of your model. \n",
    "3. Now add a skip connection by adding your original input `x` and the output of the dropout layer. \n",
    "4. After adding the skip connection, pass the output through the first layer normalization.\n",
    "5. Finally, repeat steps 1-4 but with the feed forward neural network instead of the multi-head attention layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3581794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha\n",
    "        #-> To compute self-attention Q, V and K should be the same (x)\n",
    "        self_attn_output = self.mha(x, x, x, mask) # Self attention (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to the self-attention output\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer\n",
    "        mult_attn_out = self.layernorm1(x + self_attn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea4f9d",
   "metadata": {},
   "source": [
    "Let's try this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a745953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;90m\u001b[1mOriginal Input\u001b[0m\n",
      "['This is a test sentence.', \"Another example sentence for testing. It's just a simple sentence\", 'The model should process text correctly.']\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mTokenized Input\u001b[0m\n",
      "[[4, 5, 3, 6, 2], [7, 8, 2, 9, 10, 11, 12, 3, 13, 2], [14, 15, 16, 17, 18, 19]]\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mPadded Sequences\u001b[0m\n",
      "[[ 4  5  3  6  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 7  8  2  9 10 11 12  3 13  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [14 15 16 17 18 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mInput Tensor\u001b[0m\n",
      "tf.Tensor(\n",
      "[[ 4.  5.  3.  6.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 7.  8.  2.  9. 10. 11. 12.  3. 13.  2.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [14. 15. 16. 17. 18. 19.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]], shape=(3, 50), dtype=float32)\n",
      " Shape: (3, 50)\n",
      "\n",
      "\n",
      "\u001b[1m Embedded input \u001b[0m\n",
      "tf.Tensor(\n",
      "[[[-3.651e-02 -3.662e-02  2.339e-02 ...  2.637e-02  4.827e-02  5.742e-03]\n",
      "  [-3.822e-02 -3.534e-03  3.663e-02 ...  6.075e-03 -1.228e-02  1.634e-02]\n",
      "  [ 4.040e-02 -1.030e-02 -4.029e-02 ... -4.519e-02 -4.894e-03  4.192e-02]\n",
      "  ...\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]]\n",
      "\n",
      " [[ 5.409e-03 -9.045e-03 -2.808e-02 ... -2.107e-02 -2.939e-02  2.404e-02]\n",
      "  [-3.278e-02 -4.092e-02 -4.832e-02 ...  2.716e-02 -2.853e-02 -3.673e-02]\n",
      "  [-3.384e-02  1.383e-02 -2.426e-05 ... -4.105e-02 -2.196e-02 -2.025e-02]\n",
      "  ...\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]]\n",
      "\n",
      " [[ 1.569e-02 -4.494e-03  3.124e-02 ... -6.345e-03  2.890e-02  1.255e-04]\n",
      "  [-4.593e-02 -4.393e-02  4.694e-02 ... -2.376e-02  2.097e-02  1.641e-03]\n",
      "  [-3.152e-02 -2.553e-02  4.184e-02 ... -4.851e-02  1.866e-02  2.033e-02]\n",
      "  ...\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]\n",
      "  [ 5.015e-03  2.830e-02  6.788e-04 ... -3.724e-02  2.105e-02 -1.181e-02]]], shape=(3, 50, 128), dtype=float32)\n",
      "\n",
      "\n",
      "\u001b[1m Positional Encoding \u001b[0m\n",
      "tf.Tensor(\n",
      "[[[ 0.000e+00  1.000e+00  0.000e+00 ...  1.000e+00  0.000e+00  1.000e+00]\n",
      "  [ 8.415e-01  5.403e-01  7.617e-01 ...  1.000e+00  1.155e-04  1.000e+00]\n",
      "  [ 9.093e-01 -4.161e-01  9.870e-01 ...  1.000e+00  2.310e-04  1.000e+00]\n",
      "  ...\n",
      "  [ 1.236e-01 -9.923e-01  1.399e-01 ...  1.000e+00  5.427e-03  1.000e+00]\n",
      "  [-7.683e-01 -6.401e-01 -6.636e-01 ...  1.000e+00  5.543e-03  1.000e+00]\n",
      "  [-9.538e-01  3.006e-01 -9.998e-01 ...  1.000e+00  5.658e-03  1.000e+00]]], shape=(1, 50, 128), dtype=float32)\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mEmbedded Input with Positional Encoding\u001b[0m\n",
      "tf.Tensor(\n",
      "[[[-0.037  0.963  0.023 ...  1.026  0.048  1.006]\n",
      "  [ 0.803  0.537  0.798 ...  1.006 -0.012  1.016]\n",
      "  [ 0.95  -0.426  0.947 ...  0.955 -0.005  1.042]\n",
      "  ...\n",
      "  [ 0.129 -0.964  0.141 ...  0.963  0.026  0.988]\n",
      "  [-0.763 -0.612 -0.663 ...  0.963  0.027  0.988]\n",
      "  [-0.949  0.329 -0.999 ...  0.963  0.027  0.988]]\n",
      "\n",
      " [[ 0.005  0.991 -0.028 ...  0.979 -0.029  1.024]\n",
      "  [ 0.809  0.499  0.713 ...  1.027 -0.028  0.963]\n",
      "  [ 0.875 -0.402  0.987 ...  0.959 -0.022  0.98 ]\n",
      "  ...\n",
      "  [ 0.129 -0.964  0.141 ...  0.963  0.026  0.988]\n",
      "  [-0.763 -0.612 -0.663 ...  0.963  0.027  0.988]\n",
      "  [-0.949  0.329 -0.999 ...  0.963  0.027  0.988]]\n",
      "\n",
      " [[ 0.016  0.996  0.031 ...  0.994  0.029  1.   ]\n",
      "  [ 0.796  0.496  0.809 ...  0.976  0.021  1.002]\n",
      "  [ 0.878 -0.442  1.029 ...  0.951  0.019  1.02 ]\n",
      "  ...\n",
      "  [ 0.129 -0.964  0.141 ...  0.963  0.026  0.988]\n",
      "  [-0.763 -0.612 -0.663 ...  0.963  0.027  0.988]\n",
      "  [-0.949  0.329 -0.999 ...  0.963  0.027  0.988]]], shape=(3, 50, 128), dtype=float32)\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mMask\u001b[0m\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1.]]]], shape=(3, 1, 1, 50), dtype=float32)\n",
      "\n",
      "\n",
      "\u001b[1;90m\u001b[1mOutput of the Encoder Layer\u001b[0m\n",
      "tf.Tensor(\n",
      "[[[-0.91   0.152 -0.658 ...  0.956  0.088  1.368]\n",
      "  [ 0.341 -0.582  0.513 ...  1.166 -0.104  1.323]\n",
      "  [ 0.274 -1.657  0.489 ...  1.097 -0.092  1.206]\n",
      "  ...\n",
      "  [ 0.261 -1.806  0.143 ...  0.877  0.296  1.221]\n",
      "  [-1.12  -1.471 -0.949 ...  0.698 -0.304  1.291]\n",
      "  [-1.57   0.233 -1.151 ...  0.546  0.263  1.1  ]]\n",
      "\n",
      " [[-0.888  0.223 -0.938 ...  0.961 -0.047  1.47 ]\n",
      "  [ 0.396 -0.606  0.137 ...  1.237 -0.135  1.152]\n",
      "  [ 0.055 -2.323  0.468 ...  1.076 -0.173  1.018]\n",
      "  ...\n",
      "  [ 0.258 -1.768  0.025 ...  0.906  0.276  1.282]\n",
      "  [-1.164 -1.33  -1.009 ...  0.681  0.378  1.264]\n",
      "  [-1.533  0.244 -1.348 ...  0.546  0.263  1.155]]\n",
      "\n",
      " [[-0.794  0.192 -0.66  ...  0.904  0.098  0.914]\n",
      "  [ 0.305 -0.702  0.35  ...  1.243  0.059  1.24 ]\n",
      "  [ 0.171 -2.429  0.602 ...  1.067 -0.112  1.041]\n",
      "  ...\n",
      "  [ 0.238 -1.83  -0.013 ...  0.866  0.261  1.235]\n",
      "  [-1.133 -1.298 -0.983 ...  0.68   0.392  1.272]\n",
      "  [-1.532  0.196 -1.297 ...  0.564  0.43   1.138]]], shape=(3, 50, 128), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Define parameters for the Encoder Layer\n",
    "embedding_dim = 128\n",
    "num_heads = 8\n",
    "fully_connected_dim = 512\n",
    "dropout_rate = 0.1\n",
    "layernorm_eps = 1e-6\n",
    "input_vocab_size = 1000  # Define a vocabulary size for the tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Example text data\n",
    "text_data = [\n",
    "    \"This is a test sentence.\",\n",
    "    \"Another example sentence for testing. It's just a simple sentence\",\n",
    "    \"The model should process text correctly.\"\n",
    "]\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mOriginal Input\\033[0m\")\n",
    "print(text_data)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(num_words=input_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mTokenized Input\\033[0m\")\n",
    "print(sequences)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Pad sequences to ensure consistent length\n",
    "max_length = 50  # Matches input_seq_len for the model\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mPadded Sequences\\033[0m\")\n",
    "print(padded_sequences)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Convert to Tensor\n",
    "input_tensor = tf.convert_to_tensor(padded_sequences, dtype=tf.float32)\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mInput Tensor\\033[0m\")\n",
    "print(input_tensor)\n",
    "print(f\" Shape: {input_tensor.shape}\")  # 3x50; 3 sentences with max_length 50\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create an Embedding layer to convert tokens to dense vectors\n",
    "embedding_layer = Embedding(input_dim=input_vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "# Embed the input sequences\n",
    "embedded_input = embedding_layer(input_tensor)  # Shape: (batch_size, max_length, embedding_dim)\n",
    "        # Example: (3, 50, 128) for 3 sentences in a batch, each with a maximum length of 50, \n",
    "        # and an embedding dimension of 128. \n",
    "\n",
    "\n",
    "print(f\"\\033[1m Embedded input \\033[0m\")\n",
    "print(embedded_input)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Add positional encoding\n",
    "positions = max_length\n",
    "pos_encoding = positional_encoding(positions, embedding_dim)\n",
    "\n",
    "print(f\"\\033[1m Positional Encoding \\033[0m\")\n",
    "print(pos_encoding) # shape: (1, max_length, embedding_dim)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "embedded_with_positional_encoding = embedded_input + pos_encoding\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mEmbedded Input with Positional Encoding\\033[0m\")\n",
    "print(embedded_with_positional_encoding)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create a padding mask for the input\n",
    "mask = create_padding_mask(input_tensor)\n",
    "#mask = tf.cast(tf.not_equal(input_tensor, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "print(f\"\\033[1;90m\\033[1mMask\\033[0m\")\n",
    "print(mask)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Instantiate the EncoderLayer\n",
    "encoder_layer = EncoderLayer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    fully_connected_dim=fully_connected_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layernorm_eps=layernorm_eps\n",
    ")\n",
    "\n",
    "# Test the EncoderLayer\n",
    "output = encoder_layer(embedded_with_positional_encoding, training=True, mask=mask)\n",
    "\n",
    "# Print the output\n",
    "print(f\"\\033[1;90m\\033[1mOutput of the Encoder Layer\\033[0m\")\n",
    "print(output)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a74950",
   "metadata": {},
   "source": [
    "<a name=\"7.2\"></a>\n",
    "### <font color='orange'> <b> 7.2. Full Encoder </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c6ada",
   "metadata": {},
   "source": [
    "Now you're ready to build the full Transformer Encoder, where you will embedd your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"images/FullEncoder.png\" alt=\"Encoder\" width=\"750\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You will initialize your Encoder with an Embedding layer, positional encoding, and multiple EncoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your input through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6f368",
   "metadata": {},
   "source": [
    "<img src=\"images/encoder.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d59b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "\n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x,training, mask)\n",
    "\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc409a6b",
   "metadata": {},
   "source": [
    "Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d523abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input tensor:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[ 3  4  5  6  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 7  8  2  9 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [11 12 13 14 15 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]], shape=(3, 50), dtype=int32)\n",
      "\n",
      "\n",
      "\u001b[1mEncoder Output:\u001b[0m (3, 50, 128)\n",
      "tf.Tensor(\n",
      "[[[ 6.222e-02  8.392e-01  3.714e-01 ...  2.900e+00 -5.785e-01  1.452e+00]\n",
      "  [ 8.274e-01  3.232e-01  3.000e+00 ...  2.425e+00 -1.398e-01 -1.206e+00]\n",
      "  [ 2.261e+00  6.924e-02  1.419e+00 ...  1.822e+00 -9.670e-01 -9.161e-01]\n",
      "  ...\n",
      "  [ 4.066e-01 -1.257e+00  7.381e-01 ...  1.372e+00  9.401e-01 -3.576e-01]\n",
      "  [ 8.756e-01 -2.218e+00  2.820e-02 ...  4.745e-01  1.643e+00 -2.017e-01]\n",
      "  [-8.921e-01 -6.914e-01 -1.169e+00 ...  1.691e+00  1.462e+00  1.136e-01]]\n",
      "\n",
      " [[ 1.634e+00  1.064e+00  3.224e-01 ...  2.484e+00 -3.431e-01  8.705e-01]\n",
      "  [ 1.967e+00  3.436e-01  3.463e-01 ...  1.938e+00 -7.841e-01  1.388e+00]\n",
      "  [ 1.244e+00 -6.123e-01  1.481e+00 ...  1.649e+00  9.671e-02  5.757e-01]\n",
      "  ...\n",
      "  [ 1.730e+00 -1.265e+00 -5.425e-01 ...  5.281e-01  1.355e+00 -9.276e-01]\n",
      "  [ 2.793e-01 -1.219e+00  4.292e-01 ...  4.288e-01  1.228e+00 -3.345e-01]\n",
      "  [-3.549e-01 -5.565e-01 -1.562e+00 ...  1.027e+00  1.034e+00 -2.084e-01]]\n",
      "\n",
      " [[ 6.646e-01  2.823e-01  7.134e-01 ...  5.243e-01 -2.924e-02  8.468e-01]\n",
      "  [ 2.102e+00  1.918e-01  1.645e+00 ...  2.844e+00 -5.252e-01 -2.352e-01]\n",
      "  [ 1.323e+00 -1.639e+00 -1.454e-01 ...  2.494e+00 -5.064e-01  1.093e+00]\n",
      "  ...\n",
      "  [ 1.592e+00 -2.282e+00  5.746e-01 ...  2.386e+00  1.835e+00 -4.179e-01]\n",
      "  [ 4.344e-01 -1.371e+00 -1.899e-01 ...  1.610e+00  1.213e+00  2.264e-01]\n",
      "  [-5.385e-01 -2.880e-04  4.423e-01 ...  1.920e+00  9.340e-01 -1.025e+00]]], shape=(3, 50, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "text_data = [\n",
    "    \"This is a test sentence.\",\n",
    "    \"Another example sentence for testing.\",\n",
    "    \"The model should process text correctly.\"\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "num_layers = 4\n",
    "embedding_dim = 128\n",
    "num_heads = 8\n",
    "fully_connected_dim = 512\n",
    "input_vocab_size = 1000\n",
    "maximum_position_encoding = 50\n",
    "dropout_rate = 0.1\n",
    "layernorm_eps = 1e-6\n",
    "max_length = 50  # Matches maximum_position_encoding\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(num_words=input_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to ensure consistent length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# Convert padded sequences to tensor\n",
    "input_tensor = tf.convert_to_tensor(padded_sequences)\n",
    "\n",
    "print(\"\\033[1m Input tensor:\\033[0m\")\n",
    "print(input_tensor)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Padding mask\n",
    "mask = create_padding_mask(input_tensor)\n",
    "\n",
    "\n",
    "# Instantiate the Encoder\n",
    "encoder = Encoder(\n",
    "    num_layers=num_layers,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    fully_connected_dim=fully_connected_dim,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    maximum_position_encoding=maximum_position_encoding,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layernorm_eps=layernorm_eps\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "encoder_output = encoder(input_tensor, training=True, mask=mask)\n",
    "\n",
    "# Display the output\n",
    "print(\"\\033[1mEncoder Output:\\033[0m\", encoder_output.shape)\n",
    "print(encoder_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10296e93",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## <font color='red'> <b>  8. Decoder </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee3068",
   "metadata": {},
   "source": [
    "The Decoder layer takes the K and V matrices generated by the Encoder and in computes the second multi-head attention layer with the Q matrix from the output:\n",
    "\n",
    "<img src=\"images/Decoder.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1075e",
   "metadata": {},
   "source": [
    "<a name=\"8.1\"></a>\n",
    "### <font color='orange'> <b> 8.1. Decoder Layer </b> </font>\n",
    "\n",
    "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training .\n",
    "\n",
    "Implement `DecoderLayer()` using the `call()` method\n",
    "    \n",
    "1. Block 1 is a multi-head attention layer with a residual connection, dropout layer, and look-ahead mask.\n",
    "\n",
    "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a dropout layer, layer normalization and a residual connection, just like you've done before. \n",
    "\n",
    "3. Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95126964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # enc_output.shape == (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        #x = tf.cast(x, tf.float32)\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1 (~1 line)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        \n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the attention output and the input (~1 line)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
    "        # MultiHeadAttention's call takes input (Query, Value, Key, attention_mask, return_attention_scores, training)\n",
    "        # Return attention scores as attn_weights_block2 (~1 line)\n",
    "        attn2, attn_weights_block2 = self.mha2( out1,enc_output, enc_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the attention output and the output of the first block (~1 line)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the ffn output and the output of the second block\n",
    "        out3 =  self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0c1c5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 15, 64)\n",
      "Attention weights block 1 shape: (2, 4, 15, 15)\n",
      "Attention weights block 2 shape: (2, 4, 15, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 15, 64), dtype=float32, numpy=\n",
       "array([[[-1.954,  0.069, -0.78 , ...,  0.59 , -0.406,  0.768],\n",
       "        [-0.657, -0.195, -1.372, ..., -0.243, -1.091,  0.467],\n",
       "        [-1.113, -0.641, -0.384, ..., -1.154,  0.462,  1.47 ],\n",
       "        ...,\n",
       "        [-0.366,  1.158, -0.297, ...,  1.094,  1.011, -0.164],\n",
       "        [-0.368,  1.137, -0.323, ...,  1.071,  0.992, -0.233],\n",
       "        [-0.307,  1.109, -0.265, ...,  1.161,  0.787, -0.177]],\n",
       "\n",
       "       [[-0.647, -1.08 ,  0.911, ..., -0.761,  1.967, -0.525],\n",
       "        [-0.942,  0.577,  0.269, ..., -1.648, -0.143, -1.17 ],\n",
       "        [-0.514,  1.08 , -0.708, ...,  0.964,  0.815, -0.183],\n",
       "        ...,\n",
       "        [-0.385,  1.132, -0.29 , ...,  1.151,  1.035, -0.22 ],\n",
       "        [-0.371,  1.028, -0.261, ...,  1.176,  1.087, -0.178],\n",
       "        [-0.396,  1.039, -0.342, ...,  1.054,  0.984,  0.255]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer and example sentences\n",
    "input_text = [\"This is a test.\", \"How are you?\"]\n",
    "target_text = [\"Esto es una prueba.\", \"¿Cómo estás?\"]\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "tokenizer.fit_on_texts(input_text + target_text)\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "fully_connected_dim = 256\n",
    "dropout_rate = 0.1\n",
    "layernorm_eps = 1e-6\n",
    "max_seq_len = 15  # Maximum sequence length for padding\n",
    "batch_size = len(input_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_text)\n",
    "\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences, maxlen=max_seq_len, padding='post'\n",
    ")\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences, maxlen=max_seq_len, padding='post'\n",
    ")\n",
    "\n",
    "# Embedding layer to simulate word embeddings\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Inputs to the model\n",
    "encoder_output = embedding_layer(input_sequences)  # Simulated encoder output\n",
    "decoder_input = embedding_layer(target_sequences)  # Target sequence embeddings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Masks\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # Shape: (size, size)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(max_seq_len)\n",
    "padding_mask = create_padding_mask(input_sequences)\n",
    "\n",
    "# Initialize DecoderLayer\n",
    "decoder_layer = DecoderLayer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    fully_connected_dim=fully_connected_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layernorm_eps=layernorm_eps\n",
    ")\n",
    "\n",
    "# Forward pass through the DecoderLayer\n",
    "out3, attn_weights_block1, attn_weights_block2 = decoder_layer(\n",
    "    decoder_input, encoder_output, training=True, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(\"Output shape:\", out3.shape)\n",
    "print(\"Attention weights block 1 shape:\", attn_weights_block1.shape)\n",
    "print(\"Attention weights block 2 shape:\", attn_weights_block2.shape)\n",
    "\n",
    "\n",
    "out3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a3586",
   "metadata": {},
   "source": [
    "<a name=\"8.2\"></a>\n",
    "### <font color='orange'> <b> 8.2. Full Decoder </b> </font>\n",
    "\n",
    "You will embedd your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers. \n",
    "\n",
    "<img src=\"images/FullDecoder.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c38426",
   "metadata": {},
   "source": [
    "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers\n",
    " \n",
    "You will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps: \n",
    "\n",
    "1. Pass your generated output through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of Decoding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "601fa501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder is starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # create word embeddings \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # calculate positional encodings and add to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # apply a dropout layer to x\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, embedding_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb1678ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (2, 15, 64)\n",
      "decoder_layer1_block1_self_att shape: (2, 4, 15, 15)\n",
      "decoder_layer1_block2_decenc_att shape: (2, 4, 15, 15)\n",
      "decoder_layer2_block1_self_att shape: (2, 4, 15, 15)\n",
      "decoder_layer2_block2_decenc_att shape: (2, 4, 15, 15)\n",
      "decoder_layer3_block1_self_att shape: (2, 4, 15, 15)\n",
      "decoder_layer3_block2_decenc_att shape: (2, 4, 15, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 15, 64), dtype=float32, numpy=\n",
       "array([[[ 1.058e+00,  1.346e+00, -5.090e-01, ..., -1.852e-01,\n",
       "         -1.106e+00,  5.112e-01],\n",
       "        [ 4.595e-01,  9.560e-01, -6.160e-01, ..., -8.296e-01,\n",
       "         -1.024e+00,  1.054e+00],\n",
       "        [-4.885e-01, -1.668e+00,  1.413e+00, ...,  2.886e-01,\n",
       "         -9.335e-02,  5.451e-01],\n",
       "        ...,\n",
       "        [-8.727e-01,  4.817e-01,  1.129e+00, ...,  5.764e-01,\n",
       "         -5.469e-01,  8.673e-02],\n",
       "        [ 1.145e+00,  9.164e-01, -7.235e-01, ...,  1.595e-01,\n",
       "         -1.957e-01, -1.427e+00],\n",
       "        [ 2.037e+00, -8.225e-01, -1.217e+00, ...,  4.961e-01,\n",
       "         -3.399e-01, -1.160e-01]],\n",
       "\n",
       "       [[ 9.116e-01,  7.945e-01, -1.768e+00, ..., -4.350e-01,\n",
       "         -4.551e-01, -7.759e-01],\n",
       "        [ 5.105e-01,  6.132e-01, -1.179e-01, ..., -1.171e+00,\n",
       "         -5.238e-01,  1.111e+00],\n",
       "        [ 5.228e-01, -9.927e-02,  2.643e-02, ...,  8.525e-01,\n",
       "         -1.007e+00,  7.368e-01],\n",
       "        ...,\n",
       "        [ 2.295e-01,  1.000e+00,  1.048e-01, ...,  3.975e-01,\n",
       "         -6.071e-01, -4.430e-01],\n",
       "        [ 3.810e-01,  1.444e+00, -2.292e-01, ...,  4.804e-01,\n",
       "          8.015e-02, -5.446e-01],\n",
       "        [ 2.120e+00,  1.559e-04, -1.123e+00, ...,  6.477e-01,\n",
       "         -2.491e-02, -5.564e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentences for translation\n",
    "input_text = [\"This is a test.\", \"How are you?\"]\n",
    "target_text = [\"Esto es una prueba.\", \"¿Cómo estás?\"]\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "tokenizer.fit_on_texts(input_text + target_text)\n",
    "\n",
    "# Parameters\n",
    "num_layers = 3\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "fully_connected_dim = 256\n",
    "dropout_rate = 0.1\n",
    "layernorm_eps = 1e-6\n",
    "max_seq_len = 15\n",
    "target_vocab_size = len(tokenizer.word_index) + 1\n",
    "maximum_position_encoding = max_seq_len\n",
    "batch_size = len(input_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_text)\n",
    "\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences, maxlen=max_seq_len, padding='post'\n",
    ")\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences, maxlen=max_seq_len, padding='post'\n",
    ")\n",
    "\n",
    "# Embedding layer to simulate encoder output\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Simulated encoder output\n",
    "encoder_output = embedding_layer(input_sequences)\n",
    "\n",
    "# masks\n",
    "look_ahead_mask = create_look_ahead_mask(max_seq_len)\n",
    "padding_mask = create_padding_mask(input_sequences)\n",
    "\n",
    "\n",
    "# Initialize Decoder\n",
    "decoder = Decoder(\n",
    "    num_layers=num_layers,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    fully_connected_dim=fully_connected_dim,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    maximum_position_encoding=maximum_position_encoding,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layernorm_eps=layernorm_eps\n",
    ")\n",
    "\n",
    "# Forward pass through the Decoder\n",
    "decoder_input = tf.convert_to_tensor(target_sequences)\n",
    "out, attention_weights = decoder(\n",
    "    decoder_input, encoder_output, training=True,\n",
    "    look_ahead_mask=look_ahead_mask, padding_mask=padding_mask\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(\"Decoder output shape:\", out.shape)\n",
    "for key, value in attention_weights.items():\n",
    "    print(f\"{key} shape: {value.shape}\")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06667e6",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## <font color='red'> <b>  9. TRANSFORMER </b> </font>\n",
    "\n",
    "<a name=\"9.1\"></a>\n",
    "### <font color='orange'> <b> 9.1. Model </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f8c5a",
   "metadata": {},
   "source": [
    "<img src=\"images/TRANSFORMER.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "457cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            inp -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            tar -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output -- Describe me\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(inp,training,enc_padding_mask) # (batch_size, inp_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # pass decoder output through a linear layer and softmax\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043f92a",
   "metadata": {},
   "source": [
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f93beb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.033 0.019 0.033 0.023 0.029 0.02  0.022 0.046], shape=(8,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 6\n",
    "embedding_dim = 4\n",
    "num_heads = 4\n",
    "fully_connected_dim = 8\n",
    "input_vocab_size = 30\n",
    "target_vocab_size = 35\n",
    "max_positional_encoding_input = 1000\n",
    "max_positional_encoding_target = 1000\n",
    "\n",
    "# Define tokenizer parameters\n",
    "vocab_size = 10000\n",
    "max_seq_len_input = 100\n",
    "max_seq_len_target = 100\n",
    "\n",
    "# Original sentences (before tokenization)\n",
    "original_text_lang_a = \"The quick brown fox jumps over the lazy dog\"\n",
    "original_text_lang_b = \"El rápido zorro marrón salta sobre el perro perezoso\"\n",
    "\n",
    "# Create tokenizers\n",
    "tokenizer_lang_a = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer_lang_b = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit tokenizers on the respective text\n",
    "tokenizer_lang_a.fit_on_texts([original_text_lang_a])\n",
    "tokenizer_lang_b.fit_on_texts([original_text_lang_b])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "sentence_lang_a = tokenizer_lang_a.texts_to_sequences([original_text_lang_a])\n",
    "sentence_lang_b = tokenizer_lang_b.texts_to_sequences([original_text_lang_b])\n",
    "\n",
    "sentence_lang_a = pad_sequences(sentence_lang_a, maxlen=max_seq_len_input, padding='post')\n",
    "sentence_lang_b = pad_sequences(sentence_lang_b, maxlen=max_seq_len_target, padding='post')\n",
    "\n",
    "# Generate padding and look-ahead masks\n",
    "enc_padding_mask = create_padding_mask(sentence_lang_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_lang_b)\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_lang_b.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "trans = Transformer(num_layers, \n",
    "                    embedding_dim, \n",
    "                    num_heads, \n",
    "                    fully_connected_dim, \n",
    "                    input_vocab_size, \n",
    "                    target_vocab_size, \n",
    "                    max_positional_encoding_input,\n",
    "                    max_positional_encoding_target)\n",
    "\n",
    "\n",
    "# Translate using the transformer\n",
    "translation, weights = trans(\n",
    "    sentence_lang_a,\n",
    "    sentence_lang_b,\n",
    "    training=True,\n",
    "    enc_padding_mask=enc_padding_mask,\n",
    "    look_ahead_mask=look_ahead_mask,\n",
    "    dec_padding_mask=dec_padding_mask\n",
    ")\n",
    "\n",
    "# Print part of the translation output for inspection\n",
    "print(translation[0, 0, 0:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadbae6",
   "metadata": {},
   "source": [
    "<a name=\"9.2\"></a>\n",
    "### <font color='orange'> <b> 9.2. Training and predictions </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fb3b2",
   "metadata": {},
   "source": [
    "Now we will train our model on a very small fictitious dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be1394cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c745fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small dataset of input-output sentence pairs\n",
    "input_texts = [\n",
    "    \"The quick brown fox.\",\n",
    "    \"The lazy dog.\",\n",
    "    \"I love programming.\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"AI is the future.\",\n",
    "    \"I am powerful.\",\n",
    "    \"Transformers are interesting.\",\n",
    "]\n",
    "target_texts = [\n",
    "    \"El rápido zorro marrón.\",\n",
    "    \"El perro perezoso.\",\n",
    "    \"Amo programar.\",\n",
    "    \"Los transformadores son poderosos.\",\n",
    "    \"La inteligencia artificial es el futuro.\",\n",
    "    \"Soy poderoso.\",\n",
    "    \"Los transformadores son interesantes.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b067ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "embedding_dim = 16\n",
    "num_heads = 2\n",
    "fully_connected_dim = 32\n",
    "input_vocab_size = 10000\n",
    "target_vocab_size = 10000\n",
    "max_positional_encoding_input = 100\n",
    "max_positional_encoding_target = 100\n",
    "max_seq_len_input = 20\n",
    "max_seq_len_target = 20\n",
    "batch_size = 2\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec39679",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5db5b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit tokenizers\n",
    "tokenizer_lang_a = Tokenizer(num_words=input_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer_lang_b = Tokenizer(num_words=target_vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer_lang_a.fit_on_texts(input_texts)\n",
    "tokenizer_lang_b.fit_on_texts(target_texts)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "input_sequences = tokenizer_lang_a.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer_lang_b.texts_to_sequences(target_texts)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len_input, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_len_target, padding='post')\n",
    "\n",
    "# Split target sequences into decoder input and expected output\n",
    "decoder_input = target_sequences[:, :-1]  # Exclude the last token for input\n",
    "decoder_target = target_sequences[:, 1:]  # Exclude the first token for target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a73a",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a6ff047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Transformer\n",
    "trans = Transformer(num_layers, \n",
    "                    embedding_dim, \n",
    "                    num_heads, \n",
    "                    fully_connected_dim, \n",
    "                    input_vocab_size, \n",
    "                    target_vocab_size, \n",
    "                    max_positional_encoding_input,\n",
    "                    max_positional_encoding_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cba65",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ff24b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcos/.local/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.6402\n",
      "Epoch 2/10, Loss: 3.6138\n",
      "Epoch 3/10, Loss: 3.5920\n",
      "Epoch 4/10, Loss: 3.5679\n",
      "Epoch 5/10, Loss: 3.5414\n",
      "Epoch 6/10, Loss: 3.5174\n",
      "Epoch 7/10, Loss: 3.4934\n",
      "Epoch 8/10, Loss: 3.4638\n",
      "Epoch 9/10, Loss: 3.4340\n",
      "Epoch 10/10, Loss: 3.4016\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss\n",
    "optimizer = Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = loss_object(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss * mask)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(inp, tar_inp, tar_real, trans, optimizer):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = trans(inp, tar_inp, True, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, trans.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trans.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, decoder_input, decoder_target))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch, (inp, tar_inp, tar_real) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, tar_inp, tar_real, trans, optimizer)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6c818",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fadc16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: son son son son son son son son son son son son son son son son son son son\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with an input\n",
    "test_input = \"Transformers are powerful.\"\n",
    "test_input_seq = tokenizer_lang_a.texts_to_sequences([test_input])\n",
    "test_input_seq = pad_sequences(test_input_seq, maxlen=max_seq_len_input, padding='post')\n",
    "\n",
    "start_token = tokenizer_lang_b.word_index[\"<OOV>\"]\n",
    "end_token = tokenizer_lang_b.word_index.get(\".\")\n",
    "\n",
    "test_target_seq = [start_token]\n",
    "for _ in range(max_seq_len_target - 1):\n",
    "    target_seq = pad_sequences([test_target_seq], maxlen=max_seq_len_target, padding='post')\n",
    "    enc_padding_mask = create_padding_mask(test_input_seq)\n",
    "    look_ahead_mask = create_look_ahead_mask(len(target_seq[0]))\n",
    "    dec_padding_mask = create_padding_mask(target_seq)\n",
    "    \n",
    "    predictions, _ = trans(\n",
    "        test_input_seq, target_seq, False, enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "    )\n",
    "    next_word = tf.argmax(predictions[:, -1, :], axis=-1).numpy()[0]\n",
    "    \n",
    "    if next_word == end_token:\n",
    "        break\n",
    "    test_target_seq.append(next_word)\n",
    "\n",
    "translated_text = \" \".join(\n",
    "    [tokenizer_lang_b.index_word.get(word, \"<OOV>\") for word in test_target_seq[1:]]\n",
    ")\n",
    "print(f\"Translated Text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487096a",
   "metadata": {},
   "source": [
    "Our translation is not good, but we have trained our model with a minimal dataset. Remember that the goal was to show how the implemented transformer could be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c81618",
   "metadata": {},
   "source": [
    "<a name=\"annex\"></a>\n",
    "## <font color='red'> <b>  ANNEX </b> </font>\n",
    "\n",
    "<a name=\"custom_training\"></a>\n",
    "### <font color='orange'> <b> Custom Training in TF </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07ea8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset\n",
    "x_train = np.random.randint(0, 100, size=(1000, 2)).astype(np.float32)\n",
    "y_train = np.sum(x_train, axis=1, keepdims=True).astype(np.float32)\n",
    "\n",
    "x_val = np.random.randint(0, 100, size=(200, 2)).astype(np.float32)\n",
    "y_val = np.sum(x_val, axis=1, keepdims=True).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75e742",
   "metadata": {},
   "source": [
    "#### Define a simple model\n",
    "\n",
    "- The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n",
    "\n",
    "\n",
    "- In the\"call\" method we define how the input flows through the model (forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ab82cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple custom model\n",
    "class SumModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SumModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(16, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1)  # Single output for the sum\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "model = SumModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797aa08",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87f0ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 34243.1719, Val Loss: 313.8351\n",
      "Epoch 6/20, Loss: 23.3643, Val Loss: 3.4290\n",
      "Epoch 11/20, Loss: 6.2386, Val Loss: 1.0130\n",
      "Epoch 16/20, Loss: 3.5554, Val Loss: 0.7106\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define a training step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x)\n",
    "        loss = loss_object(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Define a validation step\n",
    "@tf.function\n",
    "def validate_step(x, y):\n",
    "    predictions = model(x)\n",
    "    loss = loss_object(y, predictions)\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle and batch the training data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "\n",
    "    # Train\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in dataset:\n",
    "        batch_loss = train_step(x_batch, y_batch)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    # Validate\n",
    "    val_loss = 0\n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        batch_val_loss = validate_step(x_batch, y_batch)\n",
    "        val_loss += batch_val_loss\n",
    "\n",
    "    if (epoch%5==0):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.numpy():.4f}, Val Loss: {val_loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21be68f",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f75199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: [25. 75.], Prediction: 100.53\n"
     ]
    }
   ],
   "source": [
    "test_input = np.array([[25, 75]], dtype=np.float32)\n",
    "prediction = model(test_input)\n",
    "print(f\"Test Input: {test_input[0]}, Prediction: {prediction.numpy()[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6f4d3",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a>\n",
    "## <font color='red'> <b>  References </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f48dd6",
   "metadata": {},
   "source": [
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "- [NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafa5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
