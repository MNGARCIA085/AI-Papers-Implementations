{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00453283",
   "metadata": {},
   "source": [
    "# <font color='red'> <b> <center> TRANSFORMER </center> </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4abf77",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - Positional Encoding](#1)\n",
    "    - [1.1 - Sine and Cosine Angles](#1-1)\n",
    "        - [Exercise 1 - get_angles](#ex-1)\n",
    "    - [1.2 - Sine and Cosine Positional Encodings](#1-2)\n",
    "        - [Exercise 2 - positional_encoding](#ex-2)\n",
    "- [2 - Masking](#2)\n",
    "    - [2.1 - Padding Mask](#2-1)\n",
    "    - [2.2 - Look-ahead Mask](#2-2)\n",
    "- [3 - Self-Attention](#3)\n",
    "    - [Exercise 3 - scaled_dot_product_attention](#ex-3)\n",
    "- [4 - Encoder](#4)\n",
    "    - [4.1 Encoder Layer](#4-1)\n",
    "        - [Exercise 4 - EncoderLayer](#ex-4)\n",
    "    - [4.2 - Full Encoder](#4-2)\n",
    "        - [Exercise 5 - Encoder](#ex-5)\n",
    "- [5 - Decoder](#5)\n",
    "    - [5.1 - Decoder Layer](#5-1)\n",
    "        - [Exercise 6 - DecoderLayer](#ex-6)\n",
    "    - [5.2 - Full Decoder](#5-2)\n",
    "        - [Exercise 7 - Decoder](#ex-7)\n",
    "- [6 - Transformer](#6)\n",
    "    - [Exercise 8 - Transformer](#ex-8)\n",
    "- [7 - References](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1b6da",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <font color='red'> <b>  1. Intro </b> </font>\n",
    "\n",
    "This notebook is dedicated to implementing a Transformer model from scratch, following the architecture described in the groundbreaking paper \"Attention Is All You Need\" by Vaswani et al. The primary objective of this implementation is to learn and understand the inner workings of Transformers, rather than simply using prebuilt libraries.\n",
    "\n",
    "I am going to cover key components such as:\n",
    "\n",
    "- Positional Encoding: Injecting order into sequences.\n",
    "- Scaled Dot-Product Attention: The core mechanism behind attention.\n",
    "- Multi-Head Attention: Learning diverse relationships in data.\n",
    "- Feed-Forward Networks: Extending attention's capabilities.\n",
    "- Layer Normalization and Residual Connections: Stabilizing and enhancing learning.\n",
    "- Encoder-Decoder Structure: Processing input and generating output.\n",
    "\n",
    "By the end of this notebook, you should gain a conceptual understanding of each component and how they fit together to form the Transformer. This hands-on approach will help demystify the architecture and prepare you to adapt or experiment with it for your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc891d8",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <font color='red'> <b>  2. Setup </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d678e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aacec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the print options\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69acea5",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <font color='red'> <b>  3. Positional Encoding </b> </font>\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### <font color='orange'> <b> 3.1. Concept and motivation </b> </font>\n",
    "\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model.  However, when you train a Transformer network, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network.\n",
    "\n",
    "\n",
    "There are various types of positional encoding, for example:\n",
    "\n",
    "- Sinusoidal encoding\n",
    "- Learned Positional Encoding\n",
    "- Relative Positional encoding\n",
    "\n",
    "\n",
    "In this notebook I will use the sinusoidal encoding, given by the following formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{n}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{n}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $i$ refers to each of the different dimensions of the positional encoding.\n",
    "* $n$ is a user defined scalar (10000 in the paper)\n",
    "\n",
    "\n",
    "The values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data. \n",
    "\n",
    "In this notebook I am going to use horizontal vectors, that's why all the matrix multiplications should be adjusted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb63d2b",
   "metadata": {},
   "source": [
    "Let’s clarify the **number of positions in the sequence** with an example.\n",
    "\n",
    "\n",
    "Imagine you have a sequence of words:\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\n",
    "\n",
    "Here:\n",
    "\n",
    "- Each word in the sequence corresponds to a position.\n",
    "- The total number of positions is the number of words in the sequence, which is 6 in this case.\n",
    "\n",
    "Now, if you're embedding these words (e.g., using word embeddings), you might represent the sequence as a matrix where:\n",
    "\n",
    "- Rows correspond to positions in the sequence (one row for each word).\n",
    "- Columns correspond to the embedding dimensions (let’s assume d=4 for simplicity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b735c3",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### <font color='orange'> <b> 3.2. Implementation </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5601a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get the angles for the positional encoding\n",
    "def get_angles(pos, i, d, n=10000):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        i --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "        n -- user defined scalar\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    angles = pos/ (np.power(n, (2 * (i//2)) / np.float32(d)))\n",
    "\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3cb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If i = 0 -> i//2 = 0\n",
      "If i = 1 -> i//2 = 0\n",
      "If i = 2 -> i//2 = 1\n",
      "If i = 3 -> i//2 = 1\n"
     ]
    }
   ],
   "source": [
    "# NOTE\n",
    "for i in range(4):\n",
    "    print(f\"If i = {i} -> i//2 = {i//2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e7168",
   "metadata": {},
   "source": [
    "Let's try our function.\n",
    "\n",
    "First let's remember that we can use np.newaxis to add an extra dimension to a numpy array.\n",
    "\n",
    "For example, in np.arange(7)[:, np.newaxis], [:, np.newaxis] adds a new axis (dimension) to the array along the columns, essentially converting the 1D array into a 2D column vector.\n",
    "\n",
    "In the case of np.arange(3)[np.newaxis, :], the new axis is added along the rows, resulting in a row vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5245bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7,), (7, 1), (3,), (1, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(7).shape, np.arange(7)[:, np.newaxis].shape, np.arange(3).shape,  np.arange(3)[np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb099cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(7), np.arange(7)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5748742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([[0, 1, 2]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3), np.arange(3)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13af562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  ],\n",
       "       [1.  , 1.  , 0.01],\n",
       "       [2.  , 2.  , 0.02],\n",
       "       [3.  , 3.  , 0.03],\n",
       "       [4.  , 4.  , 0.04],\n",
       "       [5.  , 5.  , 0.05],\n",
       "       [6.  , 6.  , 0.06]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = get_angles(np.arange(7)[:, np.newaxis], np.arange(3)[np.newaxis, :], 4)\n",
    "print(example.shape)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad6f37",
   "metadata": {},
   "source": [
    "Now, let's implement the positional encoding. We have to use the sine equation when $i$ is an even number and the cosine equation when $i$ is an odd number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c540b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[ np.newaxis,:],\n",
    "                            d)\n",
    "      \n",
    "    # -> angle_rads has dim (positions,d)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # #  add an extra dimension at the beginning of the array, \n",
    "    # so the final shape of pos_encoding becomes (1, positions, d)\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e10d5",
   "metadata": {},
   "source": [
    "**Note.**. Let's talk a little bit more about the line \"pos_encoding = angle_rads[np.newaxis, ...]\". The purpose of np.newaxis is to add an extra dimension at the beginning of the array, so the final shape of pos_encoding becomes (1, positions, d). This means the positional encoding has an additional dimension at the start, representing a \"batch\" of size 1 (this is typical in TensorFlow and other deep learning frameworks).\n",
    "\n",
    "For example, if positions = 3 and d = 4, the shape of pos_encoding will be (1, 3, 4).\n",
    "\n",
    "- 1 represents the batch size (in this case, just one sequence set).\n",
    "- 3 is the number of positions in the sequence.\n",
    "- 4 is the size of the embedding vectors.\n",
    "\n",
    "To retrieve the positional encoding for the positions, you can take pos_encoding[0], which selects the matrix with dimensions (3, 4). This is the positional encoding to be added to your input embeddings, representing the application of positional encoding to your input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b607f1",
   "metadata": {},
   "source": [
    "Let's try our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e8a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 3), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  1.        ,  0.        ],\n",
       "        [ 0.84147096,  0.5403023 ,  0.00215443],\n",
       "        [ 0.9092974 , -0.41614684,  0.00430886],\n",
       "        [ 0.14112   , -0.9899925 ,  0.00646326],\n",
       "        [-0.7568025 , -0.6536436 ,  0.00861763],\n",
       "        [-0.9589243 ,  0.2836622 ,  0.01077196],\n",
       "        [-0.2794155 ,  0.96017027,  0.01292625],\n",
       "        [ 0.6569866 ,  0.75390226,  0.01508047]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 positions with an embedding size of 3\n",
    "example = positional_encoding(8, 3)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8468ff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 3), dtype=float32, numpy=\n",
       "array([[ 0.        ,  1.        ,  0.        ],\n",
       "       [ 0.84147096,  0.5403023 ,  0.00215443],\n",
       "       [ 0.9092974 , -0.41614684,  0.00430886],\n",
       "       [ 0.14112   , -0.9899925 ,  0.00646326],\n",
       "       [-0.7568025 , -0.6536436 ,  0.00861763],\n",
       "       [-0.9589243 ,  0.2836622 ,  0.01077196],\n",
       "       [-0.2794155 ,  0.96017027,  0.01292625],\n",
       "       [ 0.6569866 ,  0.75390226,  0.01508047]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43604db5",
   "metadata": {},
   "source": [
    "Let's plot our positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2888e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_positions = 50\n",
    "d = 512\n",
    "\n",
    "pos_encoding = positional_encoding(num_positions, d)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu') # we take pos_encoding[0] (we remove batch dimension)\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, d))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014364d",
   "metadata": {},
   "source": [
    "Each row represents a positional encoding - notice how none of the rows are identical! We have created a unique positional encoding for each of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d29500",
   "metadata": {},
   "source": [
    "Finally, let's look at an example where we add the positional encoding to the word embeddings.\n",
    "\n",
    "<img src=\"images/PosEncoding.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c679e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput embedding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[0.1 0.2 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]], shape=(3, 4), dtype=float32)\n",
      "\u001b[1m\n",
      "Positional Encoding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[[ 0.          1.          0.          1.        ]\n",
      "  [ 0.84147096  0.5403023   0.00999983  0.99995   ]\n",
      "  [ 0.9092974  -0.41614684  0.01999867  0.9998    ]]], shape=(1, 3, 4), dtype=float32)\n",
      "\u001b[1m\n",
      "Sum of the embedding with positional encoding:\u001b[0m\n",
      "tf.Tensor(\n",
      "[[0.1       1.2       0.3       1.4      ]\n",
      " [1.341471  1.1403023 0.7099998 1.79995  ]\n",
      " [1.8092973 0.5838531 1.1199987 2.1998   ]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let' say that we have an embedding with 3 tokens and 4 dimensions\n",
    "\n",
    "\n",
    "# input text: \"I am happy\"\n",
    "embedding = np.array([[0.1, 0.2, 0.3, 0.4], # corresponds to \"I\"\n",
    "                     [0.5, 0.6, 0.7, 0.8],  # corresponds to \"am\"\n",
    "                     [0.9, 1.0, 1.1, 1.2]]) # corresponds to \"happy\"\n",
    "\n",
    "embedding = tf.constant(embedding, dtype=tf.float32)\n",
    "\n",
    "# Positional encoding for a sequence of length 3 and embedding size 4\n",
    "pos_encoding = positional_encoding(3, 4)\n",
    "\n",
    "# sum of embedding with positional encoding\n",
    "encoded_input = embedding + pos_encoding[0]\n",
    "\n",
    "# Results\n",
    "print(\"\\033[1mInput embedding:\\033[0m\")\n",
    "print(embedding)\n",
    "\n",
    "print(\"\\033[1m\\nPositional Encoding:\\033[0m\")\n",
    "print(pos_encoding)\n",
    "\n",
    "print(\"\\033[1m\\nSum of the embedding with positional encoding:\\033[0m\")\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5b752",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <font color='red'> <b>  4. Masking </b> </font>\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: \n",
    "\n",
    "- The *padding mask*\n",
    "- The *look-ahead mask*\n",
    "\n",
    "Both help the softmax computation give the appropriate weights to the words in your input sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a71e83",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### <font color='orange'> <b> 4.1. Padding mask </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991f41c",
   "metadata": {},
   "source": [
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, they zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by a small negative number (for example -1e9) and adding it to your sequence, you mask out the zeros by setting them to close to negative infinity.\n",
    "\n",
    "After masking, your input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when you take the softmax, the zeros don't affect the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283a014",
   "metadata": {},
   "source": [
    "Let's remember that in in a softmax function, when x approach to -infinity, the value tends to 0.\n",
    "\n",
    "<img src=\"images/softmax.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c2a64",
   "metadata": {},
   "source": [
    "We will use [tf.math.equal](https://www.tensorflow.org/api_docs/python/tf/math/equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db8e4a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1 0 3]\n",
      " [4 0 6]]\n",
      "\n",
      " Mask:\n",
      " [[False  True False]\n",
      " [False  True False]]\n",
      "\n",
      " Mask Float:\n",
      " [[0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example sequence\n",
    "seq = tf.constant([[1, 0, 3], [4, 0, 6]])\n",
    "print(f\"Original data:\\n {seq}\")\n",
    "\n",
    "# Check where elements are equal to 0\n",
    "mask = tf.math.equal(seq, 0)  # Result: [[False, True, False], [False, True, False]]\n",
    "print(f\"\\n Mask:\\n {mask}\")\n",
    "\n",
    "# Convert to float32\n",
    "mask_float = tf.cast(mask, tf.float32)  # Result: [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]\n",
    "\n",
    "print(f\"\\n Mask Float:\\n {mask_float}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c57fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] # shape (n, 1, 1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "797089cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's try our function\n",
    "\n",
    "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c93471",
   "metadata": {},
   "source": [
    "We can see that in every place where there was a number other than 0, it was changed to 0, and in the places where there was a 1, it was changed to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618dca1",
   "metadata": {},
   "source": [
    "Let's multiply this by a small number (1e-9 in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d592bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[-0.e+00, -0.e+00, -1.e+09, -1.e+09, -0.e+00]]],\n",
       "\n",
       "\n",
       "       [[[-0.e+00, -0.e+00, -0.e+00, -1.e+09, -1.e+09]]],\n",
       "\n",
       "\n",
       "       [[[-1.e+09, -1.e+09, -1.e+09, -0.e+00, -0.e+00]]]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask(x) * -1.0e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb306f16",
   "metadata": {},
   "source": [
    "We will now see that the difference between the softmax of x and that of x added to this mask is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed902c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[7.288e-01, 2.681e-01, 6.645e-04, 6.645e-04, 1.806e-03],\n",
       "       [8.444e-02, 2.295e-01, 6.239e-01, 3.106e-02, 3.106e-02],\n",
       "       [4.854e-03, 4.854e-03, 4.854e-03, 2.650e-01, 7.204e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.softmax(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44b8030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 3, 5), dtype=float32, numpy=\n",
       "array([[[[0.09 , 0.245, 0.665, 0.   , 0.   ],\n",
       "         [0.016, 0.117, 0.867, 0.   , 0.   ],\n",
       "         [0.844, 0.114, 0.042, 0.   , 0.   ]]],\n",
       "\n",
       "\n",
       "       [[[0.   , 0.259, 0.705, 0.035, 0.   ],\n",
       "         [0.   , 0.114, 0.844, 0.042, 0.   ],\n",
       "         [0.   , 0.576, 0.212, 0.212, 0.   ]]],\n",
       "\n",
       "\n",
       "       [[[0.245, 0.665, 0.   , 0.   , 0.09 ],\n",
       "         [0.107, 0.787, 0.   , 0.   , 0.107],\n",
       "         [0.665, 0.09 , 0.   , 0.   , 0.245]]]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.softmax(x + create_padding_mask(x) * -1.0e9, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af314c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Padding Mask\n",
    "    - A mask that indicates which elements in the sequence are padding and which are not.\n",
    "- Usage in Attention.\n",
    "    - It ensures that padding values do not affect the attention scores and, therefore, the result of the softmax.\n",
    "- Effect: \n",
    "    - Padding values do not influence the calculation of the output representations, allowing the model to focus only on the relevant parts of the sequence.\n",
    "This is crucial for proper training and for preventing the model from being \"confused\" by the artificially added zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e3e34",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <font color='orange'> <b> 4.2. Look-ahead mask </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e4c7",
   "metadata": {},
   "source": [
    "The look-ahead mask (or causal mask) is a special type of mask used in the Transformer, especially in the context of the decoder. Its main function is to prevent the model from using future information when predicting the next word or token in a sequence, ensuring that the model is autoregressive.\n",
    "\n",
    "Why is it used? The purpose of the look-ahead mask is to prevent the model from seeing the future when generating an output sequence. This means that when the model is predicting the token at position i, it should not \"see\" tokens at later positions (i+1, i+2, ...). This is crucial during both training and inference because, during inference, the model does not have access to future sequence information.\n",
    "\n",
    "How does it work? The look-ahead mask is a lower triangular matrix (with 1s in the lower part and 0s in the upper part), which is applied to the attention scores. This mask ensures that the attention values corresponding to future tokens in the sequence are masked.\n",
    "\n",
    "During the attention calculation, the attention scores for elements at future positions are set to a very negative value (e.g., negative infinity), so that after applying softmax, these positions do not influence the probability calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7997dd",
   "metadata": {},
   "source": [
    "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765213c1",
   "metadata": {},
   "source": [
    "For the implementation we will use [tf.linalg.band_part](https://www.tensorflow.org/api_docs/python/tf/linalg/band_part).\n",
    "\n",
    "More specifically:\n",
    "\n",
    "tf.linalg.band_part(matrix, -1, 0):\n",
    "\n",
    "This is a function that extracts the lower triangular part of a matrix, with the specific options given:\n",
    "\n",
    "- matrix: The matrix to extract from, which is the matrix of ones in this case.\n",
    "\n",
    "- -1: The number of diagonals to include below the main diagonal. A value of -1 means all diagonals below the main diagonal.\n",
    "\n",
    "- 0: The number of diagonals to include above the main diagonal. A value of 0 means no diagonals above the main diagonal are included.\n",
    "\n",
    "So, it creates a lower triangular matrix, meaning it keeps the elements below and on the main diagonal, and sets all the elements above the main diagonal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5f2b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c27b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71b172",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Look-ahead mask: \n",
    "    - A mask used in the decoder to prevent the model from seeing future tokens when making predictions in a sequence. \n",
    "\n",
    "- Purpose: \n",
    "    - Ensure that the model is autoregressive and generates tokens in sequential order, without accessing future information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e22d2",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <font color='red'> <b>  5. Attention</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad454cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4021286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ef339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38dd9cd1",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### <font color='orange'> <b> 5.1. Types of Attention </b> </font>\n",
    "\n",
    "The 3 main types of attention in the transformer model are:\n",
    "\n",
    "- Encoder-decoder attention.\n",
    "- Self-Attention\n",
    "- Masked Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab8bc4",
   "metadata": {},
   "source": [
    "### The 3 Main Types of Attention in the Transformer Model:\n",
    "\n",
    "1. **Encoder-Decoder Attention**:\n",
    "   - This occurs in the decoder and allows the decoder to attend to all positions in the encoder's output. \n",
    "   - It is crucial for tasks like translation, where the decoder generates words based on the source sentence encoded by the encoder.\n",
    "\n",
    "2. **Self-Attention**:\n",
    "   - Used in both the encoder and decoder.\n",
    "   - Each token in the input sequence attends to all other tokens in the same sequence, enabling the model to capture dependencies between words.\n",
    "\n",
    "3. **Masked Self-Attention**:\n",
    "   - A variation of self-attention, used in the decoder.\n",
    "   - It prevents the decoder from attending to future tokens (positions ahead of the current token) during training.\n",
    "   - This is achieved by applying a look-ahead mask to the attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1defb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c5c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41af0693",
   "metadata": {},
   "source": [
    "### Encoder decoder-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136ccab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbc628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509219e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f367d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34427698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e9d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2cd9a19",
   "metadata": {},
   "source": [
    "### Scaled dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c4195",
   "metadata": {},
   "source": [
    "In scale dot-products attention:\n",
    "\n",
    "- We have queries, keys and values.\n",
    "- The attention layer outputs context vectors for each query.\n",
    "     - The context vectors are weighted sums of the values where the similarity between the queries and keys determines the weights assigned to each value.\n",
    "- The softmax ensures that the weights add up to 1.\n",
    "- The division by the square roots of the dimension of the key factors is used to improve performance.\n",
    "\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode.\n",
    "\n",
    "\n",
    "The scale dot-product attention mechanism is very efficient since it relies only on matrix multiplication and a softmax. Additionally, we can implement this attention mechanism to run on GPUs or TPUs to speed up training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d4450",
   "metadata": {},
   "source": [
    "To get the query, key and value matrices, you must first transform the words in your sequences toward embeddings.\n",
    "\n",
    "- Let 's take the sentence \"Soy feliz\" as the source for the queries. \n",
    "    - You'll need to get the embedding vector for the words \"Soy\" and \"feliz\".\n",
    "    - The query matrix will contain all of these embedding vectors as rows.\n",
    "    - The matrix size is given by the size of the word embeddings and the length of the sequence. \n",
    "        - If the word embbedimgs size is for example d=4, then, in this example, the size will be 4x2 (because the length of our sequence (\"Soy feliz\") is 2).\n",
    "- To get the key matrix, let's use the sentence “I am happy”. You will get the embedding for each word in the sentence and stack them together to form the key matrix.\n",
    "    - The size will be: sequence length of the keys x dimension of the embedding (3 x 4 in this example).\n",
    "\n",
    "You will generally use the same vectors used for the key matrix for the value matrix. But you could also transform them first.\n",
    "\n",
    "Note that the number of vectors used to form the key and value matrix must be the same. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcfaf8",
   "metadata": {},
   "source": [
    "<img src=\"images/scaled_dot_product.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b88fa",
   "metadata": {},
   "source": [
    "Now, we can implement the formula.\n",
    "\n",
    "- First, you compute the matrix products between the query and the transpose of the key matrix. You scale it by the inverse of the square of the dimension of the key vectors, dk and calculate the softmax. \n",
    "    - This computation will give you a matrix with the weights for each key per query. Therefore the weight matrix will have a total number of elements equal to the number of queries times the number of keys.\n",
    "        - For example, the third element in the second row would correspond to the weights assigned to the third key for the second query.\n",
    "- After the computation of the weights matrix, you can multiply it with the value matrix to get a matrix that has rows and the context vector corresponding to each query. \n",
    "    - The number of columns on this matrix is equal to the size of the value vectors, which is often the same as the embedding size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba115e3",
   "metadata": {},
   "source": [
    "<img src=\"images/math_scd.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a52c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_v0(q, k, v):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0306cc0",
   "metadata": {},
   "source": [
    "**Note.** About the ellipsis in the docstring.\n",
    "\n",
    "The ... (ellipsis) represents the batch dimensions and any additional dimensions (e.g., for multi-head attention or hierarchical models). In simpler cases, this is just the batch size. For example:\n",
    "\n",
    "- In a single-head attention scenario, the shape might be (batch_size,).\n",
    "- In a multi-head attention scenario, the shape might be (batch_size, num_heads).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bb019ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery shape:\u001b[0m (2, 3, 5)\n",
      "\u001b[1mKey shape:\u001b[0m (2, 4, 5)\n",
      "\u001b[1mValue shape:\u001b[0m (2, 4, 6)\n",
      "\u001b[1mOutput shape:\u001b[0m (2, 3, 6)\n",
      "\u001b[1mAttention weights shape:\u001b[0m (2, 3, 4)\n",
      "\n",
      "\u001b[1mAttention Weights:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[0.273 0.215 0.305 0.207]\n",
      "  [0.258 0.219 0.284 0.239]\n",
      "  [0.246 0.23  0.282 0.242]]\n",
      "\n",
      " [[0.215 0.239 0.25  0.296]\n",
      "  [0.227 0.24  0.249 0.283]\n",
      "  [0.239 0.228 0.224 0.309]]], shape=(2, 3, 4), dtype=float32)\n",
      "\n",
      "\u001b[1mOutput:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[0.49  0.527 0.654 0.747 0.213 0.49 ]\n",
      "  [0.465 0.527 0.631 0.751 0.219 0.489]\n",
      "  [0.454 0.527 0.629 0.746 0.219 0.489]]\n",
      "\n",
      " [[0.273 0.125 0.413 0.323 0.479 0.443]\n",
      "  [0.273 0.124 0.415 0.334 0.487 0.438]\n",
      "  [0.287 0.117 0.431 0.336 0.494 0.438]]], shape=(2, 3, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: Query, Key, Value tensors\n",
    "batch_size = 2  # Number of sequences in the batch\n",
    "seq_len_q = 3   # Length of the query sequence\n",
    "seq_len_kv = 4  # Length of the key and value sequences\n",
    "depth = 5       # Embedding size for query/key\n",
    "depth_v = 6     # Embedding size for value\n",
    "\n",
    "# Generate random tensors for q, k, v\n",
    "q = tf.random.uniform((batch_size, seq_len_q, depth))   # Shape: (2, 3, 5)\n",
    "k = tf.random.uniform((batch_size, seq_len_kv, depth))  # Shape: (2, 4, 5)\n",
    "v = tf.random.uniform((batch_size, seq_len_kv, depth_v))  # Shape: (2, 4, 6)\n",
    "\n",
    "# Apply scaled dot-product attention\n",
    "output, attention_weights = scaled_dot_product_attention_v0(q, k, v)\n",
    "\n",
    "# Print shapes and results\n",
    "print(\"\\033[1mQuery shape:\\033[0m\", q.shape)\n",
    "print(\"\\033[1mKey shape:\\033[0m\", k.shape)\n",
    "print(\"\\033[1mValue shape:\\033[0m\", v.shape)\n",
    "print(\"\\033[1mOutput shape:\\033[0m\", output.shape)  # Should match (batch_size, seq_len_q, depth_v)\n",
    "print(\"\\033[1mAttention weights shape:\\033[0m\", attention_weights.shape)  # Should match (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "# Print the tensors (optional)\n",
    "print(\"\\n\\033[1mAttention Weights:\\033[0m\\n\", attention_weights)\n",
    "print(\"\\n\\033[1mOutput:\\033[0m\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a9771",
   "metadata": {},
   "source": [
    "Now, let's generalize the previous function to include an optional mask that will allow us to implement masked self-attention.\n",
    "\n",
    "Mathematicaly:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df77c5c",
   "metadata": {},
   "source": [
    "<img src=\"images/math_scdt_mask.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5fe40ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c85e37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery shape:\u001b[0m (2, 3, 5)\n",
      "\u001b[1mKey shape:\u001b[0m (2, 4, 5)\n",
      "\u001b[1mValue shape:\u001b[0m (2, 4, 6)\n",
      "\u001b[1mMask shape:\u001b[0m (2, 1, 1, 4)\n",
      "\u001b[1mOutput shape:\u001b[0m (2, 2, 3, 6)\n",
      "\u001b[1mAttention weights shape:\u001b[0m (2, 2, 3, 4)\n",
      "\n",
      "\u001b[1mAttention Weights:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[[0.346 0.309 0.345 0.   ]\n",
      "   [0.332 0.312 0.356 0.   ]\n",
      "   [0.331 0.292 0.376 0.   ]]\n",
      "\n",
      "  [[0.358 0.306 0.336 0.   ]\n",
      "   [0.35  0.324 0.326 0.   ]\n",
      "   [0.32  0.352 0.328 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.332 0.    0.331 0.337]\n",
      "   [0.313 0.    0.336 0.352]\n",
      "   [0.318 0.    0.361 0.322]]\n",
      "\n",
      "  [[0.357 0.    0.335 0.309]\n",
      "   [0.355 0.    0.331 0.315]\n",
      "   [0.362 0.    0.371 0.266]]]], shape=(2, 2, 3, 4), dtype=float32)\n",
      "\n",
      "\u001b[1mOutput:\u001b[0m\n",
      " tf.Tensor(\n",
      "[[[[0.507 0.418 0.349 0.552 0.775 0.377]\n",
      "   [0.513 0.416 0.35  0.547 0.77  0.371]\n",
      "   [0.502 0.421 0.334 0.543 0.762 0.366]]\n",
      "\n",
      "  [[0.67  0.493 0.661 0.743 0.67  0.357]\n",
      "   [0.671 0.485 0.662 0.736 0.678 0.35 ]\n",
      "   [0.683 0.472 0.665 0.725 0.687 0.336]]]\n",
      "\n",
      "\n",
      " [[[0.361 0.411 0.183 0.57  0.739 0.426]\n",
      "   [0.367 0.406 0.184 0.567 0.735 0.422]\n",
      "   [0.363 0.414 0.177 0.561 0.728 0.413]]\n",
      "\n",
      "  [[0.74  0.433 0.692 0.858 0.601 0.479]\n",
      "   [0.741 0.429 0.693 0.859 0.602 0.479]\n",
      "   [0.736 0.456 0.688 0.856 0.588 0.475]]]], shape=(2, 2, 3, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example: Query, Key, Value tensors and Mask\n",
    "batch_size = 2  # Number of sequences in the batch\n",
    "seq_len_q = 3   # Length of the query sequence\n",
    "seq_len_kv = 4  # Length of the key and value sequences\n",
    "depth = 5       # Embedding size for query/key\n",
    "depth_v = 6     # Embedding size for value\n",
    "\n",
    "# Generate random tensors for q, k, v\n",
    "q = tf.random.uniform((batch_size, seq_len_q, depth))   # Shape: (2, 3, 5)\n",
    "k = tf.random.uniform((batch_size, seq_len_kv, depth))  # Shape: (2, 4, 5)\n",
    "v = tf.random.uniform((batch_size, seq_len_kv, depth_v))  # Shape: (2, 4, 6)\n",
    "\n",
    "# Generate a mask (e.g., padding mask)\n",
    "mask = tf.constant([[0, 0, 0, 1], [0, 1, 0, 0]], dtype=tf.float32)  # Shape: (2, 4)\n",
    "mask = tf.reshape(mask, (batch_size, 1, 1, seq_len_kv))  # Reshape to broadcast shape\n",
    "\n",
    "# Apply scaled dot-product attention with masking\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "# Print shapes and results\n",
    "print(\"\\033[1mQuery shape:\\033[0m\", q.shape)\n",
    "print(\"\\033[1mKey shape:\\033[0m\", k.shape)\n",
    "print(\"\\033[1mValue shape:\\033[0m\", v.shape)\n",
    "print(\"\\033[1mMask shape:\\033[0m\", mask.shape)\n",
    "print(\"\\033[1mOutput shape:\\033[0m\", output.shape)  # Should match (batch_size, seq_len_q, depth_v)\n",
    "print(\"\\033[1mAttention weights shape:\\033[0m\", attention_weights.shape)  # Should match (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "# Print the tensors (optional)\n",
    "print(\"\\n\\033[1mAttention Weights:\\033[0m\\n\", attention_weights)\n",
    "print(\"\\n\\033[1mOutput:\\033[0m\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a373413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be582656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6cd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9bebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58d929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93bcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b3004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad32af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a30ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e9e799",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<img src=\"self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></center></caption>\n",
    "    \n",
    "The use of self-attention paired with traditional convolutional networks allows for the parallization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to returns rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    <font color='red'> poner las dimensiones de las matrices y la foto  </font>\n",
    "    \n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - scaled_dot_product_attention \n",
    "\n",
    "    Implement the function `scaled_dot_product_attention()` to create attention-based representations\n",
    "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead. Multiply it by -1e9 before applying the softmax. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e917da",
   "metadata": {},
   "source": [
    "Cuando en la documentación de TensorFlow o NumPy ves algo como (..., seq_len, depth), no es un caso específico de indexación, sino una convención para describir tensores. En este contexto:\n",
    "\n",
    "La elipsis ... indica que puede haber dimensiones adicionales antes de seq_len y depth.\n",
    "Por ejemplo:\n",
    "(..., seq_len, depth) podría representar batch_size x num_heads x seq_len x depth si hay dimensiones de batch y de atención multi-cabeza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903988de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask, verbose=False): # verbose\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    #\n",
    "    if verbose:\n",
    "        print(matmul_qk)\n",
    "        print(dk)\n",
    "        print(scaled_attention_logits)\n",
    "        print(attention_weights)\n",
    "        print(output)\n",
    "    \n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28156408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[2.279387   1.825793   0.63186634 1.8834672 ]\n",
      "  [2.052302   1.8852794  1.0053144  1.7148666 ]\n",
      "  [3.4180002  3.1079035  1.5024791  2.8961418 ]\n",
      "  [2.9149811  2.2005377  1.0817361  2.4234016 ]]\n",
      "\n",
      " [[2.116521   1.5778772  2.4985871  1.7644594 ]\n",
      "  [1.5468688  2.1394327  2.2381554  1.2607064 ]\n",
      "  [2.2123241  1.8038421  2.423389   1.9076121 ]\n",
      "  [1.1489832  1.3651273  1.2714128  0.96710193]]], shape=(2, 4, 4), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-1.0000000e+09  6.4551532e-01  2.2339849e-01  6.6590625e-01]\n",
      "  [-1.0000000e+09 -1.0000000e+09  3.5543230e-01  6.0629690e-01]\n",
      "  [-1.0000000e+09 -1.0000000e+09 -1.0000000e+09  1.0239408e+00]\n",
      "  [-1.0000000e+09 -1.0000000e+09 -1.0000000e+09 -1.0000000e+09]]\n",
      "\n",
      " [[-1.0000000e+09  5.5786383e-01  8.8338399e-01  6.2383062e-01]\n",
      "  [-1.0000000e+09 -1.0000000e+09  7.9130745e-01  4.4572705e-01]\n",
      "  [-1.0000000e+09 -1.0000000e+09 -1.0000000e+09  6.7444271e-01]\n",
      "  [-1.0000000e+09 -1.0000000e+09 -1.0000000e+09 -1.0000000e+09]]], shape=(2, 4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.         0.37365612 0.24499042 0.38135353]\n",
      "  [0.         0.         0.4376107  0.5623893 ]\n",
      "  [0.         0.         0.         0.99999994]\n",
      "  [0.24999999 0.24999999 0.24999999 0.24999999]]\n",
      "\n",
      " [[0.         0.2896081  0.401035   0.3093568 ]\n",
      "  [0.         0.         0.5855454  0.41445458]\n",
      "  [0.         0.         0.         0.99999994]\n",
      "  [0.24999999 0.24999999 0.24999999 0.24999999]]], shape=(2, 4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.29261336 0.64584213 0.51472086 0.50637573 0.25639287 0.8927281\n",
      "   0.42770582 0.12196278 0.671261   0.5080507  0.43207064 0.49039036\n",
      "   0.50862706 0.55751455 0.77940774 0.2476519 ]\n",
      "  [0.17242122 0.85290194 0.3390094  0.31038782 0.06826868 0.92711407\n",
      "   0.31345698 0.1750462  0.52686274 0.37120757 0.65300053 0.7184882\n",
      "   0.4290567  0.42024097 0.6834776  0.37660035]\n",
      "  [0.10508799 0.9509441  0.10184347 0.29983863 0.07333135 0.9251179\n",
      "   0.01105356 0.10428309 0.3034663  0.44025478 0.7376397  0.6211514\n",
      "   0.39170882 0.13091873 0.5138349  0.3293154 ]\n",
      "  [0.33506998 0.57535976 0.59100395 0.37981403 0.33571663 0.7510025\n",
      "   0.5387277  0.33165675 0.6729842  0.5059224  0.46411175 0.42616862\n",
      "   0.58122283 0.6134159  0.6461593  0.21963981]]\n",
      "\n",
      " [[0.23208578 0.501287   0.5318092  0.52097696 0.49544936 0.38445038\n",
      "   0.24096185 0.6683731  0.3396231  0.6471406  0.36714303 0.65330774\n",
      "   0.60848564 0.75363004 0.25903597 0.33985057]\n",
      "  [0.07244253 0.49966514 0.73109066 0.46881995 0.48221862 0.32165167\n",
      "   0.32467854 0.52624094 0.42988163 0.8078637  0.20328626 0.6717726\n",
      "   0.54130745 0.8279036  0.26763028 0.35756764]\n",
      "  [0.12006497 0.29757234 0.87238795 0.07725393 0.8209589  0.55903405\n",
      "   0.4048985  0.96170753 0.10955559 0.99574536 0.35682043 0.4157276\n",
      "   0.7003005  0.65392894 0.19503592 0.8193812 ]\n",
      "  [0.4094275  0.41658548 0.6046822  0.42107826 0.48319775 0.3581132\n",
      "   0.4078126  0.5858355  0.31546727 0.6272345  0.41144893 0.71988374\n",
      "   0.6459741  0.7265859  0.19448115 0.43993694]]], shape=(2, 4, 16), dtype=float32)\n",
      "\n",
      "Salida:\n",
      "output shape: (2, 4, 16)\n",
      "attention_weights shape: (2, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define parámetros de prueba\n",
    "batch_size = 2\n",
    "seq_len_q = 4\n",
    "seq_len_k = 4\n",
    "depth = 8\n",
    "depth_v = 16\n",
    "\n",
    "# Generar datos aleatorios para q, k y v\n",
    "q = tf.random.uniform((batch_size, seq_len_q, depth), dtype=tf.float32)\n",
    "k = tf.random.uniform((batch_size, seq_len_k, depth), dtype=tf.float32)\n",
    "v = tf.random.uniform((batch_size, seq_len_k, depth_v), dtype=tf.float32)\n",
    "\n",
    "# Crear una máscara de ejemplo (por ejemplo, para una máscara de atención futura)\n",
    "mask = tf.linalg.band_part(tf.ones((seq_len_q, seq_len_k)), -1, 0)  # Triangular inferior\n",
    "mask = tf.expand_dims(mask, axis=0)  # Agregar dimensión batch\n",
    "mask = tf.cast(mask, dtype=tf.float32)  # Asegurar que sea flotante\n",
    "\n",
    "# Activar modo verbose\n",
    "verbose = True\n",
    "\n",
    "\n",
    "# Ejecutar prueba\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask, verbose=verbose)\n",
    "\n",
    "# Imprimir formas de salida para confirmar\n",
    "print(\"\\nSalida:\")\n",
    "print(\"output shape:\", output.shape)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d75780",
   "metadata": {},
   "source": [
    "q (query):\n",
    "\n",
    "Forma: (..., seq_len_q, depth)\n",
    "Representa el query o consulta en el mecanismo de atención.\n",
    "Es un tensor que contiene las representaciones del input que queremos comparar con las keys (k).\n",
    "seq_len_q es la longitud de la secuencia del query, y depth es la dimensionalidad de la representación.\n",
    "\n",
    "\n",
    "k (key):\n",
    "\n",
    "Forma: (..., seq_len_k, depth)\n",
    "Representa las keys que se utilizan para calcular la similitud con el query.\n",
    "seq_len_k es la longitud de la secuencia de las keys. Debe coincidir con la longitud de las values (v).\n",
    "depth debe coincidir con la dimensión del query para permitir la multiplicación.\n",
    "\n",
    "\n",
    "v (value):\n",
    "\n",
    "Forma: (..., seq_len_v, depth_v)\n",
    "Representa los valores asociados a las keys.\n",
    "seq_len_v (longitud de la secuencia de valores) debe ser igual a seq_len_k, ya que cada key tiene un valor asociado.\n",
    "depth_v es la dimensionalidad de cada valor. Puede ser diferente de depth.\n",
    "mask:\n",
    "\n",
    "Forma: Broadcastable a (..., seq_len_q, seq_len_k)\n",
    "Es una máscara opcional que se aplica a las similitudes calculadas (logits) para ignorar ciertos elementos en la atención.\n",
    "Tipos de máscaras:\n",
    "Máscara de relleno: Se utiliza para ignorar tokens de relleno (padding) en secuencias más cortas.\n",
    "Máscara de atención futura: Se utiliza en modelos como transformers para evitar que un token preste atención a tokens futuros.\n",
    "\n",
    "Si tienes una secuencia de palabras de longitud 5 (seq_len_q=5) con representaciones de 8 dimensiones (depth=8), y estás usando un conjunto de keys y values de longitud 6 (seq_len_k=6 y seq_len_v=6), las formas serían:\n",
    "\n",
    "q: (batch_size, 5, 8)\n",
    "k: (batch_size, 6, 8)\n",
    "v: (batch_size, 6, depth_v) (por ejemplo, depth_v=16).\n",
    "mask: Broadcastable a (batch_size, 5, 6).\n",
    "Esta flexibilidad permite que la función maneje diferentes longitudes de secuencia y dimensiones de representación según la arquitectura del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a6a6d",
   "metadata": {},
   "source": [
    "seq_len_q y seq_len_k\n",
    "seq_len_q (Sequence Length of Query):\n",
    "\n",
    "Es la longitud de la secuencia del query (q).\n",
    "Representa cuántos tokens, palabras o elementos hay en la secuencia que queremos evaluar en términos de atención.\n",
    "Ejemplo:\n",
    "Si estás procesando una frase con 5 palabras como consulta, seq_len_q = 5.\n",
    "seq_len_k (Sequence Length of Key/Value):\n",
    "\n",
    "Es la longitud de la secuencia de las keys (k) y values (v).\n",
    "\n",
    "Representa cuántos tokens, palabras o elementos están disponibles como contexto para la atención.\n",
    "\n",
    "Regla importante: seq_len_k debe ser igual a seq_len_v, ya que cada key tiene un value asociado.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Si tienes una frase de 7 palabras como contexto, seq_len_k = 7 y seq_len_v = 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f32749",
   "metadata": {},
   "source": [
    "Elipsis (...):\n",
    "\n",
    "Representa dimensiones adicionales opcionales al principio del tensor.\n",
    "Permite trabajar con entradas que tienen diferentes niveles de jerarquía, como lotes de datos (batch) o niveles de anidación.\n",
    "Ejemplo de forma (..., seq_len_q, depth):\n",
    "\n",
    "Caso 1: Sin lotes (batch_size = 1):\n",
    "\n",
    "Si procesas una sola secuencia, el tensor podría tener la forma (seq_len_q, depth).\n",
    "Ejemplo: (5, 8) para 5 tokens con representaciones de 8 dimensiones cada uno.\n",
    "Caso 2: Con lotes (batch_size = 2):\n",
    "\n",
    "Para procesar múltiples secuencias en paralelo, ... puede ser el tamaño del lote.\n",
    "Ejemplo: (batch_size, seq_len_q, depth) → (2, 5, 8).\n",
    "Caso 3: Dimensiones más complejas:\n",
    "\n",
    "Si trabajas con datos anidados o multi-modales, ... podría incluir más dimensiones adicionales.\n",
    "Ejemplo: (num_heads, batch_size, seq_len_q, depth)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01591fd",
   "metadata": {},
   "source": [
    "Supongamos que trabajamos con un modelo que analiza secuencias de texto en lotes:\n",
    "\n",
    "Un lote tiene 2 secuencias.\n",
    "Cada secuencia tiene:\n",
    "4 palabras como query (seq_len_q=4).\n",
    "6 palabras como contexto (seq_len_k=6).\n",
    "Las representaciones tienen una dimensionalidad de 8 (depth=8).\n",
    "Las formas de los tensores serían:\n",
    "\n",
    "q: (2, 4, 8) → 2 secuencias, 4 palabras de query, 8 dimensiones.\n",
    "k: (2, 6, 8) → 2 secuencias, 6 palabras de key, 8 dimensiones.\n",
    "v: (2, 6, depth_v) (por ejemplo, depth_v=16) → Valores asociados a las keys.\n",
    "Con esta estructura, la función calcula cómo cada palabra de las queries se relaciona con cada palabra de las keys, y luego aplica los pesos a los valores para generar la salida final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c26b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beff47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39373dd0",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    " you can think of as computing the self-attention several times to detect different features.\n",
    " \n",
    " \n",
    " ----------->>>> FOTO\n",
    " \n",
    " We will use the Keras implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c83918",
   "metadata": {},
   "source": [
    "### masked multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239b27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab57325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d63f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d57c84",
   "metadata": {},
   "source": [
    "### Feed forward network\n",
    "\n",
    "It will contain 2 Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45fd8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed17f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ceaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd448c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581794b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacee2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "126c26c4",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. You will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
    "<img src=\"encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</font></center></caption>\n",
    "\n",
    "    \n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab54bd",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training.\n",
    "\n",
    "In this exercise, you will implement one encoder block (Figure 2) using the `call()` method. The function should perform the following steps: \n",
    "1. You will pass the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K should be the same.\n",
    "2. Next, you will pass the output of the multi-head attention layer to a dropout layer. Don't forget to use the `training` parameter to set the mode of your model. \n",
    "3. Now add a skip connection by adding your original input `x` and the output of the dropout layer. \n",
    "4. After adding the skip connection, pass the output through the first layer normalization.\n",
    "5. Finally, repeat steps 1-4 but with the feed forward neural network instead of the multi-head attention layer.\n",
    "\n",
    "**Additional Hints**:\n",
    "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n",
    "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. *Note that if query, key and value are the same, then this function performs self-attention.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf6fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha(~1 line)\n",
    "        #-> To compute self-attention Q, V and K should be the same (x)\n",
    "        self_attn_output = self.mha(x, x, x, mask) # Self attention (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to the self-attention output (~1 line)\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer (~1 line)\n",
    "        mult_attn_out = self.layernorm1(x + self_attn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
    "        ffn_output = self.ffn(mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output (~1 line)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer (~1 line)\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b4c4d",
   "metadata": {},
   "source": [
    "### Full encoder\n",
    "\n",
    "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embedd your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</font></center></caption>\n",
    "\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - Encoder\n",
    "\n",
    "Complete the `Encoder()` function using the `call()` method to embed your input, add positional encoding, and implement multiple encoder layers \n",
    "\n",
    "In this exercise, you will initialize your Encoder with an Embedding layer, positional encoding, and multiple EncoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your input through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "507c6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "\n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x,training, mask)\n",
    "\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18757f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35b0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95126964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf3877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43cb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6e297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9198c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cdb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6579c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e5158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff24b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85bb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc16ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0ce29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d666169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aa082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ab6d9a2",
   "metadata": {},
   "source": [
    "### Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "014fcbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 43.3206\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.0862\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 20.9029\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 14.5307\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.1090\n",
      "Predicción para 5.0: [[4.88076]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definimos una clase que hereda de tf.keras.Model\n",
    "class SimpleLinearModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        # Agregamos una capa completamente conectada con 1 unidad\n",
    "        self.dense = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Implementamos el paso de inferencia del modelo\n",
    "        return self.dense(inputs)\n",
    "\n",
    "# Creamos una instancia del modelo\n",
    "model = SimpleLinearModel()\n",
    "\n",
    "# Datos de ejemplo\n",
    "x = tf.constant([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = tf.constant([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.fit(x, y, epochs=5, verbose=0)\n",
    "\n",
    "# Hacemos una predicción\n",
    "predictions = model(tf.constant([[5.0]]))\n",
    "print(\"Predicción para 5.0:\", predictions.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1339297",
   "metadata": {},
   "source": [
    "class SimpleLinearModel(tf.keras.Model):\n",
    "\n",
    "Define una subclase personalizada que hereda de tf.keras.Model.\n",
    "Sirve para estructurar el modelo de forma más flexible.\n",
    "self.dense = tf.keras.layers.Dense(units=1):\n",
    "\n",
    "Creamos una capa densa con 1 unidad. Esto representa un modelo lineal.\n",
    "Método call:\n",
    "\n",
    "Es el método donde definimos cómo pasa la entrada a través del modelo (forward pass).\n",
    "Entrenamiento y predicción:\n",
    "\n",
    "El modelo funciona como cualquier otro modelo en Keras: lo compilamos, entrenamos y hacemos predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58af4d7",
   "metadata": {},
   "source": [
    "En el constructor (__init__):\n",
    "\n",
    "Definimos las capas personalizadas o predefinidas que va a utilizar nuestro modelo.\n",
    "Esto incluye capas de Keras como Dense, Conv2D, o incluso nuestras propias capas personalizadas.\n",
    "En el método call:\n",
    "\n",
    "Construimos la arquitectura del modelo, especificando cómo fluyen los datos a través de las capas.\n",
    "Este método define el forward pass del modelo, es decir, cómo se transforma la entrada para producir la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a617d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedc653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129be415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58aef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e78986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate logits for Category 1\n",
    "logits = np.linspace(-10, 10, 100)  # Logits from -10 to 10\n",
    "logits_matrix = np.stack([logits, np.zeros_like(logits), -logits], axis=1)  # Three categories\n",
    "\n",
    "# Convert logits to a TensorFlow tensor\n",
    "logits_tensor = tf.constant(logits_matrix, dtype=tf.float32)\n",
    "\n",
    "# Compute the softmax\n",
    "softmax_values = tf.nn.softmax(logits_tensor, axis=1).numpy()\n",
    "\n",
    "# Plot the softmax for Category 1\n",
    "plt.plot(logits, softmax_values[:, 0],  color=\"red\")\n",
    "\n",
    "# Add a vertical line at x=0\n",
    "plt.axvline(x=0, color=\"orange\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Softmax Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
